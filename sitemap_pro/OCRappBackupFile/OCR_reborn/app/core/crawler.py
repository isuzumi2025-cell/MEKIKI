"""
Webã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼
æŒ‡å®šã•ã‚ŒãŸURLã‚’å·¡å›ã—ã€PCãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ãƒˆã§ã€Œè¦‹ãŸã¾ã¾ã€ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’æ’®å½±
Playwright ã‚’ä½¿ç”¨ã—ãŸé«˜å“è³ªãªã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆæ©Ÿèƒ½
"""

from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout
from typing import Optional, Dict, List
from pathlib import Path
import time
import json
import os


class WebCrawler:
    """
    Webã‚µã‚¤ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’æ’®å½±ã™ã‚‹ã‚¯ãƒ©ã‚¹
    
    æ©Ÿèƒ½:
    - PCãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ãƒˆ (1920x1080) ã§ã®æ’®å½±
    - å®Œå…¨ãªãƒ­ãƒ¼ãƒ‰å¾…æ©Ÿ (networkidle + sleep)
    - Basicèªè¨¼å¯¾å¿œ
    - Cookieä¿å­˜/å¾©å…ƒ
    - ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾å¿œï¼ˆå…¨ä½“æ’®å½±ï¼‰
    """
    
    def __init__(self, viewport_width: int = 1920, viewport_height: int = 1080):
        """
        Args:
            viewport_width: ãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ãƒˆå¹…ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1920pxï¼‰
            viewport_height: ãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ãƒˆé«˜ã•ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1080pxï¼‰
        """
        self.viewport_width = viewport_width
        self.viewport_height = viewport_height
        self.auth_file = "auth.json"
    
    def crawl(
        self,
        url: str,
        output_path: str,
        username: Optional[str] = None,
        password: Optional[str] = None,
        wait_time: int = 2,
        full_page: bool = True,
        headless: bool = True
    ) -> Dict[str, any]:
        """
        æŒ‡å®šURLã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’æ’®å½±
        
        Args:
            url: å¯¾è±¡URL
            output_path: å‡ºåŠ›ç”»åƒãƒ‘ã‚¹ (.png)
            username: Basicèªè¨¼ãƒ¦ãƒ¼ã‚¶ãƒ¼åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            password: Basicèªè¨¼ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            wait_time: è¿½åŠ å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
            full_page: ãƒšãƒ¼ã‚¸å…¨ä½“ã‚’æ’®å½±ã™ã‚‹ã‹ï¼ˆTrue: å…¨ä½“, False: ãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ãƒˆã®ã¿ï¼‰
            headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
        
        Returns:
            {
                "success": bool,
                "title": str,
                "url": str,
                "screenshot_path": str,
                "viewport_size": (width, height),
                "full_page_size": (width, height) or None,
                "error": str or None
            }
        """
        result = {
            "success": False,
            "title": "",
            "url": url,
            "screenshot_path": "",
            "viewport_size": (self.viewport_width, self.viewport_height),
            "full_page_size": None,
            "error": None
        }
        
        with sync_playwright() as p:
            try:
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æº–å‚™
                context_options = {
                    "viewport": {
                        "width": self.viewport_width,
                        "height": self.viewport_height
                    }
                }
                
                # Cookieå¾©å…ƒ
                if os.path.exists(self.auth_file):
                    try:
                        with open(self.auth_file, 'r') as f:
                            json.load(f)  # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                        context_options['storage_state'] = self.auth_file
                        print(f"ğŸ”“ ä¿å­˜ã•ã‚ŒãŸCookieã‚’ä½¿ç”¨ã—ã¾ã™")
                    except:
                        print(f"âš ï¸  Cookieãƒ•ã‚¡ã‚¤ãƒ«ãŒç„¡åŠ¹ã§ã™")
                
                # Basicèªè¨¼è¨­å®š
                if username and password:
                    context_options['http_credentials'] = {
                        'username': username,
                        'password': password
                    }
                    print(f"ğŸ”‘ Basicèªè¨¼ã‚’ä½¿ç”¨ã—ã¾ã™: {username}")
                
                # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
                browser = p.chromium.launch(headless=headless)
                context = browser.new_context(**context_options)
                page = context.new_page()
                
                # ãƒšãƒ¼ã‚¸é·ç§»
                print(f"ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
                page.goto(url, timeout=60000, wait_until="domcontentloaded")
                
                # å®Œå…¨ãªãƒ­ãƒ¼ãƒ‰å¾…æ©Ÿ
                print(f"â³ ãƒ­ãƒ¼ãƒ‰å¾…æ©Ÿä¸­...")
                page.wait_for_load_state("networkidle", timeout=30000)
                time.sleep(wait_time)  # è¿½åŠ å¾…æ©Ÿï¼ˆå‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œï¼‰
                
                # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                result["title"] = page.title()
                
                # ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºå–å¾—ï¼ˆfull_pageãƒ¢ãƒ¼ãƒ‰ã®å ´åˆï¼‰
                if full_page:
                    page_height = page.evaluate("document.documentElement.scrollHeight")
                    page_width = page.evaluate("document.documentElement.scrollWidth")
                    result["full_page_size"] = (page_width, page_height)
                    print(f"ğŸ“ ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º: {page_width}x{page_height}")
                
                # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆæ’®å½±
                print(f"ğŸ“¸ ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆæ’®å½±ä¸­...")
                page.screenshot(path=output_path, full_page=full_page)
                
                result["screenshot_path"] = output_path
                result["success"] = True
                print(f"âœ… å®Œäº†: {output_path}")
                
                # Cookieä¿å­˜
                context.storage_state(path=self.auth_file)
                
                browser.close()
                
            except PlaywrightTimeout as e:
                result["error"] = f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {str(e)}"
                print(f"âŒ {result['error']}")
            except Exception as e:
                result["error"] = str(e)
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
        
        return result
    
    def crawl_multiple(
        self,
        urls: List[str],
        output_dir: str,
        **kwargs
    ) -> List[Dict[str, any]]:
        """
        è¤‡æ•°URLã‚’ä¸€æ‹¬ã‚¯ãƒ­ãƒ¼ãƒ«
        
        Args:
            urls: URLä¸€è¦§
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            **kwargs: crawl()ã«æ¸¡ã™ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        
        Returns:
            å„URLã®çµæœãƒªã‚¹ãƒˆ
        """
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        results = []
        
        for i, url in enumerate(urls):
            filename = f"page_{i+1:03d}.png"
            output_path = os.path.join(output_dir, filename)
            
            print(f"\n{'='*60}")
            print(f"[{i+1}/{len(urls)}] {url}")
            print(f"{'='*60}")
            
            result = self.crawl(url, output_path, **kwargs)
            results.append(result)
        
        return results
    
    def interactive_login(
        self,
        url: str,
        callback_on_ready: callable = None
    ):
        """
        æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ç”¨ï¼ˆç”»é¢æ“ä½œãŒå¿…è¦ãªã‚µã‚¤ãƒˆç”¨ï¼‰
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ–ãƒ©ã‚¦ã‚¶ã§æ“ä½œã—ãŸå¾Œã€Cookieã‚’ä¿å­˜
        
        Args:
            url: ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸URL
            callback_on_ready: ãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œå®Œäº†ã‚’å¾…ã¤ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        """
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False)
            context = browser.new_context(
                viewport={
                    "width": self.viewport_width,
                    "height": self.viewport_height
                }
            )
            page = context.new_page()
            
            print(f"ğŸ”µ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ã—ã¾ã—ãŸ: {url}")
            try:
                page.goto(url, timeout=60000)
            except Exception as e:
                print(f"âš ï¸  ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿è­¦å‘Š: {e}")
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œå¾…æ©Ÿ
            if callback_on_ready:
                callback_on_ready()
            else:
                input("ãƒ­ã‚°ã‚¤ãƒ³æ“ä½œå®Œäº†å¾Œã€Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„...")
            
            # Cookieä¿å­˜
            try:
                context.storage_state(path=self.auth_file)
                print(f"âœ… Cookieæƒ…å ±ã‚’ {self.auth_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
            except Exception as e:
                print(f"âŒ Cookieä¿å­˜å¤±æ•—: {e}")
            
            browser.close()


class URLManager:
    """
    ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚„ãƒªãƒ³ã‚¯ä¸€è¦§ã‹ã‚‰URLã‚’ç®¡ç†ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹
    """
    
    @staticmethod
    def load_from_file(filepath: str) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰URLä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€
        ï¼ˆ1è¡Œã«1URLï¼‰
        """
        with open(filepath, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        return urls
    
    @staticmethod
    def save_to_file(filepath: str, urls: List[str]):
        """
        URLä¸€è¦§ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        """
        with open(filepath, 'w', encoding='utf-8') as f:
            for url in urls:
                f.write(url + '\n')
    
    @staticmethod
    def extract_links_from_page(url: str, same_domain_only: bool = True) -> List[str]:
        """
        æŒ‡å®šURLã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        
        Args:
            url: å¯¾è±¡URL
            same_domain_only: åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿æŠ½å‡ºã™ã‚‹ã‹
        
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸURLä¸€è¦§
        """
        from urllib.parse import urljoin, urlparse
        
        links = []
        base_domain = urlparse(url).netloc
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            try:
                page.goto(url, timeout=60000)
                page.wait_for_load_state("networkidle")
                
                # å…¨<a>ã‚¿ã‚°ã‹ã‚‰hrefã‚’æŠ½å‡º
                link_elements = page.query_selector_all('a[href]')
                
                for elem in link_elements:
                    href = elem.get_attribute('href')
                    if not href:
                        continue
                    
                    # çµ¶å¯¾URLã«å¤‰æ›
                    absolute_url = urljoin(url, href)
                    
                    # åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                    if same_domain_only:
                        if urlparse(absolute_url).netloc != base_domain:
                            continue
                    
                    if absolute_url not in links:
                        links.append(absolute_url)
                
            except Exception as e:
                print(f"âŒ ãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                browser.close()
        
        return links

