"""
åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹
Webãƒšãƒ¼ã‚¸ã¨PDFã®æ¯”è¼ƒã‚’è‡ªå‹•åŒ–
"""

import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.core.crawler import WebCrawler
from app.core.pdf_loader import PDFLoader
from app.core.engine_clustering import ClusteringEngine
from app.core.comparator import Comparator


def main():
    """åŸºæœ¬çš„ãªæ¯”è¼ƒãƒ•ãƒ­ãƒ¼"""
    
    # 1. Webã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°
    print("=" * 60)
    print("1. Webãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆæ’®å½±")
    print("=" * 60)
    
    crawler = WebCrawler()
    web_result = crawler.crawl(
        url="https://example.com",
        output_path="temp_web.png",
        wait_time=2,
        full_page=True
    )
    
    if not web_result["success"]:
        print(f"ã‚¨ãƒ©ãƒ¼: {web_result['error']}")
        return
    
    print(f"âœ… Webç”»åƒã‚’å–å¾—: {web_result['title']}")
    
    # 2. PDFèª­ã¿è¾¼ã¿
    print("\n" + "=" * 60)
    print("2. PDFã®é«˜è§£åƒåº¦ç”»åƒåŒ–")
    print("=" * 60)
    
    pdf_loader = PDFLoader(dpi=300)
    pdf_images = pdf_loader.load("example.pdf", page_numbers=[1])
    
    if not pdf_images:
        print("ã‚¨ãƒ©ãƒ¼: PDFã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")
        return
    
    print(f"âœ… PDFç”»åƒã‚’å–å¾—: {len(pdf_images)} ãƒšãƒ¼ã‚¸")
    
    # 3. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒ‡ãƒ¢ç”¨ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼‰
    print("\n" + "=" * 60)
    print("3. é ˜åŸŸæ¤œå‡ºï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼‰")
    print("=" * 60)
    
    # å®Ÿéš›ã«ã¯Google Cloud Vision APIã‚’ä½¿ç”¨
    # ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
    web_clusters = [
        {"id": 1, "rect": [50, 50, 300, 150], "text": "ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ1"},
        {"id": 2, "rect": [50, 200, 400, 300], "text": "ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ2"}
    ]
    
    pdf_clusters = [
        {"id": 1, "rect": [50, 50, 300, 150], "text": "ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ1"},
        {"id": 2, "rect": [50, 200, 400, 300], "text": "ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ2ï¼ˆä¿®æ­£ç‰ˆï¼‰"}
    ]
    
    print(f"âœ… Web: {len(web_clusters)} ã‚¨ãƒªã‚¢æ¤œå‡º")
    print(f"âœ… PDF: {len(pdf_clusters)} ã‚¨ãƒªã‚¢æ¤œå‡º")
    
    # 4. æ¯”è¼ƒå®Ÿè¡Œ
    print("\n" + "=" * 60)
    print("4. Web vs PDF æ¯”è¼ƒ")
    print("=" * 60)
    
    comparator = Comparator()
    comparator.set_data(web_clusters, pdf_clusters)
    results = comparator.compare_all()
    
    # 5. ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    summary = comparator.get_summary()
    
    print(f"\nğŸ“Š æ¯”è¼ƒã‚µãƒãƒªãƒ¼")
    print(f"  ç·æ•°: {summary['total']}")
    print(f"  âœ… ä¸€è‡´: {summary['match']}")
    print(f"  âš ï¸ ä¸ä¸€è‡´: {summary['mismatch']}")
    print(f"  ğŸŒ Webå°‚ç”¨: {summary['web_only']}")
    print(f"  ğŸ“„ PDFå°‚ç”¨: {summary['pdf_only']}")
    print(f"  å¹³å‡é¡ä¼¼åº¦: {summary['average_similarity']:.2%}")
    
    # 6. è©³ç´°è¡¨ç¤º
    print(f"\nğŸ“ è©³ç´°çµæœ")
    for result in results:
        status_icon = {
            "match": "âœ…",
            "mismatch": "âš ï¸",
            "web_only": "ğŸŒ",
            "pdf_only": "ğŸ“„"
        }.get(result["status"], "â“")
        
        print(f"\n{status_icon} Area {result['area_id']} - {result['status'].upper()}")
        print(f"   é¡ä¼¼åº¦: {result['similarity']:.2%}")
        if result["web_text"]:
            print(f"   Web: {result['web_text'][:50]}...")
        if result["pdf_text"]:
            print(f"   PDF: {result['pdf_text'][:50]}...")
    
    # 7. ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    print("\n" + "=" * 60)
    print("5. ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›")
    print("=" * 60)
    
    comparator.export_to_csv("comparison_result.csv")
    comparator.export_to_html("comparison_result.html")
    
    print("\nâœ… å®Œäº†ï¼")


if __name__ == "__main__":
    main()

