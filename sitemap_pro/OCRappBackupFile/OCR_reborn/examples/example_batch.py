"""
ãƒãƒƒãƒå‡¦ç†ã®ä¾‹
è¤‡æ•°ã®URLã‚’ä¸€æ‹¬ã§å‡¦ç†
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from app.core.crawler import WebCrawler, URLManager
from app.utils.helpers import create_output_directory, save_clusters_to_json


def main():
    """è¤‡æ•°URLã®ä¸€æ‹¬å‡¦ç†"""
    
    # URLãƒªã‚¹ãƒˆã®æº–å‚™
    urls = [
        "https://example.com/page1",
        "https://example.com/page2",
        "https://example.com/page3",
    ]
    
    # ã¾ãŸã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
    # urls = URLManager.load_from_file("urls.txt")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = create_output_directory("batch_output")
    
    # ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼åˆæœŸåŒ–
    crawler = WebCrawler()
    
    # ä¸€æ‹¬ã‚¯ãƒ­ãƒ¼ãƒ«
    print("ğŸš€ ãƒãƒƒãƒå‡¦ç†é–‹å§‹")
    print(f"   å¯¾è±¡URLæ•°: {len(urls)}")
    print(f"   å‡ºåŠ›å…ˆ: {output_dir}")
    print()
    
    results = crawler.crawl_multiple(
        urls=urls,
        output_dir=str(output_dir),
        username=None,  # Basicèªè¨¼ãŒå¿…è¦ãªå ´åˆã¯æŒ‡å®š
        password=None,
        wait_time=2,
        full_page=True,
        headless=True
    )
    
    # çµæœã®ã‚µãƒãƒªãƒ¼
    success_count = sum(1 for r in results if r["success"])
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ãƒãƒƒãƒå‡¦ç†å®Œäº†")
    print("=" * 60)
    print(f"  æˆåŠŸ: {success_count} / {len(urls)}")
    print(f"  å¤±æ•—: {len(urls) - success_count}")
    
    # å¤±æ•—ã—ãŸURLã‚’è¡¨ç¤º
    failed_urls = [r["url"] for r in results if not r["success"]]
    if failed_urls:
        print("\nâš ï¸ å¤±æ•—ã—ãŸURL:")
        for url in failed_urls:
            print(f"  - {url}")


if __name__ == "__main__":
    main()

