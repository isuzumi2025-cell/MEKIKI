"""
Analyzer Module
ã‚³ã‚¢åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ - ãƒ†ã‚­ã‚¹ãƒˆæ¯”è¼ƒã€é¡ä¼¼åº¦è¨ˆç®—ã€è‡ªå‹•ãƒãƒƒãƒãƒ³ã‚°
"""
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass, field
from pathlib import Path
from PIL import Image
import difflib
import uuid


@dataclass
class PageData:
    """
    ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€ï¼‰
    """
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    source_type: str = ""  # "web" or "pdf"
    source_id: str = ""  # URL or PDF filename
    page_num: Optional[int] = None  # PDFã®å ´åˆã¯ãƒšãƒ¼ã‚¸ç•ªå·
    title: str = ""  # Webãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«
    text: str = ""  # ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆ
    image: Optional[Image.Image] = None  # PIL Image
    image_path: Optional[str] = None  # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    areas: List['DetectedArea'] = field(default_factory=list)  # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã®ãƒªã‚¹ãƒˆ
    error: Optional[str] = None  # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    
    def to_dict(self) -> Dict:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            "id": self.id,
            "source_type": self.source_type,
            "source_id": self.source_id,
            "page_num": self.page_num,
            "title": self.title,
            "text": self.text,
            "image_path": self.image_path,
            "areas_count": len(self.areas),
            "error": self.error
        }


@dataclass
class DetectedArea:
    """
    æ¤œå‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢
    """
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    text: str = ""
    bbox: List[int] = field(default_factory=lambda: [0, 0, 0, 0])  # [x0, y0, x1, y1]
    confidence: float = 0.0
    source_type: str = ""  # "web" or "pdf"
    source_id: str = ""  # URL or PDF filename
    page_num: Optional[int] = None  # PDFã®å ´åˆã¯ãƒšãƒ¼ã‚¸ç•ªå·
    
    def to_dict(self) -> Dict:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            "id": self.id,
            "text": self.text,
            "bbox": self.bbox,
            "confidence": self.confidence,
            "source_type": self.source_type,
            "source_id": self.source_id,
            "page_num": self.page_num
        }


@dataclass
class MatchedPair:
    """
    ãƒãƒƒãƒãƒ³ã‚°ã•ã‚ŒãŸWebãƒšãƒ¼ã‚¸ã¨PDFãƒšãƒ¼ã‚¸ã®ãƒšã‚¢
    """
    web_page: PageData
    pdf_page: PageData
    similarity_score: float
    match_type: str = "auto"  # "auto" or "manual"
    
    def to_dict(self) -> Dict:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            "web_page": self.web_page.to_dict(),
            "pdf_page": self.pdf_page.to_dict(),
            "similarity_score": self.similarity_score,
            "match_type": self.match_type
        }


class ContentAnalyzer:
    """
    ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æã‚¨ãƒ³ã‚¸ãƒ³
    OCRçµæœã®ç®¡ç†ã€è‡ªå‹•ãƒãƒƒãƒãƒ³ã‚°ã€é¡ä¼¼åº¦è¨ˆç®—
    """
    
    def __init__(self, ocr_engine=None):
        """
        Args:
            ocr_engine: OCREngineã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        """
        self.ocr_engine = ocr_engine
        
        # ãƒšãƒ¼ã‚¸ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆç”»åƒä»˜ãï¼‰
        self.web_pages: List[PageData] = []
        self.pdf_pages: List[PageData] = []
        
        # ã‚¨ãƒªã‚¢ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
        self.web_areas: List[DetectedArea] = []
        self.pdf_areas: List[DetectedArea] = []
        
        # ãƒãƒƒãƒãƒ³ã‚°çµæœ
        self.matched_pairs: List[MatchedPair] = []
    
    def analyze_image(
        self,
        image_path: str,
        source_type: str,
        source_id: str,
        page_num: Optional[int] = None
    ) -> List[DetectedArea]:
        """
        ç”»åƒã‚’OCRã«ã‹ã‘ã€çµæœã‚’DetectedAreaã®ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¦ä¿å­˜
        
        Args:
            image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            source_type: "web" or "pdf"
            source_id: URL or PDF filename
            page_num: PDFã®å ´åˆã¯ãƒšãƒ¼ã‚¸ç•ªå·
        
        Returns:
            DetectedAreaã®ãƒªã‚¹ãƒˆ
        """
        # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
        if not Path(image_path).exists():
            print(f"âš ï¸ ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_path}")
            return []
        
        # OCRã‚¨ãƒ³ã‚¸ãƒ³ã®ç¢ºèª
        if not self.ocr_engine:
            print("âš ï¸ OCRã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        try:
            # OCRå®Ÿè¡Œ
            print(f"ğŸ” OCRå®Ÿè¡Œä¸­: {Path(image_path).name}")
            result = self.ocr_engine.detect_document_text(image_path)
            
            if not result:
                print(f"âš ï¸ OCRçµæœãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {image_path}")
                return []
            
            # DetectedAreaã«å¤‰æ›
            areas = []
            for block in result.get("blocks", []):
                area = DetectedArea(
                    text=block["text"],
                    bbox=block["bbox"],
                    confidence=block["confidence"],
                    source_type=source_type,
                    source_id=source_id,
                    page_num=page_num
                )
                areas.append(area)
            
            # ãƒªã‚¹ãƒˆã«è¿½åŠ 
            if source_type == "web":
                self.web_areas.extend(areas)
            elif source_type == "pdf":
                self.pdf_areas.extend(areas)
            
            print(f"âœ… {len(areas)} ã‚¨ãƒªã‚¢æ¤œå‡º: {Path(image_path).name}")
            return areas
            
        except Exception as e:
            print(f"âš ï¸ ç”»åƒåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
    
    def compute_auto_matches(
        self,
        threshold: float = 0.05,  # æ¥­å‹™ç”¨: ä½é–¾å€¤ã§åºƒããƒãƒƒãƒãƒ³ã‚°
        method: str = "hybrid",
        force_match: bool = True  # å¼·åˆ¶ãƒãƒƒãƒãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
    ) -> List[MatchedPair]:
        """
        Webãƒšãƒ¼ã‚¸ã¨PDFãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã€è‡ªå‹•ãƒšã‚¢ãƒªãƒ³ã‚°ï¼ˆæ¥­å‹™ç”¨æ”¹è‰¯ç‰ˆï¼‰
        
        Args:
            threshold: ãƒãƒƒãƒãƒ³ã‚°é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.05 = 5%ä»¥ä¸Šã®é¡ä¼¼åº¦ï¼‰
            method: é¡ä¼¼åº¦è¨ˆç®—æ–¹æ³• ("difflib", "jaccard", "hybrid")
            force_match: Trueã®å ´åˆã€é–¾å€¤ã«é–¢ä¿‚ãªãå¿…ãšæœ€ã‚‚è¿‘ã„ãƒšãƒ¼ã‚¸ã‚’ãƒšã‚¢ãƒªãƒ³ã‚°
        
        Returns:
            MatchedPairã®ãƒªã‚¹ãƒˆ
        """
        # ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
        use_pages = bool(self.web_pages and self.pdf_pages)
        
        if use_pages:
            web_items = self.web_pages
            pdf_items = self.pdf_pages
            print(f"\n{'='*60}")
            print(f"ğŸ”„ [Matching] è‡ªå‹•ãƒãƒƒãƒãƒ³ã‚°é–‹å§‹ï¼ˆãƒšãƒ¼ã‚¸å˜ä½ï¼‰")
            print(f"  Web Pages: {len(web_items)}")
            print(f"  PDF Pages: {len(pdf_items)}")
            print(f"  Threshold: {threshold*100:.1f}%")
            print(f"  Method: {method}")
            print(f"  Force Match: {'ON' if force_match else 'OFF'}")
            print(f"{'='*60}\n")
        else:
            # å¾Œæ–¹äº’æ›æ€§: ã‚¨ãƒªã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            if not self.web_areas or not self.pdf_areas:
                print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return []
            web_items = self.web_areas
            pdf_items = self.pdf_areas
            print(f"ğŸ”„ è‡ªå‹•ãƒãƒƒãƒãƒ³ã‚°é–‹å§‹ï¼ˆã‚¨ãƒªã‚¢å˜ä½ï¼‰: Web {len(web_items)} Ã— PDF {len(pdf_items)}")
        
        self.matched_pairs.clear()
        
        try:
            # å„Webã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã—ã¦æœ€é©ãªPDFã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¢ã™
            for i, web_item in enumerate(web_items, start=1):
                best_match = None
                best_score = 0.0
                
                web_text = web_item.text if hasattr(web_item, 'text') else ""
                web_id = web_item.source_id if hasattr(web_item, 'source_id') else f"Web{i}"
                
                # ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã¯ã‚¹ã‚­ãƒƒãƒ—
                if hasattr(web_item, 'error') and web_item.error:
                    print(f"[Match] â­ï¸  Skipped: {web_id[:60]} (Error page)")
                    continue
                
                # ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if not web_text or len(web_text.strip()) < 10:
                    print(f"[Match] â­ï¸  Skipped: {web_id[:60]} (Empty text)")
                    continue
                
                print(f"[Match] [{i}/{len(web_items)}] Processing: {web_id[:60]}")
                print(f"        Web text: {len(web_text)} chars")
                
                # å…¨PDFãƒšãƒ¼ã‚¸ã¨ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                scores = []
                for j, pdf_item in enumerate(pdf_items, start=1):
                    pdf_text = pdf_item.text if hasattr(pdf_item, 'text') else ""
                    pdf_id = f"PDF-P{pdf_item.page_num}" if hasattr(pdf_item, 'page_num') else f"PDF{j}"
                    
                    if not pdf_text or len(pdf_text.strip()) < 10:
                        continue
                    
                    # é¡ä¼¼åº¦ã‚’è¨ˆç®—
                    if method == "difflib":
                        score = self._calculate_similarity(web_text, pdf_text)
                    elif method == "jaccard":
                        score = self._calculate_jaccard(web_text, pdf_text)
                    else:  # hybrid
                        score1 = self._calculate_similarity(web_text, pdf_text)
                        score2 = self._calculate_jaccard(web_text, pdf_text)
                        score = (score1 + score2) / 2
                    
                    scores.append((score, pdf_item, pdf_id))
                    
                    # æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²
                    if score > best_score:
                        best_score = score
                        best_match = pdf_item
                
                # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
                scores.sort(reverse=True, key=lambda x: x[0])
                
                # Top 3ã‚’è¡¨ç¤º
                print(f"        Top matches:")
                for rank, (score, pdf_item, pdf_id) in enumerate(scores[:3], start=1):
                    indicator = "âœ…" if rank == 1 and (force_match or score >= threshold) else "  "
                    print(f"        {indicator} #{rank}: {pdf_id} â†’ {score*100:.2f}%")
                
                # ãƒãƒƒãƒãƒ³ã‚°åˆ¤å®š
                if force_match:
                    # å¼·åˆ¶ãƒãƒƒãƒãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰: å¿…ãšæœ€ã‚‚è¿‘ã„ãƒšãƒ¼ã‚¸ã‚’ãƒšã‚¢ãƒªãƒ³ã‚°
                    if best_match:
                        pair = MatchedPair(
                            web_page=web_item,
                            pdf_page=best_match,
                            similarity_score=best_score,
                            match_type="auto_forced"
                        )
                        self.matched_pairs.append(pair)
                        print(f"        âœ… Matched (Forced): {best_score*100:.2f}%\n")
                else:
                    # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: é–¾å€¤ã‚’è¶…ãˆã¦ã„ã‚‹å ´åˆã®ã¿ãƒšã‚¢ãƒªãƒ³ã‚°
                    if best_match and best_score >= threshold:
                        pair = MatchedPair(
                            web_page=web_item,
                            pdf_page=best_match,
                            similarity_score=best_score,
                            match_type="auto"
                        )
                        self.matched_pairs.append(pair)
                        print(f"        âœ… Matched: {best_score*100:.2f}%\n")
                    else:
                        print(f"        âŒ No match (Best: {best_score*100:.2f}% < {threshold*100:.1f}%)\n")
            
            print(f"\n{'='*60}")
            print(f"âœ… [Matching] Complete: {len(self.matched_pairs)} ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            print(f"{'='*60}\n")
            return self.matched_pairs
            
        except Exception as e:
            print(f"âš ï¸ ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """
        difflib ã‚’ä½¿ç”¨ã—ãŸé¡ä¼¼åº¦è¨ˆç®—
        
        Args:
            text1: ãƒ†ã‚­ã‚¹ãƒˆ1
            text2: ãƒ†ã‚­ã‚¹ãƒˆ2
        
        Returns:
            é¡ä¼¼åº¦ (0.0-1.0)
        """
        if not text1 or not text2:
            return 0.0
        
        return difflib.SequenceMatcher(None, text1, text2).ratio()
    
    def _calculate_jaccard(self, text1: str, text2: str) -> float:
        """
        Jaccardä¿‚æ•°ã‚’è¨ˆç®—ï¼ˆå˜èªãƒ™ãƒ¼ã‚¹ï¼‰
        
        Args:
            text1: ãƒ†ã‚­ã‚¹ãƒˆ1
            text2: ãƒ†ã‚­ã‚¹ãƒˆ2
        
        Returns:
            Jaccardä¿‚æ•° (0.0-1.0)
        """
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    def find_differences(self, text1: str, text2: str) -> List[Dict]:
        """
        2ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆé–“ã®å·®åˆ†ã‚’æ¤œå‡º
        
        Args:
            text1: ãƒ†ã‚­ã‚¹ãƒˆ1
            text2: ãƒ†ã‚­ã‚¹ãƒˆ2
        
        Returns:
            å·®åˆ†ãƒªã‚¹ãƒˆ [{"type": "add/delete/change", "text": str, "line": int}, ...]
        """
        differences = []
        
        # è¡Œã”ã¨ã«åˆ†å‰²
        lines1 = text1.splitlines()
        lines2 = text2.splitlines()
        
        # ndiff ã§å·®åˆ†ã‚’å–å¾—
        diff = difflib.ndiff(lines1, lines2)
        
        line_num = 0
        for item in diff:
            if item.startswith('+ '):  # è¿½åŠ 
                differences.append({
                    "type": "add",
                    "text": item[2:],
                    "line": line_num
                })
            elif item.startswith('- '):  # å‰Šé™¤
                differences.append({
                    "type": "delete",
                    "text": item[2:],
                    "line": line_num
                })
            elif item.startswith('? '):  # å¤‰æ›´ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿
                pass  # ã‚¹ã‚­ãƒƒãƒ—
            else:  # åŒä¸€è¡Œ
                line_num += 1
        
        return differences
    
    def add_manual_pair(
        self,
        web_area: DetectedArea,
        pdf_area: DetectedArea
    ) -> MatchedPair:
        """
        æ‰‹å‹•ã§ãƒšã‚¢ã‚’è¿½åŠ 
        
        Args:
            web_area: Webã‚¨ãƒªã‚¢
            pdf_area: PDFã‚¨ãƒªã‚¢
        
        Returns:
            ä½œæˆã•ã‚ŒãŸMatchedPair
        """
        # é¡ä¼¼åº¦ã‚’è¨ˆç®—
        score = self._calculate_similarity(web_area.text, pdf_area.text)
        
        pair = MatchedPair(
            web_area=web_area,
            pdf_area=pdf_area,
            similarity_score=score,
            match_type="manual"
        )
        
        self.matched_pairs.append(pair)
        print(f"âœ… æ‰‹å‹•ãƒšã‚¢è¿½åŠ : ã‚¹ã‚³ã‚¢ {score:.2%}")
        
        return pair
    
    def clear_all(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢"""
        self.web_pages.clear()
        self.pdf_pages.clear()
        self.web_areas.clear()
        self.pdf_areas.clear()
        self.matched_pairs.clear()
        print("ğŸ—‘ï¸ å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
    
    def get_statistics(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        return {
            "web_areas_count": len(self.web_areas),
            "pdf_areas_count": len(self.pdf_areas),
            "matched_pairs_count": len(self.matched_pairs),
            "average_similarity": sum(p.similarity_score for p in self.matched_pairs) / len(self.matched_pairs) if self.matched_pairs else 0.0
        }

