from playwright.sync_api import sync_playwright
from PIL import Image, ImageDraw, ImageFont
from typing import List, Dict, Optional, Set
from urllib.parse import urlparse, urljoin
import os
import json
import io
import time
import re
import base64

# å·¨å¤§ãªç”»åƒã‚’è¨±å¯
Image.MAX_IMAGE_PIXELS = None

class WebScraper:
    def __init__(self, auth_file="auth.json"):
        self.auth_file = auth_file
        self.visited_urls: Set[str] = set()
        self.failed_urls: Set[str] = set()

    def interactive_login(self, url, wait_callback):
        """æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ç”¨"""
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False)
            context = browser.new_context()
            page = context.new_page()
            
            print(f"ğŸ”µ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ã—ã¾ã—ãŸ: {url}")
            try:
                page.goto(url, timeout=60000)
            except Exception as e:
                print(f"âš ï¸ ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿è­¦å‘Š: {e}")

            wait_callback()
            self.save_session(context)

    def save_session(self, context):
        try:
            context.storage_state(path=self.auth_file)
            print(f"âœ… Cookieæƒ…å ±ã‚’ {self.auth_file} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±æ•—: {e}")

    def _auto_scroll(self, page):
        """Lazy Loadingå¯¾ç­–: å°‘ã—ãšã¤ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦èª­ã¿è¾¼ã¾ã›ã‚‹"""
        print("â¬ ç”»åƒèª­ã¿è¾¼ã¿ã®ãŸã‚ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­...")
        page.evaluate("""
            async () => {
                await new Promise((resolve) => {
                    var totalHeight = 0;
                    var distance = 200; // ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¹…
                    var timer = setInterval(() => {
                        var scrollHeight = document.body.scrollHeight;
                        window.scrollBy(0, distance);
                        totalHeight += distance;

                        if(totalHeight >= scrollHeight - window.innerHeight){
                            clearInterval(timer);
                            resolve();
                        }
                    }, 50); // ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«é€Ÿåº¦
                });
            }
        """)
        # èª­ã¿è¾¼ã¿å¾…ã¡
        time.sleep(2)
        # ä¸€ç•ªä¸Šã«æˆ»ã™
        page.evaluate("window.scrollTo(0, 0)")
        time.sleep(1)

    def fetch_text(self, url, username=None, password=None):
        """
        ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—
        - å…¨ä½“ç”»åƒ: å®¹é‡ã‚ªãƒ¼ãƒãƒ¼é˜²æ­¢ã®ãŸã‚ 1.5å€ç”»è³ª
        - 1ç”»é¢ç”»åƒ: ç²¾å¯†OCRã®ãŸã‚ 2.0å€ç”»è³ª
        """
        with sync_playwright() as p:
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰
            context_options = {
                'viewport': {'width': 1920, 'height': 1080},
                'device_scale_factor': 2.0,
                'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            # Cookie/ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±
            if os.path.exists(self.auth_file):
                try:
                    with open(self.auth_file, 'r') as f: 
                        json.load(f)
                    context_options['storage_state'] = self.auth_file
                    print(f"âœ… Cookieæƒ…å ±èª­ã¿è¾¼ã¿: {self.auth_file}")
                except Exception as e:
                    print(f"âš ï¸ Cookieèª­ã¿è¾¼ã¿å¤±æ•—: {e}")

            # Basicèªè¨¼æƒ…å ±
            if username and password:
                context_options['http_credentials'] = {
                    'username': username,
                    'password': password
                }
                print(f"ğŸ” Basicèªè¨¼è¨­å®š: {username} / {'*' * len(password)}")
            else:
                print(f"â„¹ï¸ Basicèªè¨¼ãªã—")

            # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
            print(f"ğŸš€ Chromiumãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ä¸­...")
            browser = p.chromium.launch(headless=True)
            
            # --- 1. é«˜ç”»è³ªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (OCRç”¨ãƒ»1ç”»é¢åˆ†) ---
            print(f"ğŸ“Š ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ: 1920x1080 @ 2.0x")
            context_high = browser.new_context(**context_options)
            page_high = context_high.new_page()
            
            # context_fullã‚’Noneã§åˆæœŸåŒ–ï¼ˆfinallyãƒ–ãƒ­ãƒƒã‚¯ã§ã®ã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰
            context_full = None
            
            try:
                print(f"ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
                
                # Basicèªè¨¼ã®ãŸã‚ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®šï¼ˆã‚ˆã‚Šç¢ºå®Ÿãªæ–¹æ³•ï¼‰
                if username and password:
                    credentials = base64.b64encode(f"{username}:{password}".encode()).decode()
                    page_high.set_extra_http_headers({
                        "Authorization": f"Basic {credentials}"
                    })
                    print(f"ğŸ”‘ Authorization ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®šå®Œäº†")
                
                # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
                try:
                    # networkidleã§å¾…æ©Ÿ + è¿½åŠ ã®é…å»¶ã§CSS/JSã‚’å®Œå…¨ã«é©ç”¨
                    response = page_high.goto(url, timeout=60000, wait_until="networkidle")
                    
                    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                    if response:
                        status = response.status
                        if status == 404:
                            raise Exception(f"404 Not Found - ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                        elif status == 401:
                            raise Exception(f"401 Unauthorized - èªè¨¼å¤±æ•—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å: {username}ï¼‰")
                        elif status == 403:
                            raise Exception(f"403 Forbidden - ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸ")
                        elif status >= 400:
                            raise Exception(f"HTTP {status} - ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼")
                        
                        print(f"âœ… HTTP {status} - ã‚¢ã‚¯ã‚»ã‚¹æˆåŠŸ")
                    
                    # è¿½åŠ ã®å¾…æ©Ÿï¼ˆCSS/JSãŒå®Œå…¨ã«é©ç”¨ã•ã‚Œã‚‹ã¾ã§ï¼‰
                    page_high.wait_for_timeout(3000)
                    print(f"â³ CSS/JSé©ç”¨å¾…ã¡å®Œäº†")
                    
                except Exception as goto_error:
                    print(f"âš ï¸ ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {str(goto_error)}")
                    raise

                # HTMLãƒ†ã‚­ã‚¹ãƒˆå–å¾—
                text_content = page_high.inner_text("body")
                title = page_high.title()

                # [é«˜ç”»è³ª] 1ç”»é¢åˆ†ã®ã‚¹ã‚¯ã‚·ãƒ§
                view_bytes = page_high.screenshot(full_page=False)
                
                # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ï¼ˆã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯è¿½åŠ ï¼‰
                MIN_SCREENSHOT_SIZE = 50 * 1024  # 50KBä»¥ä¸‹ã¯å¤±æ•—ã¨ã¿ãªã™
                if view_bytes and len(view_bytes) > MIN_SCREENSHOT_SIZE:
                    try:
                        img_view = Image.open(io.BytesIO(view_bytes))
                        print(f"âœ… 1ç”»é¢ã‚¹ã‚¯ã‚·ãƒ§å–å¾—æˆåŠŸ: {len(view_bytes)} bytes")
                    except Exception as e:
                        print(f"âš ï¸ ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        img_view = self._create_placeholder_image("1ç”»é¢ç”»åƒå–å¾—å¤±æ•—")
                else:
                    print(f"âš ï¸ ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒå°ã•ã™ãã¾ã™: {len(view_bytes) if view_bytes else 0} bytes (æœ€å°: {MIN_SCREENSHOT_SIZE} bytes)")
                    img_view = self._create_placeholder_image("1ç”»é¢ç”»åƒå–å¾—å¤±æ•—")
                
                # --- 2. æ¨™æº–ç”»è³ªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (å…¨ä½“è¡¨ç¤ºç”¨) ---
                # å…¨ä½“ã‚¹ã‚¯ã‚·ãƒ§ã¯é•·ããªã‚Šã™ãã‚‹ã®ã§ device_scale_factor=1.5 ã«æŠ‘ãˆã‚‹
                # ã“ã‚Œã§ã€Œé€”åˆ‡ã‚Œã€ã‚’é˜²ã
                context_options_full = context_options.copy()
                context_options_full['device_scale_factor'] = 1.5
                context_full = browser.new_context(**context_options_full)
                page_full = context_full.new_page()
                
                # Basicèªè¨¼ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å†è¨­å®š
                if username and password:
                    credentials = base64.b64encode(f"{username}:{password}".encode()).decode()
                    page_full.set_extra_http_headers({
                        "Authorization": f"Basic {credentials}"
                    })
                
                print(f"ğŸ“¸ å…¨ä½“ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆå–å¾—ä¸­...")
                page_full.goto(url, timeout=60000, wait_until="networkidle") # åŒã˜ãƒšãƒ¼ã‚¸ã‚’é–‹ã
                page_full.wait_for_timeout(3000)  # è¿½åŠ å¾…æ©Ÿ

                # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦å…¨èª­ã¿è¾¼ã¿
                self._auto_scroll(page_full)

                # [ä¸­ç”»è³ª] å…¨ä½“ã‚¹ã‚¯ã‚·ãƒ§
                full_bytes = page_full.screenshot(full_page=True)
                
                # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ï¼ˆã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯è¿½åŠ ï¼‰
                MIN_SCREENSHOT_SIZE = 50 * 1024  # 50KBä»¥ä¸‹ã¯å¤±æ•—ã¨ã¿ãªã™
                if full_bytes and len(full_bytes) > MIN_SCREENSHOT_SIZE:
                    try:
                        img_full = Image.open(io.BytesIO(full_bytes))
                        print(f"âœ… å…¨ä½“ã‚¹ã‚¯ã‚·ãƒ§å–å¾—æˆåŠŸ: {len(full_bytes)} bytes")
                    except Exception as e:
                        print(f"âš ï¸ ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        img_full = self._create_placeholder_image("å…¨ä½“ç”»åƒå–å¾—å¤±æ•—")
                else:
                    print(f"âš ï¸ ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒå°ã•ã™ãã¾ã™: {len(full_bytes) if full_bytes else 0} bytes (æœ€å°: {MIN_SCREENSHOT_SIZE} bytes)")
                    img_full = self._create_placeholder_image("å…¨ä½“ç”»åƒå–å¾—å¤±æ•—")
                
                return title, text_content, img_full, img_view

            except Exception as e:
                raise Exception(f"å–å¾—å¤±æ•—: {str(e)}")
            finally:
                if context_high:
                    context_high.close()
                if context_full:
                    context_full.close()
                if browser:
                    browser.close()
    
    def crawl_site(
        self,
        base_url: str,
        max_pages: int = 50,
        max_depth: int = 3,
        same_domain_only: bool = True,
        username: Optional[str] = None,
        password: Optional[str] = None,
        progress_callback: Optional[callable] = None
    ) -> List[Dict]:
        """
        ã‚µã‚¤ãƒˆå†…ã‚’ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ï¼ˆä¸‹å±¤ãƒšãƒ¼ã‚¸ã‚‚å–å¾—ï¼‰
        
        Args:
            base_url: é–‹å§‹URL
            max_pages: æœ€å¤§å–å¾—ãƒšãƒ¼ã‚¸æ•°
            max_depth: æœ€å¤§æ·±ã•
            same_domain_only: åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿
            username: Basicèªè¨¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å
            password: Basicèªè¨¼ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
        
        Returns:
            [{"url": str, "title": str, "text": str, "full_image": Image, "viewport_image": Image, "error": str}, ...]
        """
        self.visited_urls = set()
        self.failed_urls = set()
        
        base_domain = urlparse(base_url).netloc
        to_visit = [(base_url, 0)]  # (url, depth)
        results = []
        
        print(f"ğŸŒ ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°é–‹å§‹: {base_url}")
        print(f"ğŸ“‹ è¨­å®š: æœ€å¤§{max_pages}ãƒšãƒ¼ã‚¸, æ·±ã•{max_depth}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¨­å®š
            context_options = {
                'viewport': {'width': 1920, 'height': 1080},  # âœ… ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—è¡¨ç¤ºã«çµ±ä¸€
                'device_scale_factor': 2.0
            }
            
            # èªè¨¼æƒ…å ±
            if os.path.exists(self.auth_file):
                try:
                    with open(self.auth_file, 'r') as f:
                        json.load(f)
                    context_options['storage_state'] = self.auth_file
                except:
                    pass
            
            if username and password:
                context_options['http_credentials'] = {
                    'username': username,
                    'password': password
                }
            
            context = browser.new_context(**context_options)
            page = context.new_page()
            
            # Basicèªè¨¼ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®š
            if username and password:
                credentials = base64.b64encode(f"{username}:{password}".encode()).decode()
                page.set_extra_http_headers({
                    "Authorization": f"Basic {credentials}"
                })
                print(f"ğŸ”‘ Basicèªè¨¼ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®š: {username}")
            
            try:
                while to_visit and len(results) < max_pages:
                    current_url, depth = to_visit.pop(0)
                    
                    # è¨ªå•æ¸ˆã¿ãƒã‚§ãƒƒã‚¯
                    if current_url in self.visited_urls or current_url in self.failed_urls:
                        continue
                    
                    # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
                    if same_domain_only:
                        current_domain = urlparse(current_url).netloc
                        if current_domain != base_domain:
                            continue
                    
                    # æ·±ã•ãƒã‚§ãƒƒã‚¯
                    if depth > max_depth:
                        continue
                    
                    try:
                        print(f"ğŸ“„ [{len(results) + 1}/{max_pages}] æ·±ã•{depth}: {current_url}")
                        
                        # é€²æ—é€šçŸ¥
                        if progress_callback:
                            progress_callback(len(results) + 1, max_pages, current_url)
                        
                        # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
                        try:
                            response = page.goto(current_url, timeout=60000, wait_until="domcontentloaded")
                            
                            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                            if response:
                                status = response.status
                                if status == 404:
                                    raise Exception(f"404 Not Found")
                                elif status == 401:
                                    raise Exception(f"401 Unauthorized - èªè¨¼æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                                elif status == 403:
                                    raise Exception(f"403 Forbidden")
                                elif status >= 400:
                                    raise Exception(f"HTTP {status}")
                                
                                print(f"  âœ… HTTP {status}")
                            
                            page.wait_for_load_state("networkidle", timeout=30000)
                            
                        except Exception as goto_error:
                            raise Exception(f"ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {str(goto_error)}")
                        
                        # Lazy Loadingå¯¾å¿œã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                        self._auto_scroll(page)
                        
                        # ãƒ‡ãƒ¼ã‚¿å–å¾—
                        title = page.title()
                        text_content = page.inner_text("body")
                        
                        # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼ˆ1ç”»é¢åˆ†ï¼‰
                        page.evaluate("window.scrollTo(0, 0)")
                        time.sleep(0.3)
                        viewport_bytes = page.screenshot(full_page=False)
                        
                        # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
                        if viewport_bytes and len(viewport_bytes) > 0:
                            try:
                                viewport_image = Image.open(io.BytesIO(viewport_bytes))
                            except Exception as e:
                                print(f"âš ï¸ 1ç”»é¢ç”»åƒå¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
                                viewport_image = self._create_placeholder_image("1ç”»é¢ç”»åƒå–å¾—å¤±æ•—")
                        else:
                            viewport_image = self._create_placeholder_image("1ç”»é¢ç”»åƒå–å¾—å¤±æ•—")
                        
                        # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼ˆãƒ•ãƒ«ãƒšãƒ¼ã‚¸ï¼‰
                        full_bytes = page.screenshot(full_page=True)
                        
                        # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
                        if full_bytes and len(full_bytes) > 0:
                            try:
                                full_image = Image.open(io.BytesIO(full_bytes))
                            except Exception as e:
                                print(f"âš ï¸ å…¨ä½“ç”»åƒå¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
                                full_image = self._create_placeholder_image("å…¨ä½“ç”»åƒå–å¾—å¤±æ•—")
                        else:
                            full_image = self._create_placeholder_image("å…¨ä½“ç”»åƒå–å¾—å¤±æ•—")
                        
                        results.append({
                            "url": current_url,
                            "title": title,
                            "text": text_content,
                            "full_image": full_image,
                            "viewport_image": viewport_image,
                            "depth": depth,
                            "error": None
                        })
                        
                        self.visited_urls.add(current_url)
                        
                        # æ¬¡ã®æ·±ã•ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆæ·±ã•åˆ¶é™å†…ã®å ´åˆã®ã¿ï¼‰
                        if depth < max_depth:
                            links = self._extract_links(page, current_url)
                            for link in links:
                                # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆé™¤å»
                                link = link.split('#')[0]
                                if link and link not in self.visited_urls and link not in self.failed_urls:
                                    # åŒã˜æ·±ã•ã®ãƒªãƒ³ã‚¯ã¯ã‚­ãƒ¥ãƒ¼ã®å¾Œã‚ã«è¿½åŠ 
                                    to_visit.append((link, depth + 1))
                        
                        time.sleep(1.0)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
                        
                    except Exception as e:
                        error_msg = str(e)
                        print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {current_url} - {error_msg}")
                        self.failed_urls.add(current_url)
                        
                        # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¨˜éŒ²ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”»åƒã‚’ä½¿ç”¨ï¼‰
                        error_placeholder = self._create_placeholder_image(f"å–å¾—å¤±æ•—\n{error_msg[:30]}...")
                        
                        results.append({
                            "url": current_url,
                            "title": f"å–å¾—å¤±æ•—: {current_url}",
                            "text": "",
                            "full_image": error_placeholder,
                            "viewport_image": error_placeholder,
                            "depth": depth,
                            "error": error_msg
                        })
                        continue
                
            finally:
                context.close()
                browser.close()
        
        print(f"âœ… ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Œäº†: {len(results)}ãƒšãƒ¼ã‚¸å–å¾—")
        return results
    
    def _create_placeholder_image(self, message: str = "ç”»åƒãªã—", width: int = 1920, height: int = 1080) -> Image.Image:
        """
        ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”»åƒã‚’ä½œæˆ
        
        Args:
            message: è¡¨ç¤ºã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            width: ç”»åƒå¹…
            height: ç”»åƒé«˜ã•
        
        Returns:
            PIL Image
        """
        # ã‚°ãƒ¬ãƒ¼ã®èƒŒæ™¯ç”»åƒã‚’ä½œæˆ
        img = Image.new('RGB', (width, height), color='#2B2B2B')
        draw = ImageDraw.Draw(img)
        
        # ä¸­å¤®ã«èµ¤ã„æ ã‚’æç”»
        margin = 50
        draw.rectangle(
            [margin, margin, width - margin, height - margin],
            outline='#FF4444',
            width=5
        )
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æç”»ï¼ˆãƒ•ã‚©ãƒ³ãƒˆãªã—ã§ã‚·ãƒ³ãƒ—ãƒ«ã«ï¼‰
        text = f"âš ï¸ {message}"
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’å–å¾—ï¼ˆç°¡æ˜“è¨ˆç®—ï¼‰
        text_width = len(text) * 10
        text_height = 20
        text_x = (width - text_width) // 2
        text_y = (height - text_height) // 2
        
        draw.text((text_x, text_y), text, fill='#FF4444')
        
        return img
    
    def _extract_links(self, page, base_url: str) -> List[str]:
        """
        ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ‰åŠ¹ãªãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰
        
        Args:
            page: Playwrightã®ãƒšãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            base_url: ãƒ™ãƒ¼ã‚¹URL
        
        Returns:
            ãƒªãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """
        try:
            # JavaScriptã§ãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆç›¸å¯¾URLã‚‚å«ã‚€ï¼‰
            links = page.evaluate("""
                () => {
                    return Array.from(document.querySelectorAll('a[href]'))
                        .map(a => a.getAttribute('href'))
                        .filter(href => href && href.trim() !== '');
                }
            """)
            
            base_domain = urlparse(base_url).netloc
            absolute_links = []
            
            for link in links:
                # JavaScriptãƒªãƒ³ã‚¯ã‚„ãƒ¡ãƒ¼ãƒ«ãƒªãƒ³ã‚¯ã‚’é™¤å¤–
                if link.startswith('javascript:') or link.startswith('mailto:') or link.startswith('tel:'):
                    continue
                
                # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã®ã¿ã®ãƒªãƒ³ã‚¯ã‚’é™¤å¤–
                if link.startswith('#'):
                    continue
                
                # çµ¶å¯¾URLã«å¤‰æ›
                try:
                    absolute_url = urljoin(base_url, link)
                    
                    # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
                    absolute_url = absolute_url.split('#')[0]
                    
                    # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä¿æŒï¼ˆé‡è¦ãªãƒšãƒ¼ã‚¸è­˜åˆ¥ã«ä½¿ã‚ã‚Œã‚‹å ´åˆãŒã‚ã‚‹ï¼‰
                    
                    # URLã®æ¤œè¨¼
                    parsed = urlparse(absolute_url)
                    
                    # httpã¾ãŸã¯httpsã®ã¿
                    if parsed.scheme not in ['http', 'https']:
                        continue
                    
                    # åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿
                    if parsed.netloc != base_domain:
                        continue
                    
                    # é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–
                    if re.search(r'\.(jpg|jpeg|png|gif|svg|webp|ico|bmp|css|js|json|xml|pdf|zip|tar|gz|rar|7z|exe|dmg|mp4|avi|mov|mp3|wav)$', absolute_url, re.IGNORECASE):
                        continue
                    
                    # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã€æ­£è¦åŒ–ã•ã‚ŒãŸURLã‚’è¿½åŠ 
                    if absolute_url not in absolute_links:
                        absolute_links.append(absolute_url)
                
                except Exception as e:
                    print(f"âš ï¸ URLå¤‰æ›ã‚¨ãƒ©ãƒ¼ ({link}): {str(e)}")
                    continue
            
            print(f"ğŸ”— ãƒªãƒ³ã‚¯æŠ½å‡º: {len(absolute_links)}ä»¶ (å…ƒ: {len(links)}ä»¶)")
            return absolute_links
            
        except Exception as e:
            print(f"âš ï¸ ãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            traceback.print_exc()
            return []