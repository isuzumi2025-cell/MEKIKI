from playwright.sync_api import sync_playwright
import os
import json
import time
from urllib.parse import urljoin, urlparse

class WebScraper:
    def __init__(self, auth_file="auth.json"):
        self.auth_file = auth_file

    def interactive_login(self, url, wait_callback):
        """
        æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ç”¨ï¼ˆBasicèªè¨¼ä»¥å¤–ã®ã€ç”»é¢æ“ä½œãŒå¿…è¦ãªã‚µã‚¤ãƒˆç”¨ï¼‰
        """
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False)
            context = browser.new_context()
            page = context.new_page()
            
            print(f"ğŸ”µ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ã—ã¾ã—ãŸ: {url}")
            try:
                page.goto(url, timeout=60000)
            except Exception as e:
                print(f"âš ï¸ ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿è­¦å‘Š: {e}")

            # GUIå´ã®OKå¾…æ©Ÿ
            wait_callback()
            
            # Cookieä¿å­˜
            self.save_session(context)
            # Basicèªè¨¼æƒ…å ±ã¯ã“ã“ã‹ã‚‰ã¯å–ã‚Œãªã„ãŸã‚ã€GUIå…¥åŠ›ã«é ¼ã‚‹

    def save_session(self, context):
        try:
            context.storage_state(path=self.auth_file)
            print(f"âœ… Cookieæƒ…å ±ã‚’ {self.auth_file} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±æ•—: {e}")

    def fetch_text(self, url, username=None, password=None):
        """
        ä¿å­˜ã•ã‚ŒãŸCookieã€ã¾ãŸã¯æŒ‡å®šã•ã‚ŒãŸID/PASSã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹
        """
        with sync_playwright() as p:
            browser, context, page = self._setup_browser(p, username, password)
            try:
                print(f"ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
                page.goto(url, timeout=60000)
                page.wait_for_load_state("networkidle")

                text_content = page.inner_text("body")
                title = page.title()
                
                return title, text_content
            except Exception as e:
                raise Exception(f"å–å¾—å¤±æ•—: {str(e)}")
            finally:
                context.close()
                browser.close()

    def recursive_crawl(self, root_url, max_depth=2, max_pages=10, username=None, password=None):
        """
        æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰å†å¸°çš„ã«ãƒªãƒ³ã‚¯ã‚’è¾¿ã‚Šã€ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹
        Returns:
            nodes (list): [{"url": str, "title": str, "screenshot": path, "text": str, "depth": int}]
            edges (list): [{"from": url, "to": url}]
        """
        visited = set()
        queue = [(root_url, 0)]
        nodes = []
        edges = []
        
        # ä¿å­˜å…ˆ
        img_dir = os.path.join("output_data", "sitemap_images")
        os.makedirs(img_dir, exist_ok=True)

        with sync_playwright() as p:
            browser, context, page = self._setup_browser(p, username, password)
            
            try:
                while queue and len(visited) < max_pages:
                    url, depth = queue.pop(0)
                    if url in visited or depth > max_depth:
                        continue
                    
                    visited.add(url)
                    print(f"ğŸ•·ï¸ Crawling: {url} (Depth: {depth})")
                    
                    try:
                        page.goto(url, timeout=30000, wait_until="networkidle")
                        page.wait_for_timeout(1000) # ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®‰å®šå¾…ã¡
                        
                        # ãƒ‡ãƒ¼ã‚¿å–å¾—
                        title = page.title()
                        text_content = page.inner_text("body")
                        
                        # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
                        filename = f"node_{len(visited)}.png"
                        screenshot_path = os.path.join(img_dir, filename)
                        page.screenshot(path=screenshot_path, full_page=False)
                        
                        nodes.append({
                            "id": len(visited),
                            "url": url,
                            "title": title,
                            "screenshot": screenshot_path,
                            "text": text_content[:200] + "...", # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨
                            "full_text": text_content,
                            "depth": depth
                        })

                        # æ¬¡ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
                        if depth < max_depth:
                            hrefs = page.eval_on_selector_all("a", "elements => elements.map(e => e.href)")
                            root_domain = urlparse(root_url).netloc
                            
                            for href in hrefs:
                                # URLæ­£è¦åŒ–ã¨ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
                                parsed = urlparse(href)
                                if parsed.netloc == root_domain and href not in visited:
                                    queue.append((href, depth + 1))
                                    edges.append({"from": url, "to": href})
                                    
                    except Exception as e:
                        print(f"âš ï¸ Skip {url}: {e}")
                        continue
                        
            finally:
                context.close()
                browser.close()
                
        return nodes, edges

    def _setup_browser(self, p, username, password):
        context_options = {}
        if os.path.exists(self.auth_file):
            try:
                with open(self.auth_file, 'r') as f: json.load(f)
                context_options['storage_state'] = self.auth_file
            except: pass

        if username and password:
            context_options['http_credentials'] = {'username': username, 'password': password}

        browser = p.chromium.launch(headless=True)
        context = browser.new_context(**context_options)
        page = context.new_page()
        return browser, context, page