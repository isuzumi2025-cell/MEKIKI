"""
ContentAnalyzer ã¨ OCREngine ã®ä½¿ç”¨ä¾‹

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€æ–°ã—ã„åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚’ç¤ºã—ã¾ã™ã€‚
"""
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.core.ocr_engine import OCREngine
from app.core.analyzer import ContentAnalyzer, DetectedArea


def example_basic_ocr():
    """åŸºæœ¬çš„ãªOCRå®Ÿè¡Œä¾‹"""
    print("=" * 60)
    print("ğŸ“ ä¾‹1: åŸºæœ¬çš„ãªOCRå®Ÿè¡Œ")
    print("=" * 60)
    
    # OCRã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–
    ocr = OCREngine(credentials_path="credentials.json")
    
    if not ocr.initialize():
        print("âš ï¸ OCRã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œå‡º
    image_path = "test_images/sample.png"  # é©åˆ‡ãªãƒ‘ã‚¹ã«å¤‰æ›´ã—ã¦ãã ã•ã„
    
    result = ocr.detect_document_text(image_path)
    
    if result:
        print(f"\nğŸ“„ å…¨ä½“ãƒ†ã‚­ã‚¹ãƒˆ:\n{result['full_text']}\n")
        print(f"ğŸ” æ¤œå‡ºã•ã‚ŒãŸãƒ–ãƒ­ãƒƒã‚¯æ•°: {len(result['blocks'])}")
        
        for i, block in enumerate(result['blocks'][:3], 1):  # æœ€åˆã®3ãƒ–ãƒ­ãƒƒã‚¯ã®ã¿è¡¨ç¤º
            print(f"\n  ãƒ–ãƒ­ãƒƒã‚¯ {i}:")
            print(f"    ãƒ†ã‚­ã‚¹ãƒˆ: {block['text'][:50]}...")
            print(f"    åº§æ¨™: {block['bbox']}")
            print(f"    ä¿¡é ¼åº¦: {block['confidence']:.2%}")


def example_analyzer_workflow():
    """ContentAnalyzerã‚’ä½¿ç”¨ã—ãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¾‹"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ä¾‹2: ContentAnalyzer ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼")
    print("=" * 60)
    
    # OCRã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–
    ocr = OCREngine(credentials_path="credentials.json")
    
    if not ocr.initialize():
        print("âš ï¸ OCRã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # ContentAnalyzerã‚’ä½œæˆ
    analyzer = ContentAnalyzer(ocr_engine=ocr)
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: Webç”»åƒã‚’åˆ†æ
    print("\nğŸ“Œ ã‚¹ãƒ†ãƒƒãƒ—1: Webç”»åƒã‚’åˆ†æ")
    web_image_path = "screenshots/web_page1.png"  # é©åˆ‡ãªãƒ‘ã‚¹ã«å¤‰æ›´
    web_areas = analyzer.analyze_image(
        image_path=web_image_path,
        source_type="web",
        source_id="https://example.com/page1"
    )
    print(f"   âœ… {len(web_areas)} ã‚¨ãƒªã‚¢æ¤œå‡º")
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: PDFç”»åƒã‚’åˆ†æ
    print("\nğŸ“Œ ã‚¹ãƒ†ãƒƒãƒ—2: PDFç”»åƒã‚’åˆ†æ")
    pdf_image_path = "pdf_previews/document_page1.png"  # é©åˆ‡ãªãƒ‘ã‚¹ã«å¤‰æ›´
    pdf_areas = analyzer.analyze_image(
        image_path=pdf_image_path,
        source_type="pdf",
        source_id="document.pdf",
        page_num=1
    )
    print(f"   âœ… {len(pdf_areas)} ã‚¨ãƒªã‚¢æ¤œå‡º")
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: è‡ªå‹•ãƒãƒƒãƒãƒ³ã‚°
    print("\nğŸ“Œ ã‚¹ãƒ†ãƒƒãƒ—3: è‡ªå‹•ãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ")
    pairs = analyzer.compute_auto_matches(
        threshold=0.3,
        method="hybrid"
    )
    
    print(f"\nğŸ¯ ãƒãƒƒãƒãƒ³ã‚°çµæœ:")
    for i, pair in enumerate(pairs[:5], 1):  # æœ€åˆã®5ãƒšã‚¢ã®ã¿è¡¨ç¤º
        print(f"\n  ãƒšã‚¢ {i}:")
        print(f"    Web: {pair.web_area.text[:30]}...")
        print(f"    PDF: {pair.pdf_area.text[:30]}...")
        print(f"    é¡ä¼¼åº¦: {pair.similarity_score:.2%}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—4: çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    print("\nğŸ“Š çµ±è¨ˆæƒ…å ±:")
    stats = analyzer.get_statistics()
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.2%}")
        else:
            print(f"   {key}: {value}")


def example_manual_pairing():
    """æ‰‹å‹•ãƒšã‚¢ãƒªãƒ³ã‚°ã®ä¾‹"""
    print("\n" + "=" * 60)
    print("ğŸ–ï¸ ä¾‹3: æ‰‹å‹•ãƒšã‚¢ãƒªãƒ³ã‚°")
    print("=" * 60)
    
    # Analyzerã‚’ä½œæˆï¼ˆOCRãªã—ï¼‰
    analyzer = ContentAnalyzer()
    
    # ä»®ã®ã‚¨ãƒªã‚¢ã‚’ä½œæˆ
    web_area = DetectedArea(
        text="ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ1",
        bbox=[100, 100, 500, 200],
        confidence=0.95,
        source_type="web",
        source_id="https://example.com"
    )
    
    pdf_area = DetectedArea(
        text="ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ1",
        bbox=[120, 110, 520, 210],
        confidence=0.92,
        source_type="pdf",
        source_id="document.pdf",
        page_num=1
    )
    
    # æ‰‹å‹•ã§ãƒšã‚¢ãƒªãƒ³ã‚°
    pair = analyzer.add_manual_pair(web_area, pdf_area)
    
    print(f"\nâœ… ãƒšã‚¢ãƒªãƒ³ã‚°å®Œäº†")
    print(f"   é¡ä¼¼åº¦: {pair.similarity_score:.2%}")
    print(f"   ã‚¿ã‚¤ãƒ—: {pair.match_type}")


def example_text_difference():
    """ãƒ†ã‚­ã‚¹ãƒˆå·®åˆ†æ¤œå‡ºã®ä¾‹"""
    print("\n" + "=" * 60)
    print("ğŸ” ä¾‹4: ãƒ†ã‚­ã‚¹ãƒˆå·®åˆ†æ¤œå‡º")
    print("=" * 60)
    
    analyzer = ContentAnalyzer()
    
    text1 = """ã“ã‚Œã¯æœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚
2è¡Œç›®ã®å†…å®¹ã€‚
3è¡Œç›®ã®å†…å®¹ã€‚"""
    
    text2 = """ã“ã‚Œã¯å¤‰æ›´å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚
2è¡Œç›®ã®å†…å®¹ã€‚
4è¡Œç›®ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚"""
    
    differences = analyzer.find_differences(text1, text2)
    
    print("\nğŸ“ å·®åˆ†:")
    for diff in differences:
        symbol = "+" if diff["type"] == "add" else "-"
        print(f"   {symbol} {diff['text']}")


if __name__ == "__main__":
    print("\nğŸš€ ContentAnalyzer & OCREngine ä½¿ç”¨ä¾‹\n")
    
    # æ³¨æ„: credentials.json ãŒå¿…è¦ã§ã™
    print("âš ï¸ æ³¨æ„: ã“ã®ä¾‹ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã«")
    print("   credentials.json ã‚’é…ç½®ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n")
    
    # å„ä¾‹ã‚’å®Ÿè¡Œï¼ˆå®Ÿéš›ã®ç”»åƒãƒ‘ã‚¹ã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„ï¼‰
    try:
        # example_basic_ocr()
        # example_analyzer_workflow()
        example_manual_pairing()
        example_text_difference()
        
        print("\nâœ… å…¨ã¦ã®ä¾‹ãŒå®Œäº†ã—ã¾ã—ãŸ")
        
    except Exception as e:
        print(f"\nâš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        import traceback
        traceback.print_exc()

