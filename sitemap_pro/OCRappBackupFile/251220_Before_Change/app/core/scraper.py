from playwright.sync_api import sync_playwright
from PIL import Image, ImageDraw, ImageFont
from typing import List, Dict, Optional, Set
from urllib.parse import urlparse, urljoin
import os
import json
import io
import time
import re

# å·¨å¤§ãªç”»åƒã‚’è¨±å¯
Image.MAX_IMAGE_PIXELS = None

class WebScraper:
    def __init__(self, auth_file="auth.json"):
        self.auth_file = auth_file
        self.visited_urls: Set[str] = set()
        self.failed_urls: Set[str] = set()

    def interactive_login(self, url, wait_callback):
        """æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ç”¨"""
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False)
            context = browser.new_context()
            page = context.new_page()
            
            print(f"ğŸ”µ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ã—ã¾ã—ãŸ: {url}")
            try:
                page.goto(url, timeout=60000)
            except Exception as e:
                print(f"âš ï¸ ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿è­¦å‘Š: {e}")

            wait_callback()
            self.save_session(context)

    def save_session(self, context):
        try:
            context.storage_state(path=self.auth_file)
            print(f"âœ… Cookieæƒ…å ±ã‚’ {self.auth_file} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±æ•—: {e}")

    def _auto_scroll(self, page):
        """Lazy Loadingå¯¾ç­–: å°‘ã—ãšã¤ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦èª­ã¿è¾¼ã¾ã›ã‚‹"""
        print("â¬ ç”»åƒèª­ã¿è¾¼ã¿ã®ãŸã‚ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­...")
        page.evaluate("""
            async () => {
                await new Promise((resolve) => {
                    var totalHeight = 0;
                    var distance = 200; // ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¹…
                    var timer = setInterval(() => {
                        var scrollHeight = document.body.scrollHeight;
                        window.scrollBy(0, distance);
                        totalHeight += distance;

                        if(totalHeight >= scrollHeight - window.innerHeight){
                            clearInterval(timer);
                            resolve();
                        }
                    }, 50); // ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«é€Ÿåº¦
                });
            }
        """)
        # èª­ã¿è¾¼ã¿å¾…ã¡
        time.sleep(2)
        # ä¸€ç•ªä¸Šã«æˆ»ã™
        page.evaluate("window.scrollTo(0, 0)")
        time.sleep(1)

    def fetch_text(self, url, username=None, password=None):
        """
        ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—
        - å…¨ä½“ç”»åƒ: å®¹é‡ã‚ªãƒ¼ãƒãƒ¼é˜²æ­¢ã®ãŸã‚ 1.5å€ç”»è³ª
        - 1ç”»é¢ç”»åƒ: ç²¾å¯†OCRã®ãŸã‚ 3.0å€ç”»è³ª
        """
        with sync_playwright() as p:
            context_options = {}
            if os.path.exists(self.auth_file):
                try:
                    with open(self.auth_file, 'r') as f: json.load(f)
                    context_options['storage_state'] = self.auth_file
                except: pass

            if username and password:
                context_options['http_credentials'] = {
                    'username': username,
                    'password': password
                }

            # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
            browser = p.chromium.launch(headless=True)
            
            # --- 1. é«˜ç”»è³ªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (OCRç”¨ãƒ»1ç”»é¢åˆ†) ---
            # device_scale_factor=3.0 ã§ãã£ãã‚Šæ’®å½±
            context_high = browser.new_context(
                **context_options, 
                viewport={'width': 1280, 'height': 800},
                device_scale_factor=3.0 
            )
            page_high = context_high.new_page()
            
            # context_fullã‚’Noneã§åˆæœŸåŒ–ï¼ˆfinallyãƒ–ãƒ­ãƒƒã‚¯ã§ã®ã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰
            context_full = None
            
            try:
                print(f"ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
                page_high.goto(url, timeout=60000)
                page_high.wait_for_load_state("networkidle")

                # HTMLãƒ†ã‚­ã‚¹ãƒˆå–å¾—
                text_content = page_high.inner_text("body")
                title = page_high.title()

                # [é«˜ç”»è³ª] 1ç”»é¢åˆ†ã®ã‚¹ã‚¯ã‚·ãƒ§
                view_bytes = page_high.screenshot(full_page=False)
                
                # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
                if view_bytes and len(view_bytes) > 0:
                    try:
                        img_view = Image.open(io.BytesIO(view_bytes))
                    except Exception as e:
                        print(f"âš ï¸ ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        img_view = self._create_placeholder_image("1ç”»é¢ç”»åƒå–å¾—å¤±æ•—")
                else:
                    print(f"âš ï¸ ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                    img_view = self._create_placeholder_image("1ç”»é¢ç”»åƒå–å¾—å¤±æ•—")
                
                # --- 2. æ¨™æº–ç”»è³ªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (å…¨ä½“è¡¨ç¤ºç”¨) ---
                # å…¨ä½“ã‚¹ã‚¯ã‚·ãƒ§ã¯é•·ããªã‚Šã™ãã‚‹ã®ã§ device_scale_factor=1.5 ã«æŠ‘ãˆã‚‹
                # ã“ã‚Œã§ã€Œé€”åˆ‡ã‚Œã€ã‚’é˜²ã
                context_full = browser.new_context(
                    **context_options, 
                    viewport={'width': 1280, 'height': 800},
                    device_scale_factor=1.5
                )
                page_full = context_full.new_page()
                page_full.goto(url, timeout=60000) # åŒã˜ãƒšãƒ¼ã‚¸ã‚’é–‹ã
                page_full.wait_for_load_state("networkidle")

                # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦å…¨èª­ã¿è¾¼ã¿
                self._auto_scroll(page_full)

                # [ä¸­ç”»è³ª] å…¨ä½“ã‚¹ã‚¯ã‚·ãƒ§
                full_bytes = page_full.screenshot(full_page=True)
                
                # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
                if full_bytes and len(full_bytes) > 0:
                    try:
                        img_full = Image.open(io.BytesIO(full_bytes))
                    except Exception as e:
                        print(f"âš ï¸ ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        img_full = self._create_placeholder_image("å…¨ä½“ç”»åƒå–å¾—å¤±æ•—")
                else:
                    print(f"âš ï¸ ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                    img_full = self._create_placeholder_image("å…¨ä½“ç”»åƒå–å¾—å¤±æ•—")
                
                return title, text_content, img_full, img_view

            except Exception as e:
                raise Exception(f"å–å¾—å¤±æ•—: {str(e)}")
            finally:
                if context_high:
                    context_high.close()
                if context_full:
                    context_full.close()
                if browser:
                    browser.close()
    
    def crawl_site(
        self,
        base_url: str,
        max_pages: int = 50,
        max_depth: int = 3,
        same_domain_only: bool = True,
        username: Optional[str] = None,
        password: Optional[str] = None,
        progress_callback: Optional[callable] = None
    ) -> List[Dict]:
        """
        ã‚µã‚¤ãƒˆå†…ã‚’ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ï¼ˆä¸‹å±¤ãƒšãƒ¼ã‚¸ã‚‚å–å¾—ï¼‰
        
        Args:
            base_url: é–‹å§‹URL
            max_pages: æœ€å¤§å–å¾—ãƒšãƒ¼ã‚¸æ•°
            max_depth: æœ€å¤§æ·±ã•
            same_domain_only: åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿
            username: Basicèªè¨¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å
            password: Basicèªè¨¼ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
        
        Returns:
            [{"url": str, "title": str, "text": str, "full_image": Image, "viewport_image": Image, "error": str}, ...]
        """
        self.visited_urls = set()
        self.failed_urls = set()
        
        base_domain = urlparse(base_url).netloc
        to_visit = [(base_url, 0)]  # (url, depth)
        results = []
        
        print(f"ğŸŒ ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°é–‹å§‹: {base_url}")
        print(f"ğŸ“‹ è¨­å®š: æœ€å¤§{max_pages}ãƒšãƒ¼ã‚¸, æ·±ã•{max_depth}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¨­å®š
            context_options = {
                'viewport': {'width': 1280, 'height': 800},
                'device_scale_factor': 2.0
            }
            
            # èªè¨¼æƒ…å ±
            if os.path.exists(self.auth_file):
                try:
                    with open(self.auth_file, 'r') as f:
                        json.load(f)
                    context_options['storage_state'] = self.auth_file
                except:
                    pass
            
            if username and password:
                context_options['http_credentials'] = {
                    'username': username,
                    'password': password
                }
            
            context = browser.new_context(**context_options)
            page = context.new_page()
            
            try:
                while to_visit and len(results) < max_pages:
                    current_url, depth = to_visit.pop(0)
                    
                    # è¨ªå•æ¸ˆã¿ãƒã‚§ãƒƒã‚¯
                    if current_url in self.visited_urls or current_url in self.failed_urls:
                        continue
                    
                    # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
                    if same_domain_only:
                        current_domain = urlparse(current_url).netloc
                        if current_domain != base_domain:
                            continue
                    
                    # æ·±ã•ãƒã‚§ãƒƒã‚¯
                    if depth > max_depth:
                        continue
                    
                    try:
                        print(f"ğŸ“„ [{len(results) + 1}/{max_pages}] æ·±ã•{depth}: {current_url}")
                        
                        # é€²æ—é€šçŸ¥
                        if progress_callback:
                            progress_callback(len(results) + 1, max_pages, current_url)
                        
                        # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
                        page.goto(current_url, timeout=60000)
                        page.wait_for_load_state("networkidle")
                        
                        # Lazy Loadingå¯¾å¿œã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                        self._auto_scroll(page)
                        
                        # ãƒ‡ãƒ¼ã‚¿å–å¾—
                        title = page.title()
                        text_content = page.inner_text("body")
                        
                        # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼ˆ1ç”»é¢åˆ†ï¼‰
                        page.evaluate("window.scrollTo(0, 0)")
                        time.sleep(0.3)
                        viewport_bytes = page.screenshot(full_page=False)
                        
                        # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
                        if viewport_bytes and len(viewport_bytes) > 0:
                            try:
                                viewport_image = Image.open(io.BytesIO(viewport_bytes))
                            except Exception as e:
                                print(f"âš ï¸ 1ç”»é¢ç”»åƒå¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
                                viewport_image = self._create_placeholder_image("1ç”»é¢ç”»åƒå–å¾—å¤±æ•—")
                        else:
                            viewport_image = self._create_placeholder_image("1ç”»é¢ç”»åƒå–å¾—å¤±æ•—")
                        
                        # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼ˆãƒ•ãƒ«ãƒšãƒ¼ã‚¸ï¼‰
                        full_bytes = page.screenshot(full_page=True)
                        
                        # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
                        if full_bytes and len(full_bytes) > 0:
                            try:
                                full_image = Image.open(io.BytesIO(full_bytes))
                            except Exception as e:
                                print(f"âš ï¸ å…¨ä½“ç”»åƒå¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
                                full_image = self._create_placeholder_image("å…¨ä½“ç”»åƒå–å¾—å¤±æ•—")
                        else:
                            full_image = self._create_placeholder_image("å…¨ä½“ç”»åƒå–å¾—å¤±æ•—")
                        
                        results.append({
                            "url": current_url,
                            "title": title,
                            "text": text_content,
                            "full_image": full_image,
                            "viewport_image": viewport_image,
                            "depth": depth,
                            "error": None
                        })
                        
                        self.visited_urls.add(current_url)
                        
                        # æ¬¡ã®æ·±ã•ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆæ·±ã•åˆ¶é™å†…ã®å ´åˆã®ã¿ï¼‰
                        if depth < max_depth:
                            links = self._extract_links(page, current_url)
                            for link in links:
                                # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆé™¤å»
                                link = link.split('#')[0]
                                if link and link not in self.visited_urls and link not in self.failed_urls:
                                    # åŒã˜æ·±ã•ã®ãƒªãƒ³ã‚¯ã¯ã‚­ãƒ¥ãƒ¼ã®å¾Œã‚ã«è¿½åŠ 
                                    to_visit.append((link, depth + 1))
                        
                        time.sleep(1.0)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
                        
                    except Exception as e:
                        error_msg = str(e)
                        print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {current_url} - {error_msg}")
                        self.failed_urls.add(current_url)
                        
                        # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¨˜éŒ²ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”»åƒã‚’ä½¿ç”¨ï¼‰
                        error_placeholder = self._create_placeholder_image(f"å–å¾—å¤±æ•—\n{error_msg[:30]}...")
                        
                        results.append({
                            "url": current_url,
                            "title": f"å–å¾—å¤±æ•—: {current_url}",
                            "text": "",
                            "full_image": error_placeholder,
                            "viewport_image": error_placeholder,
                            "depth": depth,
                            "error": error_msg
                        })
                        continue
                
            finally:
                context.close()
                browser.close()
        
        print(f"âœ… ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Œäº†: {len(results)}ãƒšãƒ¼ã‚¸å–å¾—")
        return results
    
    def _create_placeholder_image(self, message: str = "ç”»åƒãªã—", width: int = 1280, height: int = 800) -> Image.Image:
        """
        ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”»åƒã‚’ä½œæˆ
        
        Args:
            message: è¡¨ç¤ºã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            width: ç”»åƒå¹…
            height: ç”»åƒé«˜ã•
        
        Returns:
            PIL Image
        """
        # ã‚°ãƒ¬ãƒ¼ã®èƒŒæ™¯ç”»åƒã‚’ä½œæˆ
        img = Image.new('RGB', (width, height), color='#2B2B2B')
        draw = ImageDraw.Draw(img)
        
        # ä¸­å¤®ã«èµ¤ã„æ ã‚’æç”»
        margin = 50
        draw.rectangle(
            [margin, margin, width - margin, height - margin],
            outline='#FF4444',
            width=5
        )
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æç”»ï¼ˆãƒ•ã‚©ãƒ³ãƒˆãªã—ã§ã‚·ãƒ³ãƒ—ãƒ«ã«ï¼‰
        text = f"âš ï¸ {message}"
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’å–å¾—ï¼ˆç°¡æ˜“è¨ˆç®—ï¼‰
        text_width = len(text) * 10
        text_height = 20
        text_x = (width - text_width) // 2
        text_y = (height - text_height) // 2
        
        draw.text((text_x, text_y), text, fill='#FF4444')
        
        return img
    
    def _extract_links(self, page, base_url: str) -> List[str]:
        """
        ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ‰åŠ¹ãªãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        
        Args:
            page: Playwrightã®ãƒšãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            base_url: ãƒ™ãƒ¼ã‚¹URL
        
        Returns:
            ãƒªãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """
        try:
            links = page.evaluate("""
                () => {
                    return Array.from(document.querySelectorAll('a[href]'))
                        .map(a => a.href)
                        .filter(href => href.startsWith('http'));
                }
            """)
            
            # çµ¶å¯¾URLã«å¤‰æ›
            absolute_links = []
            for link in links:
                absolute_url = urljoin(base_url, link)
                
                # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç”»åƒã€CSSã€JSãªã©ï¼‰
                if re.search(r'\.(jpg|jpeg|png|gif|css|js|pdf|zip)$', absolute_url, re.IGNORECASE):
                    continue
                
                absolute_links.append(absolute_url)
            
            return absolute_links
            
        except Exception as e:
            print(f"âš ï¸ ãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []