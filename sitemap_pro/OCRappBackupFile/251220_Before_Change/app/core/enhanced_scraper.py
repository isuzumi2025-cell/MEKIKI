"""
Phase 1: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ã®å¼·åŒ–
é…å»¶èª­ã¿è¾¼ã¿ï¼ˆLazy Loadingï¼‰å¯¾å¿œ
"""
from playwright.sync_api import sync_playwright, Page
from PIL import Image
from typing import List, Dict, Optional, Tuple
import io
import time


class EnhancedWebScraper:
    """å¼·åŒ–ç‰ˆWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ - ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¨Lazy Loadingå¯¾å¿œ"""
    
    def __init__(self, headless: bool = True, viewport_width: int = 1280, viewport_height: int = 800):
        """
        Args:
            headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹ã‹
            viewport_width: ãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ãƒˆå¹…
            viewport_height: ãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ãƒˆé«˜ã•
        """
        self.headless = headless
        self.viewport_width = viewport_width
        self.viewport_height = viewport_height
    
    def scrape_with_lazy_loading(
        self,
        url: str,
        username: Optional[str] = None,
        password: Optional[str] = None,
        max_scrolls: int = 10,
        scroll_delay: float = 1.0
    ) -> Tuple[str, str, Image.Image, Image.Image]:
        """
        é…å»¶èª­ã¿è¾¼ã¿å¯¾å¿œã§Webãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        
        Args:
            url: å¯¾è±¡URL
            username: Basicèªè¨¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å
            password: Basicèªè¨¼ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
            max_scrolls: æœ€å¤§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°
            scroll_delay: ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«é–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
        
        Returns:
            (title, text, full_page_image, viewport_image)
        """
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=self.headless)
            
            context_options = {
                'viewport': {'width': self.viewport_width, 'height': self.viewport_height},
                'device_scale_factor': 2.0
            }
            
            # Basicèªè¨¼è¨­å®š
            if username and password:
                context_options['http_credentials'] = {
                    'username': username,
                    'password': password
                }
            
            context = browser.new_context(**context_options)
            page = context.new_page()
            
            try:
                print(f"ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
                page.goto(url, timeout=60000)
                page.wait_for_load_state("networkidle")
                
                # ãƒšãƒ¼ã‚¸ä¸‹éƒ¨ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆLazy Loadingå¯¾å¿œï¼‰
                self._scroll_to_bottom(page, max_scrolls, scroll_delay)
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
                title = page.title()
                text_content = page.inner_text("body")
                
                # 1ç”»é¢åˆ†ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
                page.evaluate("window.scrollTo(0, 0)")  # ãƒˆãƒƒãƒ—ã«æˆ»ã‚‹
                time.sleep(0.5)
                viewport_bytes = page.screenshot(full_page=False)
                viewport_image = Image.open(io.BytesIO(viewport_bytes))
                
                # ãƒ•ãƒ«ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
                full_bytes = page.screenshot(full_page=True)
                full_image = Image.open(io.BytesIO(full_bytes))
                
                print(f"âœ… å–å¾—å®Œäº†: {title}")
                return title, text_content, full_image, viewport_image
                
            except Exception as e:
                raise Exception(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {str(e)}")
            finally:
                context.close()
                browser.close()
    
    def _scroll_to_bottom(self, page: Page, max_scrolls: int, delay: float):
        """
        ãƒšãƒ¼ã‚¸ä¸‹éƒ¨ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆé…å»¶èª­ã¿è¾¼ã¿å¯¾å¿œï¼‰
        
        Args:
            page: Playwrightã®ãƒšãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            max_scrolls: æœ€å¤§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°
            delay: ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«é–“ã®å¾…æ©Ÿæ™‚é–“
        """
        print(f"ğŸ“œ ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­...")
        
        for i in range(max_scrolls):
            # ç¾åœ¨ã®é«˜ã•ã‚’å–å¾—
            prev_height = page.evaluate("document.body.scrollHeight")
            
            # ä¸‹ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(delay)
            
            # æ–°ã—ã„é«˜ã•ã‚’å–å¾—
            new_height = page.evaluate("document.body.scrollHeight")
            
            # é«˜ã•ãŒå¤‰ã‚ã‚‰ãªã‘ã‚Œã°çµ‚äº†ï¼ˆã“ã‚Œä»¥ä¸Šèª­ã¿è¾¼ã‚€ã‚‚ã®ãŒãªã„ï¼‰
            if new_height == prev_height:
                print(f"âœ… ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†ï¼ˆ{i + 1}å›ï¼‰")
                break
        else:
            print(f"âœ… æœ€å¤§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°ã«åˆ°é”ï¼ˆ{max_scrolls}å›ï¼‰")
    
    def crawl_site(
        self,
        base_url: str,
        max_pages: int = 50,
        same_domain_only: bool = True,
        username: Optional[str] = None,
        password: Optional[str] = None
    ) -> List[Dict]:
        """
        ã‚µã‚¤ãƒˆå†…ã‚’ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°
        
        Args:
            base_url: é–‹å§‹URL
            max_pages: æœ€å¤§ãƒšãƒ¼ã‚¸æ•°
            same_domain_only: åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿
            username: Basicèªè¨¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å
            password: Basicèªè¨¼ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
        
        Returns:
            [{"url": str, "title": str, "text": str, "full_image": Image, "viewport_image": Image}, ...]
        """
        from urllib.parse import urlparse, urljoin
        import re
        
        visited = set()
        to_visit = [base_url]
        results = []
        base_domain = urlparse(base_url).netloc
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=self.headless)
            
            context_options = {
                'viewport': {'width': self.viewport_width, 'height': self.viewport_height},
                'device_scale_factor': 2.0
            }
            
            if username and password:
                context_options['http_credentials'] = {
                    'username': username,
                    'password': password
                }
            
            context = browser.new_context(**context_options)
            page = context.new_page()
            
            try:
                while to_visit and len(results) < max_pages:
                    current_url = to_visit.pop(0)
                    
                    if current_url in visited:
                        continue
                    
                    # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
                    if same_domain_only:
                        current_domain = urlparse(current_url).netloc
                        if current_domain != base_domain:
                            continue
                    
                    try:
                        print(f"ğŸŒ [{len(results) + 1}/{max_pages}] {current_url}")
                        
                        page.goto(current_url, timeout=60000)
                        page.wait_for_load_state("networkidle")
                        
                        # Lazy Loadingå¯¾å¿œã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                        self._scroll_to_bottom(page, max_scrolls=5, delay=0.5)
                        
                        # ãƒ‡ãƒ¼ã‚¿å–å¾—
                        title = page.title()
                        text_content = page.inner_text("body")
                        
                        # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
                        page.evaluate("window.scrollTo(0, 0)")
                        time.sleep(0.3)
                        viewport_bytes = page.screenshot(full_page=False)
                        viewport_image = Image.open(io.BytesIO(viewport_bytes))
                        
                        full_bytes = page.screenshot(full_page=True)
                        full_image = Image.open(io.BytesIO(full_bytes))
                        
                        results.append({
                            "url": current_url,
                            "title": title,
                            "text": text_content,
                            "full_image": full_image,
                            "viewport_image": viewport_image
                        })
                        
                        visited.add(current_url)
                        
                        # ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                        links = page.evaluate("""
                            () => {
                                return Array.from(document.querySelectorAll('a[href]'))
                                    .map(a => a.href)
                                    .filter(href => href.startsWith('http'));
                            }
                        """)
                        
                        for link in links:
                            # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆé™¤å»
                            link = link.split('#')[0]
                            if link and link not in visited and link not in to_visit:
                                to_visit.append(link)
                        
                        time.sleep(1.0)  # è² è·è»½æ¸›
                        
                    except Exception as e:
                        print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {current_url} - {str(e)}")
                        visited.add(current_url)
                        continue
                
            finally:
                context.close()
                browser.close()
        
        print(f"âœ… ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Œäº†: {len(results)}ãƒšãƒ¼ã‚¸å–å¾—")
        return results

