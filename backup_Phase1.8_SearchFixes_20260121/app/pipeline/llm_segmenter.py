"""
Phase 5: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ç”Ÿæˆ

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
1. å…¨æ–‡æŠ½å‡ºï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãªã—ï¼‰
2. å…¨æ–‡æ¯”è¼ƒ â†’ ãƒãƒƒãƒç®‡æ‰€æ¤œå‡º
3. LLMï¼ˆç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆ+ãƒãƒƒãƒæƒ…å ±ï¼‰â†’ ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ç”Ÿæˆ
4. LiveComparisonSheetã«åæ˜ 
"""

import json
import re
import os
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, field
from pathlib import Path
import google.generativeai as genai
from PIL import Image
from config import Config
try:
    from app.config import get_match_config
except ImportError:
    get_match_config = None


@dataclass
class LLMParagraph:
    """LLMç”Ÿæˆãƒ‘ãƒ©ã‚°ãƒ©ãƒ•"""
    id: str
    web_text: str
    pdf_text: str
    sync_score: float
    common_anchor: str = ""


class MultimodalLLMSegmenter:
    """
    ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ç”Ÿæˆå™¨
    
    ç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆ+ãƒãƒƒãƒæƒ…å ±ã‹ã‚‰ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
    """
    
    def __init__(self):
        self.model = None
        if Config.GEMINI_API_KEY:
            try:
                genai.configure(api_key=Config.GEMINI_API_KEY)
                self.model = genai.GenerativeModel("gemini-2.0-flash")
                print("âœ… Multimodal LLM Segmenter initialized")
            except Exception as e:
                print(f"âŒ LLM init error: {e}")
    
    def generate_paragraphs(
        self,
        web_image: Image.Image,
        pdf_image: Image.Image,
        web_full_text: str,
        pdf_full_text: str,
        match_segments: List[Dict]
    ) -> List[LLMParagraph]:
        """
        ç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆ+ãƒãƒƒãƒæƒ…å ±ã‹ã‚‰ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
        
        Args:
            web_image: Webã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
            pdf_image: PDFãƒšãƒ¼ã‚¸ç”»åƒ
            web_full_text: Webå…¨æ–‡ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãªã—ï¼‰
            pdf_full_text: PDFå…¨æ–‡ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãªã—ï¼‰
            match_segments: å…±é€šéƒ¨åˆ†ãƒªã‚¹ãƒˆ [{common, web_context, pdf_context}, ...]
        
        Returns:
            List[LLMParagraph]: ç”Ÿæˆã•ã‚ŒãŸãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ãƒšã‚¢
        """
        if not self.model:
            print("âš ï¸ LLM not available, using fallback")
            return self._fallback_paragraphs(match_segments, web_full_text, pdf_full_text)
        
        print("ğŸ§  ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ç”Ÿæˆé–‹å§‹...")
        
        # ãƒãƒƒãƒæƒ…å ±ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
        match_info = self._format_match_info(match_segments)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        prompt = f"""ã‚ãªãŸã¯ãƒ†ã‚­ã‚¹ãƒˆæ¯”è¼ƒã®å°‚é–€å®¶ã§ã™ã€‚

## ã‚¿ã‚¹ã‚¯
Webç”»åƒã¨PDFç”»åƒã‚’è¦‹ã¦ã€ä¸¡æ–¹ã«å…±é€šã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ç‰¹å®šã—ã€
è«–ç†çš„ãªãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ãƒšã‚¢ã«åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚

## å…±é€šãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ï¼ˆã™ã§ã«æ¤œå‡ºæ¸ˆã¿ï¼‰
{match_info}

## Webå…¨æ–‡ï¼ˆå‚è€ƒï¼‰
{web_full_text[:2000]}

## PDFå…¨æ–‡ï¼ˆå‚è€ƒï¼‰
{pdf_full_text[:2000]}

## å‡ºåŠ›å½¢å¼
ä»¥ä¸‹ã®JSONé…åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚ä»–ã®èª¬æ˜ã¯ä¸è¦ã§ã™ï¼š
[
  {{"web": "Webã®ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•1", "pdf": "PDFã®ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•1", "anchor": "å…±é€šã‚¢ãƒ³ã‚«ãƒ¼æ–‡å­—åˆ—"}},
  ...
]

æœ€å¤§20ãƒšã‚¢ã¾ã§ã€‚è¦–è¦šçš„ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’è€ƒæ…®ã—ã¦åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            import io
            import base64
            
            # ç”»åƒã‚’ãƒã‚¤ãƒˆã«å¤‰æ›ï¼ˆGemini APIç”¨ï¼‰
            def image_to_part(img: Image.Image):
                """PIL Imageã‚’Gemini Partå½¢å¼ã«å¤‰æ›"""
                img_byte_arr = io.BytesIO()
                # RGBã«å¤‰æ›
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                # ãƒªã‚µã‚¤ã‚ºï¼ˆå¤§ãã™ãã‚‹å ´åˆï¼‰
                max_dim = 1024
                if img.width > max_dim or img.height > max_dim:
                    scale = min(max_dim / img.width, max_dim / img.height)
                    img = img.resize((int(img.width * scale), int(img.height * scale)))
                img.save(img_byte_arr, format='JPEG', quality=85)
                return {
                    "mime_type": "image/jpeg",
                    "data": base64.b64encode(img_byte_arr.getvalue()).decode()
                }
            
            web_part = image_to_part(web_image)
            pdf_part = image_to_part(pdf_image)
            
            # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ï¼ˆç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆï¼‰
            response = self.model.generate_content([
                prompt,
                web_part,
                pdf_part
            ])
            
            if not response.text:
                print("   âš ï¸ LLM returned empty response, using fallback")
                return self._fallback_paragraphs(match_segments, web_full_text, pdf_full_text)
            
            print(f"   ğŸ“ LLMå¿œç­”: {len(response.text)} chars")
            paragraphs = self._parse_llm_response(response.text)
            
            # ãƒ‘ãƒ¼ã‚¹å¤±æ•—ã¾ãŸã¯ç©ºã®å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not paragraphs:
                print("   âš ï¸ Parse returned 0 paragraphs, using fallback")
                return self._fallback_paragraphs(match_segments, web_full_text, pdf_full_text)
            
            print(f"   âœ… LLMç”Ÿæˆå®Œäº†: {len(paragraphs)}ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•")
            return paragraphs
            
        except Exception as e:
            print(f"   âŒ LLM error: {e}")
            return self._fallback_paragraphs(match_segments, web_full_text, pdf_full_text)
    
    def _format_match_info(self, match_segments: List[Dict]) -> str:
        """ãƒãƒƒãƒæƒ…å ±ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if not match_segments:
            return "ï¼ˆå…±é€šãƒ†ã‚­ã‚¹ãƒˆãªã—ï¼‰"
        
        lines = []
        for i, m in enumerate(match_segments[:15]):
            common = m.get('common', m.get('common_text', ''))[:50]
            lines.append(f"{i+1}. å…±é€š: ã€Œ{common}ã€")
        
        return "\n".join(lines)
    
    def _parse_llm_response(self, response: str) -> List[LLMParagraph]:
        """LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹"""
        try:
            # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯é™¤å»
            clean = response.strip()
            if clean.startswith("```"):
                clean = re.sub(r'^```\w*\n?', '', clean)
                clean = re.sub(r'\n?```$', '', clean)
            
            data = json.loads(clean)
            
            paragraphs = []
            for i, item in enumerate(data):
                if isinstance(item, dict):
                    paragraphs.append(LLMParagraph(
                        id=f"LP-{i+1:03d}",
                        web_text=item.get('web', '')[:300],
                        pdf_text=item.get('pdf', '')[:300],
                        sync_score=0.8,  # LLMç”Ÿæˆã¯é«˜ã‚·ãƒ³ã‚¯ãƒ­
                        common_anchor=item.get('anchor', '')[:50]
                    ))
            
            return paragraphs
            
        except Exception as e:
            print(f"   âš ï¸ Parse error: {e}")
            return []
    
    def _fallback_paragraphs(self, match_segments: List[Dict], web_text: str = "", pdf_text: str = "") -> List[LLMParagraph]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒãƒƒãƒã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¾ãŸã¯å…¨æ–‡ã‹ã‚‰ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•åŒ–"""
        print(f"   ğŸ“‹ Fallback: {len(match_segments)} matches, web={len(web_text)} chars, pdf={len(pdf_text)} chars")
        paragraphs = []
        
        def calc_similarity(text1: str, text2: str) -> float:
            """å®Ÿéš›ã®ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã‚’è¨ˆç®—"""
            if not text1 or not text2:
                return 0.0
            # å…±é€šæ–‡å­—æ•°ã«ã‚ˆã‚‹ç°¡æ˜“é¡ä¼¼åº¦
            t1, t2 = set(text1), set(text2)
            if not t1 or not t2:
                return 0.0
            common = len(t1 & t2)
            return common / max(len(t1), len(t2))
        
        # ãƒãƒƒãƒã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆ
        if match_segments:
            for i, m in enumerate(match_segments[:20]):
                w = m.get('web_text', m.get('web_context', ''))[:300]
                p = m.get('pdf_text', m.get('pdf_context', ''))[:300]
                paragraphs.append(LLMParagraph(
                    id=f"LP-{i+1:03d}",
                    web_text=w,
                    pdf_text=p,
                    sync_score=calc_similarity(w, p),  # å®Ÿéš›ã®é¡ä¼¼åº¦
                    common_anchor=m.get('common', m.get('common_text', ''))[:50]
                ))
        
        # ãƒãƒƒãƒã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ãŒä½œã‚Œãªã‹ã£ãŸå ´åˆã¯å…¨æ–‡ã‹ã‚‰
        if not paragraphs and (web_text or pdf_text):
            print("   ğŸ“‹ Using full text for fallback")
            web_paras = [p.strip() for p in web_text.split('\n') if len(p.strip()) > 10][:20]
            pdf_paras = [p.strip() for p in pdf_text.split('\n') if len(p.strip()) > 10][:20]
            
            max_len = max(len(web_paras), len(pdf_paras), 1)
            for i in range(min(max_len, 20)):
                w = web_paras[i] if i < len(web_paras) else ""
                p = pdf_paras[i] if i < len(pdf_paras) else ""
                if w or p:
                    paragraphs.append(LLMParagraph(
                        id=f"LP-{i+1:03d}",
                        web_text=w[:300],
                        pdf_text=p[:300],
                        sync_score=calc_similarity(w, p),  # å®Ÿéš›ã®é¡ä¼¼åº¦
                        common_anchor=""
                    ))
        
        print(f"   ğŸ“‹ Fallback generated: {len(paragraphs)} paragraphs")
        return paragraphs


def find_common_segments(web_text: str, pdf_text: str, min_length: int = None) -> List[Dict]:
    """
    å…¨æ–‡æ¯”è¼ƒã§å…±é€šã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ¤œå‡º
    
    Args:
        web_text: Webå…¨æ–‡
        pdf_text: PDFå…¨æ–‡
        min_length: æœ€å°å…±é€šæ–‡å­—æ•°
    
    Returns:
        List[Dict]: å…±é€šã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
    """
    # Configã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’å–å¾—
    if min_length is None:
        if get_match_config:
            min_length = get_match_config().min_match_length
        else:
            min_length = 8  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    if not web_text or not pdf_text:
        return []
    
    segments = []
    web_clean = web_text.replace('\n', ' ')
    pdf_clean = pdf_text.replace('\n', ' ')
    
    # æœ€é•·å…±é€šéƒ¨åˆ†æ–‡å­—åˆ—ã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    found_positions = set()
    
    for length in range(min(100, len(web_clean)), min_length - 1, -1):
        for i in range(len(web_clean) - length + 1):
            substring = web_clean[i:i+length]
            if substring.strip() and substring in pdf_clean:
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                is_duplicate = False
                for pos in found_positions:
                    if abs(pos - i) < length // 2:
                        is_duplicate = True
                        break
                
                if not is_duplicate:
                    found_positions.add(i)
                    
                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                    web_start = max(0, i - 50)
                    web_end = min(len(web_clean), i + length + 50)
                    pdf_pos = pdf_clean.find(substring)
                    pdf_start = max(0, pdf_pos - 50)
                    pdf_end = min(len(pdf_clean), pdf_pos + length + 50)
                    
                    segments.append({
                        'common': substring,
                        'common_len': length,
                        'web_context': web_clean[web_start:web_end],
                        'pdf_context': pdf_clean[pdf_start:pdf_end]
                    })
                    
                    if len(segments) >= 30:  # æœ€å¤§30ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
                        break
        
        if len(segments) >= 30:
            break
    
    # é•·ã„é †ã«ã‚½ãƒ¼ãƒˆ
    segments.sort(key=lambda x: x['common_len'], reverse=True)
    return segments


def run_phase5_pipeline(
    web_image: Image.Image,
    pdf_image: Image.Image
) -> Tuple[List[LLMParagraph], List[Dict]]:
    """
    Phase 5 å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    1. å…¨æ–‡æŠ½å‡º
    2. å…¨æ–‡æ¯”è¼ƒ
    3. LLMãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ç”Ÿæˆ
    
    Returns:
        (paragraphs, match_segments)
    """
    from app.core.engine_cloud import CloudOCREngine
    
    print("=" * 60)
    print("ğŸš€ Phase 5: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ç”Ÿæˆ")
    print("=" * 60)
    
    # Step 1: å…¨æ–‡æŠ½å‡º
    ocr = CloudOCREngine()
    print("\nğŸ“„ Step 1: å…¨æ–‡æŠ½å‡ºï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãªã—ï¼‰")
    web_full_text = ocr.extract_full_text(web_image)
    pdf_full_text = ocr.extract_full_text(pdf_image)
    
    print(f"   Web: {len(web_full_text)}æ–‡å­—")
    print(f"   PDF: {len(pdf_full_text)}æ–‡å­—")
    
    # Step 2: å…¨æ–‡æ¯”è¼ƒ
    print("\nğŸ” Step 2: å…¨æ–‡æ¯”è¼ƒ â†’ ãƒãƒƒãƒç®‡æ‰€æ¤œå‡º")
    match_segments = find_common_segments(web_full_text, pdf_full_text)
    print(f"   ãƒãƒƒãƒã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: {len(match_segments)}ä»¶")
    
    # Step 3: LLMãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ç”Ÿæˆ
    print("\nğŸ§  Step 3: LLMãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ç”Ÿæˆ")
    segmenter = MultimodalLLMSegmenter()
    paragraphs = segmenter.generate_paragraphs(
        web_image, pdf_image,
        web_full_text, pdf_full_text,
        match_segments
    )
    
    print(f"\nâœ… å®Œäº†: {len(paragraphs)}ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ç”Ÿæˆ")
    return paragraphs, match_segments
