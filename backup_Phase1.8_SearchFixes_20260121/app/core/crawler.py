"""
Webã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ï¼ˆé­”æ”¹é€ ç‰ˆï¼‰
ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªã‚µã‚¤ãƒˆãƒãƒƒãƒ—æ§‹ç¯‰ + ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åé›† + é™çš„ã‚¢ã‚»ãƒƒãƒˆé™¤å¤–
"""
from urllib.parse import urljoin, urlparse, unquote
from typing import List, Set, Optional, Callable, Dict, Tuple
# EnhancedWebScraperã¯__init__å†…ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (line 50)
import time
import re
import base64


class WebCrawler:
    """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»Webã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    # é™çš„ã‚¢ã‚»ãƒƒãƒˆã®æ‹¡å¼µå­ï¼ˆé™¤å¤–å¯¾è±¡ï¼‰
    STATIC_EXTENSIONS = {
        '.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp', '.ico', '.bmp',  # ç”»åƒ
        '.css', '.scss', '.sass', '.less',  # ã‚¹ã‚¿ã‚¤ãƒ«ã‚·ãƒ¼ãƒˆ
        '.js', '.jsx', '.ts', '.tsx', '.mjs',  # JavaScript
        '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        '.zip', '.tar', '.gz', '.rar', '.7z',  # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
        '.mp4', '.avi', '.mov', '.wmv', '.flv', '.mp3', '.wav',  # ãƒ¡ãƒ‡ã‚£ã‚¢
        '.xml', '.json', '.csv', '.txt', '.log'  # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
    }
    
    def __init__(
        self,
        max_pages: int = 50,
        max_depth: int = 5,
        delay: float = 0.5,  # 2.0s â†’ 0.5s çŸ­ç¸®ï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚0ã«ã¯ã—ãªã„ï¼‰
        username: Optional[str] = None,
        password: Optional[str] = None
    ):
        """
        Args:
            max_pages: æœ€å¤§ãƒšãƒ¼ã‚¸æ•°
            max_depth: æœ€å¤§æ·±ã•ï¼ˆ0=ãƒ«ãƒ¼ãƒˆã®ã¿ã€1=ãƒ«ãƒ¼ãƒˆ+1éšå±¤ã€2=ãƒ«ãƒ¼ãƒˆ+2éšå±¤...ï¼‰
            delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®é…å»¶ï¼ˆç§’ï¼‰
            username: Basicèªè¨¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å
            password: Basicèªè¨¼ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
        """
        self.max_pages = max_pages
        self.max_depth = max_depth
        self.delay = delay
        self.username = username
        self.password = password
        
        # EnhancedWebScraperã‚’ä½¿ç”¨ï¼ˆSmart Stitchingå¯¾å¿œï¼‰
        from app.core.enhanced_scraper import EnhancedWebScraper
        self.scraper = EnhancedWebScraper(headless=True)
        
        self.visited_urls: Set[str] = set()
        self.failed_urls: Set[str] = set()
        self.sitemap: Dict[str, Dict] = {}  # URL -> ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ—
    
    def _is_static_asset(self, url: str) -> bool:
        """
        URLãŒé™çš„ã‚¢ã‚»ãƒƒãƒˆã‹ã©ã†ã‹ã‚’åˆ¤å®š
        
        Args:
            url: åˆ¤å®šã™ã‚‹URL
        
        Returns:
            é™çš„ã‚¢ã‚»ãƒƒãƒˆã®å ´åˆTrue
        """
        parsed = urlparse(url)
        path = unquote(parsed.path.lower())
        
        # æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
        for ext in self.STATIC_EXTENSIONS:
            if path.endswith(ext):
                return True
        
        # ç‰¹å®šã®ãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–
        exclude_patterns = [
            r'/assets/',
            r'/static/',
            r'/images/',
            r'/img/',
            r'/css/',
            r'/js/',
            r'/fonts/',
            r'/media/',
            r'/download/'
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, path, re.IGNORECASE):
                return True
        
        return False
    
    def _normalize_url(self, url: str) -> str:
        """
        URLã‚’æ­£è¦åŒ–ï¼ˆãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆé™¤å»ã€æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥çµ±ä¸€ï¼‰
        âš ï¸ ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä¿æŒï¼ˆé‡è¦ãªãƒšãƒ¼ã‚¸è­˜åˆ¥å­ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ï¼‰
        
        Args:
            url: æ­£è¦åŒ–ã™ã‚‹URL
        
        Returns:
            æ­£è¦åŒ–ã•ã‚ŒãŸURL
        """
        # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
        url = url.split('#')[0]
        
        parsed = urlparse(url)
        
        # ã‚¹ã‚­ãƒ¼ãƒ ã€ãƒ›ã‚¹ãƒˆã€ãƒ‘ã‚¹ã€ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
        normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚Œã°è¿½åŠ 
        if parsed.query:
            normalized += f"?{parsed.query}"
        
        # æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’çµ±ä¸€ï¼ˆã‚ã‚‹å ´åˆã¯å‰Šé™¤ã€ãƒ«ãƒ¼ãƒˆã¯ä¿æŒï¼‰
        # ãŸã ã—ã€ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’æ°—ã«ã—ãªã„
        if not parsed.query and normalized.endswith('/') and len(parsed.path) > 1:
            normalized = normalized.rstrip('/')
        
        return normalized
    
    def crawl(
        self,
        root_url: str,
        progress_callback: Optional[Callable[[str, int, int], None]] = None
    ) -> List[dict]:
        """
        ãƒ«ãƒ¼ãƒˆURLã‹ã‚‰ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚’é–‹å§‹
        
        Args:
            root_url: é–‹å§‹URL
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ (url, current, total) -> None
        
        Returns:
            [{"url": str, "title": str, "text": str, "screenshot_path": str, "error": str or None}, ...]
        """
        print(f"\n{'='*60}")
        print(f"[Web] ğŸš€ Intelligent Crawling Start")
        print(f"  Root URL: {root_url}")
        print(f"  Max Depth: {self.max_depth}")
        print(f"  Max Pages: {self.max_pages}")
        print(f"{'='*60}\n")
        
        self.visited_urls.clear()
        self.failed_urls.clear()
        self.sitemap.clear()
        
        root_url = self._normalize_url(root_url)
        root_domain = urlparse(root_url).netloc
        results = []
        queue = [(root_url, 0, None)]  # (url, depth, parent_url)
        
        while queue and len(results) < self.max_pages:
            current_url, depth, parent_url = queue.pop(0)
            
            print(f"[Web] [Depth: {depth}] [Queue: {len(queue)}] {current_url}")
            
            # æ·±ã•ãƒã‚§ãƒƒã‚¯
            if depth > self.max_depth:
                print(f"[Web]   âŒ Skipped: Depth limit ({self.max_depth}) exceeded")
                continue
            
            # æ—¢ã«è¨ªå•æ¸ˆã¿
            if current_url in self.visited_urls:
                print(f"[Web]   â­ï¸  Skipped: Already visited")
                continue
            
            # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
            current_domain = urlparse(current_url).netloc
            if current_domain != root_domain:
                print(f"[Web]   âŒ Skipped: Different domain ({current_domain})")
                continue
            
            # é™çš„ã‚¢ã‚»ãƒƒãƒˆãƒã‚§ãƒƒã‚¯
            if self._is_static_asset(current_url):
                print(f"[Web]   ğŸš« Skipped: Static asset")
                continue
            
            try:
                # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
                if progress_callback:
                    progress_callback(current_url, len(results), self.max_pages)
                
                print(f"[Web] Fetching: {current_url}")
                
                # èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                if self.username and self.password:
                    print(f"[Web]   ğŸ” Basicèªè¨¼ä½¿ç”¨: {self.username}")
                
                # EnhancedWebScraperã‚’ä½¿ç”¨
                title, text, img_full, img_view = self.scraper.scrape_with_lazy_loading(
                    url=current_url,
                    username=self.username,
                    password=self.password
                )
                
                # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                screenshot_path = None  # å¿…è¦ã«å¿œã˜ã¦ä¿å­˜å‡¦ç†ã‚’è¿½åŠ 
                
                # æš«å®šç‰ˆ: ç”»åƒå…¨ä½“ã‚’1ã¤ã®ã‚¨ãƒªã‚¢ã¨ã—ã¦æ‰±ã†
                # å°†æ¥çš„ã«ã¯Playwrightã§è¦ç´ ã”ã¨ã®ä½ç½®ã‚’å–å¾—ã™ã‚‹å¯èƒ½æ€§ã‚ã‚Š
                # img_viewã®ã‚µã‚¤ã‚ºã‚’å–å¾—
                img_width, img_height = img_view.size if img_view else (1920, 1080)
                areas = [{
                    "text": text,
                    "bbox": [0, 0, img_width, img_height],
                    "area_id": 1
                }]
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
                metadata = {
                    "url": current_url,
                    "title": title,
                    "text": text,
                    "screenshot_path": screenshot_path,
                    "areas": areas,  # bboxä»˜ããƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸ
                    "screenshot_image": img_view,  # PIL Image (Viewport)
                    "full_image": img_full, # PIL Image (Stitched Full Page)
                    "error": None,  # æˆåŠŸæ™‚ã¯None
                    "depth": depth,  # éšå±¤ãƒ¬ãƒ™ãƒ«
                    "parent_url": parent_url,  # è¦ªURL
                    "fetch_time": time.time()  # å–å¾—æ™‚åˆ»
                }
                
                results.append(metadata)
                self.visited_urls.add(current_url)
                self.sitemap[current_url] = metadata
                
                print(f"[Web]   âœ… Success: {len(text)} chars, {len(title)} title")
                
                # ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¦ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆæ¬¡ã®æ·±ã•ï¼‰
                if depth < self.max_depth:
                    try:
                        links = self._extract_links_from_page(current_url)
                        print(f"[Web]   ğŸ”— Found {len(links)} raw links")
                        
                        added_count = 0
                        filtered_count = 0
                        invalid_count = 0
                        
                        for link in links:
                            # æ­£è¦åŒ–
                            try:
                                normalized_link = self._normalize_url(link)
                            except Exception as e:
                                print(f"[Web]   âš ï¸ URL normalization failed for {link}: {e}")
                                invalid_count += 1
                                continue
                            
                            # é™çš„ã‚¢ã‚»ãƒƒãƒˆé™¤å¤–
                            if self._is_static_asset(normalized_link):
                                filtered_count += 1
                                continue
                            
                            # URLã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ï¼‰
                            parsed_link = urlparse(normalized_link)
                            if not parsed_link.netloc or not parsed_link.scheme:
                                print(f"[Web]   âš ï¸ Invalid URL format: {normalized_link}")
                                invalid_count += 1
                                continue
                            
                            # æœªè¨ªå•ã‹ã¤ã‚­ãƒ¥ãƒ¼ã«æœªç™»éŒ²
                            if normalized_link not in self.visited_urls and normalized_link not in [q[0] for q in queue]:
                                # ãƒ‡ãƒãƒƒã‚°: ãƒªãƒ³ã‚¯ã®è¿½åŠ ã‚’è©³ç´°è¡¨ç¤º
                                if added_count < 3:  # æœ€åˆã®3ä»¶ã®ã¿è¡¨ç¤º
                                    print(f"[Web]     â• Adding: {normalized_link}")
                                
                                queue.append((normalized_link, depth + 1, current_url))  # è¦ªURLã‚‚è¨˜éŒ²
                                added_count += 1
                        
                        print(f"[Web]   ğŸ“¥ Summary: {added_count} added, {filtered_count} filtered, {invalid_count} invalid")
                    except Exception as e:
                        print(f"[Web]   âš ï¸ Link extraction failed: {e}")
                        import traceback
                        traceback.print_exc()
                
                # é…å»¶ï¼ˆå°‘ã—é•·ã‚ã«è¨­å®šï¼‰
                if self.delay > 0:
                    time.sleep(self.delay)
                    
            except Exception as e:
                error_msg = str(e)
                print(f"âŒ [Web] Error: {error_msg}")
                
                # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’åˆ¤å®š
                if "404" in error_msg or "Not Found" in error_msg:
                    print(f"[Web]   ğŸ’€ 404 Not Found - URL may be invalid")
                elif "401" in error_msg or "Authorization" in error_msg:
                    print(f"[Web]   ğŸ” Authorization Required - Check credentials")
                elif "403" in error_msg or "Forbidden" in error_msg:
                    print(f"[Web]   ğŸš« Forbidden - Access denied")
                elif "timeout" in error_msg.lower():
                    print(f"[Web]   â±ï¸ Timeout - Server too slow or unresponsive")
                else:
                    print(f"[Web]   âš ï¸ Unknown error")
                    import traceback
                    traceback.print_exc()
                
                self.failed_urls.add(current_url)
                
                # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å«ã‚ã¦çµæœã«è¿½åŠ ï¼ˆãƒãƒƒãƒãƒ³ã‚°ã‹ã‚‰é™¤å¤–ã§ãã‚‹ã‚ˆã†ã«ï¼‰
                error_metadata = {
                    "url": current_url,
                    "title": f"å–å¾—å¤±æ•—",
                    "text": "",
                    "screenshot_path": None,
                    "areas": [],
                    "screenshot_image": None,
                    "error": error_msg,  # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨˜éŒ²
                    "depth": depth,
                    "parent_url": parent_url,
                    "fetch_time": time.time()
                }
                results.append(error_metadata)
                self.sitemap[current_url] = error_metadata
                continue
        
        print(f"\n{'='*60}")
        print(f"[Web] âœ… Crawling Complete")
        print(f"  Total Pages: {len(results)}")
        print(f"  Successful: {len(self.visited_urls)}")
        print(f"  Failed: {len(self.failed_urls)}")
        print(f"  Max Depth Reached: {max(r['depth'] for r in results if r.get('depth') is not None)}")
        print(f"{'='*60}\n")
        
        return results
    
    def _extract_links_from_scraper_result(self, url: str) -> List[str]:
        """
        WebScraperã‚’ä½¿ç”¨ã—ã¦ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆèªè¨¼æƒ…å ±ã‚’å¼•ãç¶™ãï¼‰
        
        Args:
            url: å¯¾è±¡URL
        
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸãƒªãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """
        try:
            from playwright.sync_api import sync_playwright
            
            links = set()
            root_domain = urlparse(url).netloc
            
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¨­å®šï¼ˆèªè¨¼æƒ…å ±ã‚’å«ã‚€ï¼‰
                context_options = {
                    "viewport": {"width": 1920, "height": 1080},  # âœ… ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—è¡¨ç¤ºã«çµ±ä¸€
                    "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                }
                
                if self.username and self.password:
                    context_options["http_credentials"] = {
                        "username": self.username,
                        "password": self.password
                    }
                
                context = browser.new_context(**context_options)
                page = context.new_page()
                
                # Basicèªè¨¼ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®š
                if self.username and self.password:
                    credentials = base64.b64encode(f"{self.username}:{self.password}".encode()).decode()
                    page.set_extra_http_headers({
                        "Authorization": f"Basic {credentials}"
                    })
                
                try:
                    # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
                    page.goto(url, timeout=60000, wait_until="domcontentloaded")
                    time.sleep(0.3)  # 1.0s â†’ 0.3s çŸ­ç¸®
                    
                    # å®Ÿéš›ã®ãƒšãƒ¼ã‚¸URLã‚’å–å¾— (ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå¾Œã®æ­£ã—ã„URL)
                    actual_url = page.url
                    # æœ«å°¾ã«ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯è¿½åŠ ï¼ˆç›¸å¯¾ãƒªãƒ³ã‚¯è§£æ±ºã®ç²¾åº¦å‘ä¸Šï¼‰
                    if not actual_url.endswith('/') and '.' not in actual_url.split('/')[-1]:
                        actual_url = actual_url + '/'
                    
                    # JavaScriptã§ãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆgetAttributeä½¿ç”¨ã§ç›¸å¯¾URLã‚‚æ­£ç¢ºã«å–å¾—ï¼‰
                    raw_links = page.evaluate("""
                        () => {
                            return Array.from(document.querySelectorAll('a[href]'))
                                .map(a => ({
                                    href: a.getAttribute('href'),
                                    text: a.textContent.trim().substring(0, 30)
                                }))
                                .filter(link => link.href && link.href.trim() !== '');
                        }
                    """)
                    
                    print(f"[Web]   ğŸ”— Extracting from: {actual_url}")
                    print(f"[Web]   ğŸ“‹ Raw links found: {len(raw_links)}")
                    
                    # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º
                    for i, link_data in enumerate(raw_links[:5]):
                        href = link_data.get('href', link_data) if isinstance(link_data, dict) else link_data
                        text = link_data.get('text', '')[:20] if isinstance(link_data, dict) else ''
                        print(f"[Web]     [{i+1}] {href} (\"{text}\")")
                    
                    # ãƒªãƒ³ã‚¯ã‚’å‡¦ç†
                    for link_data in raw_links:
                        href = link_data.get('href', link_data) if isinstance(link_data, dict) else link_data
                        
                        try:
                            # javascript:ã‚„mailto:ã‚’é™¤å¤–
                            if href.startswith(('javascript:', 'mailto:', 'tel:', '#')):
                                continue
                            
                            # çµ¶å¯¾URLã«å¤‰æ› (actual_urlã‚’ä½¿ç”¨ã—ã¦æ­£ç¢ºã«è§£æ±º)
                            absolute_url = urljoin(actual_url, href)
                            
                            # ãƒ‡ãƒãƒƒã‚°: å¤‰æ›çµæœã‚’è¡¨ç¤ºï¼ˆæœ€åˆã®3ä»¶ï¼‰
                            if len(links) < 3:
                                print(f"[Web]     âœ“ {href} â†’ {absolute_url}")
                            
                            # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
                            absolute_url = absolute_url.split('#')[0]
                            
                            # URLæ¤œè¨¼
                            parsed = urlparse(absolute_url)
                            
                            # httpã¾ãŸã¯httpsã®ã¿
                            if parsed.scheme not in ['http', 'https']:
                                continue
                            
                            # åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿
                            if parsed.netloc != root_domain:
                                continue
                            
                            # é™çš„ã‚¢ã‚»ãƒƒãƒˆã‚’é™¤å¤–
                            if not self._is_static_asset(absolute_url):
                                links.add(absolute_url)
                        
                        except Exception as e:
                            print(f"[Web]   âš ï¸ URLå‡¦ç†ã‚¨ãƒ©ãƒ¼ ({href}): {e}")
                            continue
                    
                except Exception as e:
                    print(f"[Web]   âš ï¸ ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                
                finally:
                    context.close()
                    browser.close()
            
            return list(links)
            
        except Exception as e:
            print(f"âš ï¸ [Web] Link extraction error: {url} - {str(e)}")
            return []
    
    def _extract_links_from_page(self, url: str) -> List[str]:
        """
        ãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆã‚’ä½¿ç”¨ï¼‰
        
        Args:
            url: å¯¾è±¡URL
        
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸãƒªãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """
        return self._extract_links_from_scraper_result(url)
    
    def _is_obvious_static_asset(self, url: str) -> bool:
        """
        æ˜ã‚‰ã‹ãªé™çš„ã‚¢ã‚»ãƒƒãƒˆã‚’é«˜é€Ÿåˆ¤å®šï¼ˆè»½é‡ç‰ˆï¼‰
        """
        path = urlparse(url).path.lower()
        return any(path.endswith(ext) for ext in ['.jpg', '.png', '.gif', '.css', '.js', '.pdf', '.zip'])
    
    def _extract_links(self, base_url: str, html_text: str) -> List[str]:
        """
        HTMLãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆãƒ»ãƒ¬ã‚¬ã‚·ãƒ¼ï¼‰
        å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€BeautifulSoupãªã©ã‚’ä½¿ã†ã¨ã‚ˆã‚Šæ­£ç¢º
        
        æ³¨æ„: ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯éæ¨å¥¨ã§ã™ã€‚_extract_links_from_page ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
        """
        import re
        
        # ç°¡æ˜“çš„ãªãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆhrefå±æ€§ã‚’æ¢ã™ï¼‰
        pattern = r'href=["\']([^"\']+)["\']'
        matches = re.findall(pattern, html_text, re.IGNORECASE)
        
        links = []
        for match in matches:
            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
            absolute_url = urljoin(base_url, match)
            
            # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆ#ï¼‰ã‚’é™¤å»
            absolute_url = absolute_url.split('#')[0]
            
            # æœ‰åŠ¹ãªURLã‹ãƒã‚§ãƒƒã‚¯
            parsed = urlparse(absolute_url)
            if parsed.scheme in ['http', 'https']:
                links.append(absolute_url)
        
        return links
    
    def get_statistics(self) -> dict:
        """ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆæƒ…å ±ã¨ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’å–å¾—"""
        depth_distribution = {}
        for url, metadata in self.sitemap.items():
            depth = metadata.get('depth', 0)
            depth_distribution[depth] = depth_distribution.get(depth, 0) + 1
        
        return {
            "visited_count": len(self.visited_urls),
            "failed_count": len(self.failed_urls),
            "total_pages": len(self.sitemap),
            "depth_distribution": depth_distribution,
            "visited_urls": list(self.visited_urls),
            "failed_urls": list(self.failed_urls),
            "sitemap": self.sitemap  # å®Œå…¨ãªã‚µã‚¤ãƒˆãƒãƒƒãƒ—æƒ…å ±
        }
    
    def get_sitemap_tree(self) -> Dict:
        """
        ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ãƒ„ãƒªãƒ¼æ§‹é€ ã§å–å¾—ï¼ˆéšå±¤çš„è¡¨ç¤ºç”¨ï¼‰
        
        Returns:
            éšå±¤æ§‹é€ ã®ã‚µã‚¤ãƒˆãƒãƒƒãƒ—
        """
        tree = {}
        
        # depthã§ã‚½ãƒ¼ãƒˆ
        sorted_pages = sorted(
            self.sitemap.items(),
            key=lambda x: x[1].get('depth', 0)
        )
        
        for url, metadata in sorted_pages:
            depth = metadata.get('depth', 0)
            parent = metadata.get('parent_url')
            
            if depth not in tree:
                tree[depth] = []
            
            tree[depth].append({
                'url': url,
                'title': metadata.get('title', ''),
                'parent_url': parent,
                'has_error': metadata.get('error') is not None
            })
        
        return tree

