"""
æ®µè½æ¤œå‡ºè©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Paragraph Boundary F1, Over-splitç‡, Over-mergeç‡ã‚’è¨ˆæ¸¬
"""
import json
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from pathlib import Path


@dataclass
class ParagraphSpan:
    """æ®µè½ã®ç¯„å›²ã‚’ç¤ºã™"""
    start_line: int
    end_line: int
    text: str = ""
    paragraph_id: str = ""


@dataclass
class EvaluationResult:
    """è©•ä¾¡çµæœ"""
    precision: float
    recall: float
    f1: float
    over_split_rate: float
    over_merge_rate: float
    total_gold: int
    total_pred: int
    matched: int


def extract_boundaries(paragraphs: List[ParagraphSpan]) -> List[int]:
    """æ®µè½ãƒªã‚¹ãƒˆã‹ã‚‰å¢ƒç•Œä½ç½®ï¼ˆè¡Œç•ªå·ï¼‰ã‚’æŠ½å‡º"""
    boundaries = set()
    for p in paragraphs:
        boundaries.add(p.start_line)
        boundaries.add(p.end_line + 1)  # çµ‚ç«¯ã®æ¬¡
    return sorted(boundaries)


def calculate_boundary_f1(
    gold_paragraphs: List[ParagraphSpan],
    pred_paragraphs: List[ParagraphSpan],
    tolerance: int = 0
) -> EvaluationResult:
    """
    æ®µè½å¢ƒç•Œã®F1ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    
    Args:
        gold_paragraphs: æ­£è§£æ®µè½ãƒªã‚¹ãƒˆ
        pred_paragraphs: äºˆæ¸¬æ®µè½ãƒªã‚¹ãƒˆ
        tolerance: å¢ƒç•Œä½ç½®ã®è¨±å®¹èª¤å·®ï¼ˆè¡Œæ•°ï¼‰
    
    Returns:
        EvaluationResult: è©•ä¾¡çµæœ
    """
    gold_boundaries = extract_boundaries(gold_paragraphs)
    pred_boundaries = extract_boundaries(pred_paragraphs)
    
    # ãƒãƒƒãƒãƒ³ã‚°
    matched = 0
    used_pred = set()
    
    for g in gold_boundaries:
        for i, p in enumerate(pred_boundaries):
            if i not in used_pred and abs(g - p) <= tolerance:
                matched += 1
                used_pred.add(i)
                break
    
    precision = matched / len(pred_boundaries) if pred_boundaries else 0.0
    recall = matched / len(gold_boundaries) if gold_boundaries else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    # Over-split: äºˆæ¸¬ãŒæ­£è§£ã‚ˆã‚Šå¤šã„
    over_split_rate = max(0, len(pred_paragraphs) - len(gold_paragraphs)) / len(gold_paragraphs) if gold_paragraphs else 0.0
    
    # Over-merge: äºˆæ¸¬ãŒæ­£è§£ã‚ˆã‚Šå°‘ãªã„
    over_merge_rate = max(0, len(gold_paragraphs) - len(pred_paragraphs)) / len(gold_paragraphs) if gold_paragraphs else 0.0
    
    return EvaluationResult(
        precision=precision,
        recall=recall,
        f1=f1,
        over_split_rate=over_split_rate,
        over_merge_rate=over_merge_rate,
        total_gold=len(gold_paragraphs),
        total_pred=len(pred_paragraphs),
        matched=matched
    )


def load_gold_annotations(json_path: Path) -> List[ParagraphSpan]:
    """æ­£è§£ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’JSONã‹ã‚‰èª­ã¿è¾¼ã¿"""
    if not json_path.exists():
        return []
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    paragraphs = []
    for item in data.get('paragraphs', []):
        paragraphs.append(ParagraphSpan(
            start_line=item.get('start_line', 0),
            end_line=item.get('end_line', 0),
            text=item.get('text', ''),
            paragraph_id=item.get('id', '')
        ))
    
    return paragraphs


def evaluate_document(
    gold_path: Path,
    pred_paragraphs: List[ParagraphSpan]
) -> Optional[EvaluationResult]:
    """1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è©•ä¾¡"""
    gold_paragraphs = load_gold_annotations(gold_path)
    if not gold_paragraphs:
        print(f"âš ï¸ No gold annotations found: {gold_path}")
        return None
    
    result = calculate_boundary_f1(gold_paragraphs, pred_paragraphs)
    return result


def run_evaluation_suite(eval_dir: Path) -> Dict[str, EvaluationResult]:
    """
    è©•ä¾¡ã‚»ãƒƒãƒˆå…¨ä½“ã‚’å®Ÿè¡Œ
    
    eval_diræ§‹é€ :
    - eval_dir/
      - sample_001/
        - gold.json
        - input.pdf/.html/.png
      - sample_002/
        - ...
    """
    results = {}
    
    for sample_dir in eval_dir.iterdir():
        if not sample_dir.is_dir():
            continue
        
        gold_path = sample_dir / 'gold.json'
        if not gold_path.exists():
            print(f"â­ï¸ Skipping {sample_dir.name}: no gold.json")
            continue
        
        # TODO: å®Ÿéš›ã®æ®µè½æ¤œå‡ºã‚’å®Ÿè¡Œ
        # pred_paragraphs = detect_paragraphs(sample_dir)
        pred_paragraphs = []  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        
        result = evaluate_document(gold_path, pred_paragraphs)
        if result:
            results[sample_dir.name] = result
    
    return results


def print_summary(results: Dict[str, EvaluationResult]):
    """çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    if not results:
        print("No results to summarize")
        return
    
    print("\n" + "=" * 60)
    print("ğŸ“Š Paragraph Detection Evaluation Summary")
    print("=" * 60)
    
    total_f1 = 0.0
    total_over_split = 0.0
    total_over_merge = 0.0
    
    for name, r in results.items():
        print(f"\nğŸ“„ {name}")
        print(f"   F1: {r.f1:.3f} (P={r.precision:.3f}, R={r.recall:.3f})")
        print(f"   Over-split: {r.over_split_rate:.1%}, Over-merge: {r.over_merge_rate:.1%}")
        print(f"   Gold: {r.total_gold} paragraphs, Pred: {r.total_pred} paragraphs")
        
        total_f1 += r.f1
        total_over_split += r.over_split_rate
        total_over_merge += r.over_merge_rate
    
    n = len(results)
    print("\n" + "-" * 60)
    print(f"ğŸ“ˆ Average F1: {total_f1/n:.3f}")
    print(f"ğŸ“ˆ Average Over-split: {total_over_split/n:.1%}")
    print(f"ğŸ“ˆ Average Over-merge: {total_over_merge/n:.1%}")
    print("=" * 60)


# ã‚µãƒ³ãƒ—ãƒ«æ­£è§£ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
GOLD_TEMPLATE = {
    "document_id": "sample_001",
    "source_type": "pdf",  # pdf | web | ocr
    "paragraphs": [
        {
            "id": "P1",
            "start_line": 1,
            "end_line": 5,
            "text": "æ®µè½1ã®ãƒ†ã‚­ã‚¹ãƒˆ...",
            "bbox": [0, 0, 100, 50]
        },
        {
            "id": "P2", 
            "start_line": 6,
            "end_line": 10,
            "text": "æ®µè½2ã®ãƒ†ã‚­ã‚¹ãƒˆ...",
            "bbox": [0, 50, 100, 100]
        }
    ]
}


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        eval_dir = Path(sys.argv[1])
    else:
        eval_dir = Path("Vault/40_Evals")
    
    print(f"ğŸ” Evaluating: {eval_dir}")
    results = run_evaluation_suite(eval_dir)
    print_summary(results)
