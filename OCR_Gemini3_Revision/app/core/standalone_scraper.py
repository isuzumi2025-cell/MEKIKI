"""
ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ¼ãƒ³ãƒ»ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
sitemap_proã®ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åŸ‹ã‚è¾¼ã¿ã€ã‚µãƒ¼ãƒãƒ¼ä¸è¦ã§å‹•ä½œ

Features:
- PlaywrightåŒæœŸAPIã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
- Basicèªè¨¼å¯¾å¿œ
- ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆå–å¾—
- ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
- BFSã‚¯ãƒ­ãƒ¼ãƒ«
"""
import asyncio
import base64
import io
import time
from typing import List, Dict, Optional, Tuple, Set, Callable
from urllib.parse import urljoin, urlparse
from pathlib import Path
from dataclasses import dataclass
from datetime import datetime

from PIL import Image


@dataclass
class PageResult:
    """ãƒšãƒ¼ã‚¸çµæœ"""
    url: str
    status_code: int
    title: str
    text_content: str
    screenshot_base64: Optional[str]
    depth: int
    parent_url: Optional[str]
    links: List[str]
    error: Optional[str] = None


class StandaloneScraper:
    """
    ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ¼ãƒ³ãƒ»ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
    ã‚µãƒ¼ãƒãƒ¼ä¸è¦ã€ç›´æ¥Playwrightã‚’ä½¿ç”¨
    """
    
    def __init__(
        self,
        headless: bool = True,
        timeout: int = 30000,
        viewport_width: int = 1920,
        viewport_height: int = 1080
    ):
        self.headless = headless
        self.timeout = timeout
        self.viewport_width = viewport_width
        self.viewport_height = viewport_height
        
        # é™¤å¤–æ‹¡å¼µå­
        self.excluded_extensions = {
            '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
            '.zip', '.rar', '.7z', '.tar', '.gz',
            '.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg', '.ico',
            '.mp3', '.mp4', '.avi', '.mov', '.wmv',
            '.css', '.js', '.json', '.xml'
        }
    
    def _should_exclude(self, url: str) -> bool:
        """URLã‚’é™¤å¤–ã™ã¹ãã‹"""
        parsed = urlparse(url)
        path = parsed.path.lower()
        for ext in self.excluded_extensions:
            if path.endswith(ext):
                return True
        return False
    
    def _normalize_url(self, url: str) -> str:
        """URLæ­£è¦åŒ–"""
        parsed = urlparse(url)
        # ã‚¯ã‚¨ãƒªã¨ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
        return f"{parsed.scheme}://{parsed.netloc}{parsed.path}".rstrip('/')
    
    def scrape_page(
        self,
        url: str,
        username: Optional[str] = None,
        password: Optional[str] = None,
        take_screenshot: bool = True,
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> PageResult:
        """
        å˜ä¸€ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        
        Args:
            url: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹URL
            username: Basicèªè¨¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å
            password: Basicèªè¨¼ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
            take_screenshot: ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’æ’®ã‚‹ã‹
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        Returns:
            PageResult: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ
        """
        from playwright.sync_api import sync_playwright
        
        if progress_callback:
            progress_callback(f"ğŸŒ ãƒ•ã‚§ãƒƒãƒä¸­: {url}")
        
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=self.headless)
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¨­å®š
                context_options = {
                    'viewport': {'width': self.viewport_width, 'height': self.viewport_height},
                    'device_scale_factor': 2.0
                }
                
                # Basicèªè¨¼
                if username and password:
                    context_options['http_credentials'] = {
                        'username': username,
                        'password': password
                    }
                
                context = browser.new_context(**context_options)
                page = context.new_page()
                page.set_default_timeout(self.timeout)
                
                # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
                response = page.goto(url, wait_until='domcontentloaded')
                status_code = response.status if response else 0
                
                # èª­ã¿è¾¼ã¿å¾…æ©Ÿ
                page.wait_for_load_state('networkidle', timeout=10000)
                
                # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                title = page.title()
                
                # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                text_content = page.inner_text('body')
                
                # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
                screenshot_base64 = None
                if take_screenshot:
                    try:
                        screenshot_bytes = page.screenshot(full_page=True)
                        screenshot_base64 = base64.b64encode(screenshot_bytes).decode('utf-8')
                    except Exception as e:
                        print(f"âš ï¸ ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                
                # ãƒªãƒ³ã‚¯æŠ½å‡º
                links = page.eval_on_selector_all(
                    'a[href]',
                    'elements => elements.map(e => e.href)'
                )
                
                browser.close()
                
                return PageResult(
                    url=url,
                    status_code=status_code,
                    title=title,
                    text_content=text_content,
                    screenshot_base64=screenshot_base64,
                    depth=0,
                    parent_url=None,
                    links=links
                )
                
        except Exception as e:
            return PageResult(
                url=url,
                status_code=0,
                title="",
                text_content="",
                screenshot_base64=None,
                depth=0,
                parent_url=None,
                links=[],
                error=str(e)
            )
    
    def crawl(
        self,
        start_url: str,
        max_pages: int = 10,
        max_depth: int = 2,
        username: Optional[str] = None,
        password: Optional[str] = None,
        delay: float = 1.0,
        progress_callback: Optional[Callable[[str, int, int], None]] = None,
        respect_robots: bool = False
    ) -> List[PageResult]:
        """
        BFSã‚¯ãƒ­ãƒ¼ãƒ«
        
        Args:
            start_url: é–‹å§‹URL
            max_pages: æœ€å¤§ãƒšãƒ¼ã‚¸æ•°
            max_depth: æœ€å¤§æ·±åº¦
            username: Basicèªè¨¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å
            password: Basicèªè¨¼ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
            delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ï¼‰
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ (url, current, total)
            respect_robots: robots.txtã‚’å°Šé‡ã™ã‚‹ã‹
        
        Returns:
            List[PageResult]: ã‚¯ãƒ­ãƒ¼ãƒ«çµæœ
        """
        from playwright.sync_api import sync_playwright
        
        results: List[PageResult] = []
        visited: Set[str] = set()
        queue: List[Tuple[str, int, Optional[str]]] = [(start_url, 0, None)]  # (url, depth, parent)
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¶é™
        start_domain = urlparse(start_url).netloc
        
        print(f"ğŸš€ ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹: {start_url}")
        print(f"   æœ€å¤§ãƒšãƒ¼ã‚¸: {max_pages}, æœ€å¤§æ·±åº¦: {max_depth}")
        
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=self.headless)
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¨­å®š
                context_options = {
                    'viewport': {'width': self.viewport_width, 'height': self.viewport_height},
                    'device_scale_factor': 2.0
                }
                
                # Basicèªè¨¼
                if username and password:
                    context_options['http_credentials'] = {
                        'username': username,
                        'password': password
                    }
                    print(f"   ğŸ” Basicèªè¨¼: {username}")
                
                context = browser.new_context(**context_options)
                page = context.new_page()
                page.set_default_timeout(self.timeout)
                
                while queue and len(results) < max_pages:
                    url, depth, parent_url = queue.pop(0)
                    
                    # æ­£è¦åŒ–
                    normalized_url = self._normalize_url(url)
                    
                    # è¨ªå•æ¸ˆã¿ãƒã‚§ãƒƒã‚¯
                    if normalized_url in visited:
                        continue
                    
                    # æ·±åº¦ãƒã‚§ãƒƒã‚¯
                    if depth > max_depth:
                        continue
                    
                    # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
                    if urlparse(url).netloc != start_domain:
                        continue
                    
                    # æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
                    if self._should_exclude(url):
                        continue
                    
                    visited.add(normalized_url)
                    
                    # é€²æ—
                    if progress_callback:
                        progress_callback(url, len(results) + 1, max_pages)
                    
                    print(f"ğŸ“„ [{len(results)+1}/{max_pages}] (æ·±åº¦{depth}) {url[:60]}...")
                    
                    try:
                        # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
                        response = page.goto(url, wait_until='domcontentloaded')
                        status_code = response.status if response else 0
                        
                        # èª­ã¿è¾¼ã¿å¾…æ©Ÿ
                        try:
                            page.wait_for_load_state('networkidle', timeout=10000)
                        except:
                            pass
                        
                        # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                        title = page.title()
                        
                        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                        try:
                            text_content = page.inner_text('body')
                        except:
                            text_content = ""
                        
                        # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
                        screenshot_base64 = None
                        try:
                            screenshot_bytes = page.screenshot(full_page=True)
                            screenshot_base64 = base64.b64encode(screenshot_bytes).decode('utf-8')
                        except Exception as e:
                            print(f"  âš ï¸ ã‚¹ã‚¯ã‚·ãƒ§ã‚¨ãƒ©ãƒ¼: {e}")
                        
                        # ãƒªãƒ³ã‚¯æŠ½å‡º
                        try:
                            raw_links = page.eval_on_selector_all(
                                'a[href]',
                                'elements => elements.map(e => e.href)'
                            )
                        except:
                            raw_links = []
                        
                        # çµæœä¿å­˜
                        result = PageResult(
                            url=url,
                            status_code=status_code,
                            title=title,
                            text_content=text_content,
                            screenshot_base64=screenshot_base64,
                            depth=depth,
                            parent_url=parent_url,
                            links=raw_links
                        )
                        results.append(result)
                        
                        # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
                        if depth < max_depth:
                            for link in raw_links:
                                if link and urlparse(link).netloc == start_domain:
                                    normalized = self._normalize_url(link)
                                    if normalized not in visited and not self._should_exclude(link):
                                        queue.append((link, depth + 1, url))
                        
                    except Exception as e:
                        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                        results.append(PageResult(
                            url=url,
                            status_code=0,
                            title="",
                            text_content="",
                            screenshot_base64=None,
                            depth=depth,
                            parent_url=parent_url,
                            links=[],
                            error=str(e)
                        ))
                    
                    # é…å»¶
                    if delay > 0:
                        time.sleep(delay)
                
                browser.close()
        
        except Exception as e:
            print(f"âŒ ã‚¯ãƒ­ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        
        print(f"âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†: {len(results)} ãƒšãƒ¼ã‚¸")
        return results
    
    def get_results_as_dict_list(self, results: List[PageResult]) -> List[Dict]:
        """çµæœã‚’è¾æ›¸ãƒªã‚¹ãƒˆã«å¤‰æ›ï¼ˆã‚­ãƒ¥ãƒ¼è¿½åŠ ç”¨ï¼‰"""
        return [
            {
                'type': 'web',
                'url': r.url,
                'status_code': r.status_code,
                'title': r.title,
                'text_content': r.text_content,
                'screenshot_base64': r.screenshot_base64,
                'depth': r.depth,
                'parent_url': r.parent_url,
                'error': r.error
            }
            for r in results
        ]


# ãƒ†ã‚¹ãƒˆç”¨
if __name__ == "__main__":
    scraper = StandaloneScraper(headless=True)
    
    # å˜ä¸€ãƒšãƒ¼ã‚¸ãƒ†ã‚¹ãƒˆ
    result = scraper.scrape_page("https://example.com")
    print(f"Title: {result.title}")
    print(f"Text length: {len(result.text_content)}")
    print(f"Links: {len(result.links)}")
