"""
Ultimate Sync - ParagraphMatcher
ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•å˜ä½ã§Web/PDFé–“ã®ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã€ãƒãƒƒãƒãƒ³ã‚°ã‚’è¡Œã†

Ultrathink: The edge of real value :)
"""

import re
import unicodedata
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional
from difflib import SequenceMatcher


@dataclass
class ParagraphEntry:
    """ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•æƒ…å ±ã‚’ä¿æŒ"""
    id: str                    # "W-001" or "P-001"
    source: str                # "web" or "pdf"
    text: str                  # æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ
    rect: List[int]            # [x1, y1, x2, y2]
    page: int = 1              # ãƒšãƒ¼ã‚¸ç•ªå·
    sync_id: Optional[str] = None      # ãƒãƒƒãƒã—ãŸãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã®ID
    similarity: float = 0.0    # é¡ä¼¼åº¦ (0.0-1.0)
    sync_color: str = "red"    # è‰²ã‚³ãƒ¼ãƒ‰
    
    @property
    def preview(self) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ (50æ–‡å­—)"""
        text = self.text.replace('\n', ' ')
        return text[:50] + "..." if len(text) > 50 else text
    
    @property
    def similarity_percent(self) -> str:
        """é¡ä¼¼åº¦ã‚’ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤º"""
        return f"{self.similarity * 100:.1f}%"


@dataclass
class SyncPair:
    """ãƒãƒƒãƒã—ãŸãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ãƒšã‚¢"""
    web_id: str
    pdf_id: str
    similarity: float
    color: str
    
    @classmethod
    def get_color(cls, similarity: float) -> str:
        """é¡ä¼¼åº¦ã«å¿œã˜ãŸè‰²ã‚’è¿”ã™"""
        if similarity >= 0.5:
            return "#4CAF50"  # ç·‘ (50%+)
        elif similarity >= 0.3:
            return "#FF9800"  # ã‚ªãƒ¬ãƒ³ã‚¸ (30-50%)
        else:
            return "#F44336"  # èµ¤ (<30%)


class ParagraphMatcher:
    """
    ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ãƒãƒƒãƒãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³
    
    Web/PDFã®å…¨ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã‚’æ¯”è¼ƒã—ã€æœ€é©ãªãƒãƒƒãƒãƒ³ã‚°ã‚’è¦‹ã¤ã‘ã‚‹
    """
    
    SYNC_COLORS = [
        "#4CAF50",  # ç·‘
        "#2196F3",  # é’
        "#FF9800",  # ã‚ªãƒ¬ãƒ³ã‚¸
        "#9C27B0",  # ç´«
        "#00BCD4",  # ã‚·ã‚¢ãƒ³
        "#E91E63",  # ãƒ”ãƒ³ã‚¯
        "#CDDC39",  # ãƒ©ã‚¤ãƒ 
        "#FF5722",  # æ·±ã‚ªãƒ¬ãƒ³ã‚¸
        "#607D8B",  # ãƒ–ãƒ«ãƒ¼ã‚°ãƒ¬ãƒ¼
        "#795548",  # èŒ¶
    ]
    
    def __init__(self, threshold_high: float = 0.5, threshold_low: float = 0.3):
        self.threshold_high = threshold_high
        self.threshold_low = threshold_low
        
        # ãƒãƒ«ãƒã‚·ã‚°ãƒŠãƒ«èåˆç”¨ã®åˆ†æå™¨
        self._syntax_analyzer = None
        self._spatial_analyzer = None
        
        # èåˆã‚·ã‚°ãƒŠãƒ«é‡ã¿ (æ‰¿èªæ¸ˆã¿)
        self.WEIGHT_TEXT = 0.40      # ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦
        self.WEIGHT_SPATIAL = 0.30   # ç©ºé–“ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
        self.WEIGHT_IMAGE = 0.20     # ç”»åƒãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.WEIGHT_SYNTAX = 0.10    # æ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³
    
    @property
    def syntax_analyzer(self):
        """é…å»¶åˆæœŸåŒ–ã§æ§‹æ–‡åˆ†æå™¨ã‚’å–å¾—"""
        if self._syntax_analyzer is None:
            from app.core.syntax_pattern_analyzer import SyntaxPatternAnalyzer
            self._syntax_analyzer = SyntaxPatternAnalyzer()
        return self._syntax_analyzer
    
    def calculate_fusion_score(
        self,
        text1: str, text2: str,
        rect1: List[int] = None, rect2: List[int] = None,
        image_similarity: float = None
    ) -> float:
        """
        ãƒãƒ«ãƒã‚·ã‚°ãƒŠãƒ«èåˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        
        Args:
            text1, text2: æ¯”è¼ƒã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            rect1, rect2: çŸ©å½¢åº§æ¨™ (ç©ºé–“ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç”¨)
            image_similarity: ç”»åƒé¡ä¼¼åº¦ (äº‹å‰è¨ˆç®—æ¸ˆã¿ã€ãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—)
        
        Returns:
            èåˆã‚¹ã‚³ã‚¢ (0.0-1.0)
        """
        scores = {}
        weights_sum = 0.0
        
        # 1. ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ (40%)
        text_score = self.calculate_similarity(text1, text2)
        scores['text'] = text_score
        weights_sum += self.WEIGHT_TEXT
        
        # 2. ç©ºé–“ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é¡ä¼¼åº¦ (30%)
        if rect1 and rect2:
            spatial_score = self._calculate_spatial_similarity(rect1, rect2)
            scores['spatial'] = spatial_score
            weights_sum += self.WEIGHT_SPATIAL
        
        # 3. ç”»åƒãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé¡ä¼¼åº¦ (20%)
        if image_similarity is not None:
            scores['image'] = image_similarity
            weights_sum += self.WEIGHT_IMAGE
        
        # 4. æ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³é¡ä¼¼åº¦ (10%)
        if text1 and text2:
            try:
                p1 = self.syntax_analyzer.extract_pattern(text1)
                p2 = self.syntax_analyzer.extract_pattern(text2)
                syntax_score = self.syntax_analyzer.compare_patterns(p1, p2)
                scores['syntax'] = syntax_score
                weights_sum += self.WEIGHT_SYNTAX
            except Exception:
                pass
        
        # é‡ã¿ä»˜ãå¹³å‡
        if weights_sum == 0:
            return 0.0
        
        fusion = (
            scores.get('text', 0) * self.WEIGHT_TEXT +
            scores.get('spatial', 0) * self.WEIGHT_SPATIAL +
            scores.get('image', 0) * self.WEIGHT_IMAGE +
            scores.get('syntax', 0) * self.WEIGHT_SYNTAX
        ) / weights_sum * (self.WEIGHT_TEXT + self.WEIGHT_SPATIAL + self.WEIGHT_IMAGE + self.WEIGHT_SYNTAX)
        
        return min(1.0, fusion)
    
    def _calculate_spatial_similarity(self, rect1: List[int], rect2: List[int]) -> float:
        """
        ç©ºé–“çš„é¡ä¼¼åº¦ã‚’è¨ˆç®— (ä½ç½®ãƒ»ã‚µã‚¤ã‚ºæ¯”è¼ƒ)
        """
        if not rect1 or not rect2 or len(rect1) < 4 or len(rect2) < 4:
            return 0.0
        
        x1_1, y1_1, x2_1, y2_1 = rect1
        x1_2, y1_2, x2_2, y2_2 = rect2
        
        # ã‚µã‚¤ã‚º
        w1, h1 = x2_1 - x1_1, y2_1 - y1_1
        w2, h2 = x2_2 - x1_2, y2_2 - y1_2
        
        if w1 <= 0 or h1 <= 0 or w2 <= 0 or h2 <= 0:
            return 0.0
        
        # ã‚µã‚¤ã‚ºæ¯”é¡ä¼¼åº¦
        width_ratio = min(w1, w2) / max(w1, w2)
        height_ratio = min(h1, h2) / max(h1, h2)
        size_score = (width_ratio + height_ratio) / 2
        
        # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”é¡ä¼¼åº¦
        aspect1 = w1 / h1
        aspect2 = w2 / h2
        aspect_score = min(aspect1, aspect2) / max(aspect1, aspect2)
        
        # ç›¸å¯¾ä½ç½®ã®é¡ä¼¼åº¦ (æ­£è¦åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŒã“ã“ã§ã¯è¿‘ä¼¼)
        # ä»Šå›ã¯ã‚µã‚¤ã‚ºã¨ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®ã¿
        return (size_score * 0.6 + aspect_score * 0.4)
    
    def normalize_text(self, text: str) -> str:
        """
        æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ– (é«˜ç²¾åº¦ç‰ˆ)
        - å…¨è§’/åŠè§’çµ±ä¸€
        - ç©ºç™½ãƒ»æ”¹è¡Œã®æ­£è¦åŒ–
        - ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãªå¤‰æ›
        - å¥èª­ç‚¹ãƒ»è¨˜å·ã®çµ±ä¸€
        """
        if not text:
            return ""
        
        # Unicodeæ­£è¦åŒ– (NFKC: å…¨è§’â†’åŠè§’ã€ç•°ä½“å­—çµ±ä¸€)
        text = unicodedata.normalize('NFKC', text)
        
        # ç©ºç™½ã¨æ”¹è¡Œã®æ­£è¦åŒ–
        text = re.sub(r'\s+', ' ', text).strip()
        
        # å¥èª­ç‚¹ã®çµ±ä¸€
        text = text.replace('ã€‚', '.').replace('ã€', ',')
        text = text.replace('ï¼', '!').replace('ï¼Ÿ', '?')
        text = text.replace('ï¼š', ':').replace('ï¼›', ';')
        
        # ã‚«ãƒƒã‚³ã®çµ±ä¸€
        text = text.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
        text = text.replace('ã€Œ', '"').replace('ã€', '"')
        text = text.replace('ã€', '"').replace('ã€', '"')
        
        # ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãªå¤‰æ›
        result = []
        for char in text:
            code = ord(char)
            # ã‚«ã‚¿ã‚«ãƒŠç¯„å›² (U+30A1 - U+30F6) â†’ ã²ã‚‰ãŒãª (U+3041 - U+3096)
            if 0x30A1 <= code <= 0x30F6:
                result.append(chr(code - 0x60))
            else:
                result.append(char)
        
        return ''.join(result).lower()
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """
        2ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆé–“ã®é¡ä¼¼åº¦ã‚’è¨ˆç®— (è¤‡åˆã‚¹ã‚³ã‚¢ç‰ˆ)
        
        è¤‡æ•°ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦ç²¾åº¦å‘ä¸Š:
        1. SequenceMatcher (æ–‡å­—åˆ—å…¨ä½“)
        2. å˜èªãƒ¬ãƒ™ãƒ«Jaccardé¡ä¼¼åº¦
        3. æ–‡å­—N-gramé‡è¤‡ç‡
        """
        if not text1 or not text2:
            return 0.0
        
        # æ­£è¦åŒ–
        norm1 = self.normalize_text(text1)
        norm2 = self.normalize_text(text2)
        
        if not norm1 or not norm2:
            return 0.0
        
        # çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        if norm1 == norm2:
            return 1.0
        
        # 1. SequenceMatcherã§é¡ä¼¼åº¦è¨ˆç®— (40%)
        seq_score = SequenceMatcher(None, norm1, norm2).ratio()
        
        # 2. å˜èªãƒ¬ãƒ™ãƒ«Jaccardé¡ä¼¼åº¦ (30%)
        words1 = set(norm1.split())
        words2 = set(norm2.split())
        if words1 or words2:
            jaccard = len(words1 & words2) / len(words1 | words2) if (words1 | words2) else 0.0
        else:
            jaccard = 0.0
        
        # 3. æ–‡å­—N-gram (bi-gram) é‡è¤‡ç‡ (30%)
        def get_ngrams(text, n=2):
            return set(text[i:i+n] for i in range(len(text) - n + 1))
        
        ngrams1 = get_ngrams(norm1)
        ngrams2 = get_ngrams(norm2)
        if ngrams1 or ngrams2:
            ngram_score = len(ngrams1 & ngrams2) / len(ngrams1 | ngrams2) if (ngrams1 | ngrams2) else 0.0
        else:
            ngram_score = 0.0
        
        # è¤‡åˆã‚¹ã‚³ã‚¢
        combined = 0.4 * seq_score + 0.3 * jaccard + 0.3 * ngram_score
        
        return combined
    
    def match_paragraphs(
        self, 
        web_paragraphs: List[ParagraphEntry], 
        pdf_paragraphs: List[ParagraphEntry]
    ) -> Tuple[List[ParagraphEntry], List[ParagraphEntry], List[SyncPair]]:
        """
        Web/PDFãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã‚’ãƒãƒƒãƒãƒ³ã‚°
        
        ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :
        1. å…¨ãƒšã‚¢ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        2. é¡ä¼¼åº¦ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
        3. 1å¯¾1ãƒãƒƒãƒãƒ³ã‚°ã‚’è¡Œã†ï¼ˆè²ªæ¬²æ³•ï¼‰
        4. é–¾å€¤ä»¥ä¸Šã®ãƒšã‚¢ã‚’Syncã¨ã—ã¦ãƒãƒ¼ã‚¯
        
        Returns:
            (æ›´æ–°æ¸ˆã¿webãƒ‘ãƒ©ã‚°ãƒ©ãƒ•, æ›´æ–°æ¸ˆã¿pdfãƒ‘ãƒ©ã‚°ãƒ©ãƒ•, ãƒãƒƒãƒãƒšã‚¢ãƒªã‚¹ãƒˆ)
        """
        if not web_paragraphs or not pdf_paragraphs:
            return web_paragraphs, pdf_paragraphs, []
        
        print(f"[ParagraphMatcher] ãƒãƒƒãƒãƒ³ã‚°é–‹å§‹: Web {len(web_paragraphs)}ä»¶ x PDF {len(pdf_paragraphs)}ä»¶")
        
        # å…¨ãƒšã‚¢ã®èåˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— (ãƒãƒ«ãƒã‚·ã‚°ãƒŠãƒ«)
        pairs = []
        for w in web_paragraphs:
            for p in pdf_paragraphs:
                # èåˆã‚¹ã‚³ã‚¢è¨ˆç®— (ãƒ†ã‚­ã‚¹ãƒˆ40% + ç©ºé–“30% + æ§‹æ–‡10%)
                # ç”»åƒé¡ä¼¼åº¦ (20%) ã¯äº‹å‰è¨ˆç®—ãŒå¿…è¦ãªã®ã§å¾Œã§è¿½åŠ å¯èƒ½
                sim = self.calculate_fusion_score(
                    text1=w.text,
                    text2=p.text,
                    rect1=w.rect,
                    rect2=p.rect,
                    image_similarity=None  # TODO: ç”»åƒæ¯”è¼ƒã‚’çµ±åˆ
                )
                if sim > 0.1:  # æœ€ä½é–¾å€¤
                    pairs.append((w.id, p.id, sim))
        
        # é¡ä¼¼åº¦ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
        pairs.sort(key=lambda x: x[2], reverse=True)
        
        # 1å¯¾1ãƒãƒƒãƒãƒ³ã‚°ï¼ˆè²ªæ¬²æ³•ï¼‰
        matched_web = set()
        matched_pdf = set()
        sync_pairs = []
        color_index = 0
        
        for web_id, pdf_id, sim in pairs:
            if web_id in matched_web or pdf_id in matched_pdf:
                continue
            
            # ãƒãƒƒãƒç¢ºå®š
            matched_web.add(web_id)
            matched_pdf.add(pdf_id)
            
            # è‰²å‰²ã‚Šå½“ã¦
            if sim >= self.threshold_high:
                color = self.SYNC_COLORS[color_index % len(self.SYNC_COLORS)]
                color_index += 1
            elif sim >= self.threshold_low:
                color = "#FF9800"  # ã‚ªãƒ¬ãƒ³ã‚¸ (éƒ¨åˆ†ãƒãƒƒãƒ)
            else:
                color = "#F44336"  # èµ¤ (ä½ãƒãƒƒãƒ)
            
            sync_pairs.append(SyncPair(
                web_id=web_id,
                pdf_id=pdf_id,
                similarity=sim,
                color=color
            ))
        
        # ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã«Syncæƒ…å ±ã‚’ä»˜ä¸
        sync_map_web = {sp.web_id: sp for sp in sync_pairs}
        sync_map_pdf = {sp.pdf_id: sp for sp in sync_pairs}
        
        for w in web_paragraphs:
            if w.id in sync_map_web:
                sp = sync_map_web[w.id]
                w.sync_id = sp.pdf_id
                w.similarity = sp.similarity
                w.sync_color = sp.color
            else:
                w.sync_id = None
                w.similarity = 0.0
                w.sync_color = "#F44336"  # æœªãƒãƒƒãƒã¯èµ¤
        
        for p in pdf_paragraphs:
            if p.id in sync_map_pdf:
                sp = sync_map_pdf[p.id]
                p.sync_id = sp.web_id
                p.similarity = sp.similarity
                p.sync_color = sp.color
            else:
                p.sync_id = None
                p.similarity = 0.0
                p.sync_color = "#F44336"  # æœªãƒãƒƒãƒã¯èµ¤
        
        # çµ±è¨ˆå‡ºåŠ›
        high_matches = sum(1 for sp in sync_pairs if sp.similarity >= self.threshold_high)
        mid_matches = sum(1 for sp in sync_pairs if self.threshold_low <= sp.similarity < self.threshold_high)
        low_matches = sum(1 for sp in sync_pairs if sp.similarity < self.threshold_low)
        
        print(f"[ParagraphMatcher] ãƒãƒƒãƒãƒ³ã‚°å®Œäº†:")
        print(f"  ğŸŸ¢ é«˜ãƒãƒƒãƒ (50%+): {high_matches}ä»¶")
        print(f"  ğŸŸ¡ éƒ¨åˆ†ãƒãƒƒãƒ (30-50%): {mid_matches}ä»¶")
        print(f"  ğŸ”´ ä½ãƒãƒƒãƒ (<30%): {low_matches}ä»¶")
        print(f"  âšª æœªãƒãƒƒãƒ Web: {len(web_paragraphs) - len(matched_web)}ä»¶, PDF: {len(pdf_paragraphs) - len(matched_pdf)}ä»¶")
        
        return web_paragraphs, pdf_paragraphs, sync_pairs
    
    def calculate_sync_rate(self, sync_pairs: List[SyncPair], web_count: int, pdf_count: int) -> float:
        """
        å…¨ä½“ã®Syncç‡ã‚’è¨ˆç®—
        """
        if web_count == 0 and pdf_count == 0:
            return 0.0
        
        total_paragraphs = max(web_count, pdf_count)
        matched_weight = sum(sp.similarity for sp in sync_pairs)
        
        return matched_weight / total_paragraphs if total_paragraphs > 0 else 0.0


def create_paragraph_entries_from_clusters(
    clusters: List[Dict], 
    source: str,
    page_regions: List[Tuple[int, int]] = None
) -> List[ParagraphEntry]:
    """
    OCRã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ParagraphEntryãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        clusters: OCRçµæœã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒªã‚¹ãƒˆ
        source: "web" or "pdf"
        page_regions: ãƒšãƒ¼ã‚¸é ˜åŸŸãƒªã‚¹ãƒˆ [(y_start, y_end), ...]
    """
    entries = []
    prefix = "W" if source == "web" else "P"
    
    for i, c in enumerate(clusters):
        # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æ±ºå®š
        page_num = 1
        if page_regions:
            y_center = (c['rect'][1] + c['rect'][3]) // 2
            for j, (y_start, y_end) in enumerate(page_regions):
                if y_start <= y_center < y_end:
                    page_num = j + 1
                    break
        
        entry = ParagraphEntry(
            id=f"{prefix}-{i+1:03d}",
            source=source,
            text=c.get('text', ''),
            rect=c['rect'],
            page=page_num
        )
        entries.append(entry)
    
    return entries


def create_paragraph_entries_with_spatial_clustering(
    clusters: List[Dict], 
    source: str,
    image = None,
    page_regions: List[Tuple[int, int]] = None,
    use_spatial_clustering: bool = True
) -> List[ParagraphEntry]:
    """
    OCRã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ParagraphEntryãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ (ç©ºé–“ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä»˜ã)
    
    æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã«ç©ºé–“æ¯”ç‡ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’è¿½åŠ 
    
    Args:
        clusters: OCRçµæœã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒªã‚¹ãƒˆ
        source: "web" or "pdf"
        image: PIL Image (ç©ºé–“ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”¨ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
        page_regions: ãƒšãƒ¼ã‚¸é ˜åŸŸãƒªã‚¹ãƒˆ [(y_start, y_end), ...]
        use_spatial_clustering: ç©ºé–“ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    """
    # åŸºæœ¬ã®ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ç”Ÿæˆ
    entries = create_paragraph_entries_from_clusters(clusters, source, page_regions)
    
    # ç©ºé–“ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’è¿½åŠ é©ç”¨
    if use_spatial_clustering and image is not None:
        try:
            from app.core.spatial_cluster_analyzer import enhance_paragraph_detection
            
            # OCRçµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            ocr_dicts = [{'text': c.get('text', ''), 'rect': c['rect']} for c in clusters]
            
            # ç©ºé–“ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
            enhanced = enhance_paragraph_detection(ocr_dicts, image)
            
            # ãƒ­ã‚°å‡ºåŠ›
            spatial_count = sum(1 for p in enhanced if p.get('source') == 'spatial_cluster')
            template_count = sum(1 for p in enhanced if p.get('source') == 'template_match')
            print(f"[SpatialClustering] ç©ºé–“ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼: {spatial_count}ä»¶, ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒãƒƒãƒ: {template_count}ä»¶")
            
            # çµ±åˆçµæœã‚’è¿”ã™ (æ—¢å­˜+ç©ºé–“ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æƒ…å ±)
            # â€» ç¾åœ¨ã¯æ—¢å­˜çµæœã‚’ãã®ã¾ã¾è¿”ã—ã€ç©ºé–“æƒ…å ±ã¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿æŒ
            for entry in entries:
                entry_rect = entry.rect
                for p in enhanced:
                    p_rect = p.get('rect', (0,0,0,0))
                    # é‡ãªã‚Šåˆ¤å®š
                    if _rects_overlap(entry_rect, p_rect):
                        if p.get('source') == 'template_match':
                            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒãƒƒãƒæƒ…å ±ã‚’ä»˜åŠ  (å°†æ¥çš„ã«UIã§åˆ©ç”¨)
                            pass
            
        except Exception as e:
            print(f"[SpatialClustering] ã‚¨ãƒ©ãƒ¼: {e}")
    
    return entries


def _rects_overlap(rect1, rect2) -> bool:
    """2ã¤ã®çŸ©å½¢ãŒé‡ãªã£ã¦ã„ã‚‹ã‹åˆ¤å®š"""
    x1_1, y1_1, x2_1, y2_1 = rect1
    x1_2, y1_2, x2_2, y2_2 = rect2
    
    return not (x2_1 < x1_2 or x2_2 < x1_1 or y2_1 < y1_2 or y2_2 < y1_1)
