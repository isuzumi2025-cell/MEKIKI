import os
import io
import math
import statistics
from PIL import Image
from google.cloud import vision
from google.oauth2 import service_account
from app.core.interface import OCREngineStrategy

class CloudOCREngine(OCREngineStrategy):
    """
    Google Cloud Vision API (Raw Data Return Mode)
    GUIå´ã§ç·¨é›†å¯èƒ½ã«ã™ã‚‹ãŸã‚ã€ç”»åƒã¸ã®æç”»ã¯è¡Œã‚ãšã€
    ã€Œã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã€ã¨ã€Œå…¨æ–‡å­—ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã€ã‚’è¿”ã—ã¾ã™ã€‚
    """

    def __init__(self):
        key_path = "service_account.json"
        if os.path.exists(key_path):
            credentials = service_account.Credentials.from_service_account_file(key_path)
            self.client = vision.ImageAnnotatorClient(credentials=credentials)
        else:
            self.client = vision.ImageAnnotatorClient()

    def extract_text(self, image: Image.Image):
        """
        Returns: 
            clusters (list): è‡ªå‹•è§£æã•ã‚ŒãŸã‚¨ãƒªã‚¢æƒ…å ±ã®ãƒªã‚¹ãƒˆ
            raw_words (list): å…¨æ–‡å­—ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆæ‰‹å‹•ä¿®æ­£æ™‚ã®å†è¨ˆç®—ç”¨ï¼‰
        """
        try:
            # å…ƒã®ç”»åƒã‚µã‚¤ã‚ºã‚’ä¿å­˜ï¼ˆåº§æ¨™é€†å¤‰æ›ç”¨ï¼‰
            original_width = image.width
            original_height = image.height
            scale_ratio = 1.0  # ãƒªã‚µã‚¤ã‚ºæ¯”ç‡ï¼ˆé€†å¤‰æ›ç”¨ï¼‰
            
            # ç”»åƒãƒ¢ãƒ¼ãƒ‰å¤‰æ› (RGBA/P â†’ RGB)
            if image.mode in ('RGBA', 'P', 'LA'):
                rgb_image = Image.new('RGB', image.size, (255, 255, 255))
                if image.mode == 'RGBA' or image.mode == 'LA':
                    rgb_image.paste(image, mask=image.split()[-1])
                else:
                    rgb_image.paste(image)
                image = rgb_image
            elif image.mode != 'RGB':
                image = image.convert('RGB')
            # ç”»åƒã‚µã‚¤ã‚ºåˆ¶é™ã¨ãƒªã‚µã‚¤ã‚ºæˆ¦ç•¥
            max_dimension = 4096
            min_width = 800  # OCRç²¾åº¦ã®ãŸã‚ã®æœ€å°å¹…
            
            # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ç¢ºèª
            aspect_ratio = image.height / image.width
            
            if aspect_ratio > 5:
                # éå¸¸ã«ç¸¦é•·ã®ç”»åƒ: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦OCR
                print(f"ğŸ“ ç¸¦é•·ç”»åƒæ¤œå‡º (ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”: {aspect_ratio:.1f}) - ãƒãƒ£ãƒ³ã‚¯OCRãƒ¢ãƒ¼ãƒ‰")
                return self._ocr_tall_image_chunks(image)
            elif image.width > max_dimension or image.height > max_dimension:
                # é€šå¸¸ã®ãƒªã‚µã‚¤ã‚º (æœ€å°å¹…ã‚’ç¢ºä¿)
                scale_ratio = min(max_dimension / image.width, max_dimension / image.height)
                new_width = int(image.width * scale_ratio)
                
                # æœ€å°å¹…ã‚’ç¢ºä¿
                if new_width < min_width:
                    scale_ratio = min_width / image.width
                    new_width = min_width
                
                new_height = int(image.height * scale_ratio)
                # é«˜ã•ãŒ4096ã‚’è¶…ãˆã‚‹å ´åˆã¯ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
                if new_height > max_dimension:
                    print(f"ğŸ“ ç¸¦é•·ç”»åƒæ¤œå‡º - æœ€å°å¹…ç¶­æŒã®ãŸã‚ãƒãƒ£ãƒ³ã‚¯OCRãƒ¢ãƒ¼ãƒ‰")
                    return self._ocr_tall_image_chunks(image)
                
                new_size = (new_width, new_height)
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                print(f"ğŸ“ ç”»åƒãƒªã‚µã‚¤ã‚º: {new_size[0]}x{new_size[1]} (æ¯”ç‡: {scale_ratio:.3f})")
            
            # é€†å¤‰æ›æ¯”ç‡ã‚’è¨ˆç®— (OCRåº§æ¨™ â†’ å…ƒç”»åƒåº§æ¨™)
            inverse_ratio = 1.0 / scale_ratio if scale_ratio != 0 else 1.0
            
            # â˜… PNGå½¢å¼ã§é€ä¿¡ (å“è³ªç¶­æŒ - Legacyäº’æ›)
            img_byte_arr = io.BytesIO()
            image.save(img_byte_arr, format='PNG')  # JPEG â†’ PNG ã«å¤‰æ›´
            content = img_byte_arr.getvalue()
            
            # ã‚µã‚¤ã‚ºç¢ºèª
            size_mb = len(content) / (1024 * 1024)
            print(f"ğŸ“¤ OCRé€ä¿¡ã‚µã‚¤ã‚º: {size_mb:.2f}MB")
            
            vision_image = vision.Image(content=content)
            response = self.client.document_text_detection(image=vision_image)
            
            if response.error.message:
                raise RuntimeError(f"Cloud Vision API Error: {response.error.message}")

            # --- 1. ç”Ÿãƒ‡ãƒ¼ã‚¿ã®å–å¾— ---
            raw_blocks = [] # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”¨
            raw_words = []  # æ‰‹å‹•ç·¨é›†ç”¨ï¼ˆå…¨å˜èªã®åº§æ¨™ã¨ãƒ†ã‚­ã‚¹ãƒˆï¼‰

            for page in response.full_text_annotation.pages:
                for block in page.blocks:
                    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”¨ã®ãƒ–ãƒ­ãƒƒã‚¯æƒ…å ±æ§‹ç¯‰
                    block_text_parts = []
                    symbol_heights = []
                    
                    for paragraph in block.paragraphs:
                        for word in paragraph.words:
                            word_text = "".join([symbol.text for symbol in word.symbols])
                            block_text_parts.append(word_text)
                            
                            # å˜èªå˜ä½ã®æƒ…å ±ã‚’ä¿å­˜ï¼ˆæ‰‹å‹•ç·¨é›†ç”¨ï¼‰
                            wv = word.bounding_box.vertices
                            wx = [v.x for v in wv]
                            wy = [v.y for v in wv]
                            raw_words.append({
                                "text": word_text,
                                "rect": [min(wx), min(wy), max(wx), max(wy)],
                                "center": ((min(wx)+max(wx))/2, (min(wy)+max(wy))/2)
                            })

                            for symbol in word.symbols:
                                v = symbol.bounding_box.vertices
                                h = v[3].y - v[0].y
                                symbol_heights.append(h)
                    
                    # ãƒ–ãƒ­ãƒƒã‚¯æƒ…å ±
                    v = block.bounding_box.vertices
                    x_coords = [vertex.x for vertex in v]
                    y_coords = [vertex.y for vertex in v]
                    
                    raw_blocks.append({
                        "text": "".join(block_text_parts),
                        "rect": [min(x_coords), min(y_coords), max(x_coords), max(y_coords)],
                        "center_x": (min(x_coords) + max(x_coords)) / 2,
                        "width": max(x_coords) - min(x_coords),
                        "font_size": statistics.mean(symbol_heights) if symbol_heights else 10
                    })

            # --- 2. è‡ªå‹•ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ ---
            vertical_clusters = self._vertical_stack_clustering(raw_blocks)
            final_clusters = self._orphan_absorption(vertical_clusters)

            # ã‚½ãƒ¼ãƒˆ
            def sort_key(cluster):
                x0, y0, _, _ = cluster["rect"]
                row = round(y0 / 60) * 60
                return (row, x0)
            final_clusters.sort(key=sort_key)

            # è¾æ›¸å½¢å¼ã§æ•´å½¢ã—ã¦è¿”ã™
            # GUIå´ã§æ‰±ã„ã‚„ã™ã„ã‚ˆã†ã« ID ã‚’ä»˜ä¸
            # â˜… åº§æ¨™ã‚’å…ƒã®ç”»åƒã‚µã‚¤ã‚ºã«é€†å¤‰æ›
            formatted_clusters = []
            for i, c in enumerate(final_clusters):
                rect = c["rect"]
                scaled_rect = [
                    int(rect[0] * inverse_ratio),
                    int(rect[1] * inverse_ratio),
                    int(rect[2] * inverse_ratio),
                    int(rect[3] * inverse_ratio)
                ]
                formatted_clusters.append({
                    "id": i + 1,
                    "rect": scaled_rect,
                    "text": "\n".join(c["texts"])
                })

            # raw_wordsã‚‚é€†å¤‰æ›
            scaled_raw_words = []
            for w in raw_words:
                rect = w["rect"]
                center = w["center"]
                scaled_raw_words.append({
                    "text": w["text"],
                    "rect": [
                        int(rect[0] * inverse_ratio),
                        int(rect[1] * inverse_ratio),
                        int(rect[2] * inverse_ratio),
                        int(rect[3] * inverse_ratio)
                    ],
                    "center": (center[0] * inverse_ratio, center[1] * inverse_ratio)
                })

            print(f"ğŸ”„ åº§æ¨™é€†å¤‰æ›é©ç”¨ (inverse_ratio: {inverse_ratio:.3f})")
            return formatted_clusters, scaled_raw_words

        except Exception as e:
            raise RuntimeError(str(e))

    # --- ä»¥ä¸‹ã€å‰å›ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãã®ã¾ã¾åˆ©ç”¨ï¼‰ ---
    def _vertical_stack_clustering(self, blocks):
        if not blocks: return []
        blocks.sort(key=lambda b: b["rect"][1])
        
        clusters = [{
            "rect": b["rect"], 
            "texts": [b["text"]],
            "width": b["width"],
            "center_x": b["center_x"],
            "avg_font_size": b["font_size"]
        } for b in blocks]
        
        has_merged = True
        while has_merged:
            has_merged = False
            new_clusters = []
            skip_indices = set()

            for i in range(len(clusters)):
                if i in skip_indices: continue
                current = clusters[i]
                
                for j in range(i + 1, len(clusters)):
                    if j in skip_indices: continue
                    target = clusters[j]
                    
                    # çµåˆåˆ¤å®š - ãƒªã‚µã‚¤ã‚ºå¾Œã®ç”»åƒåº§æ¨™ã«é©ç”¨ã•ã‚Œã‚‹ãŸã‚ã€ä½™è£•ã‚’æŒã£ãŸé–¾å€¤ã‚’ä½¿ç”¨
                    x_overlap = min(current["rect"][2], target["rect"][2]) - max(current["rect"][0], target["rect"][0])
                    min_width = min(current["width"], target["width"])
                    overlap_ratio = x_overlap / min_width if min_width > 0 else 0
                    left_diff = abs(current["rect"][0] - target["rect"][0])
                    # æ•´åˆ—åˆ¤å®š (ãƒªã‚µã‚¤ã‚ºå¾Œã®åº§æ¨™ãªã®ã§å…ƒã®20pxã«æˆ»ã™)
                    is_aligned = overlap_ratio > 0.6 or left_diff < 30
                    
                    if not is_aligned: continue

                    gap_y = target["rect"][1] - current["rect"][3]
                    base_size = max(current["avg_font_size"], target["avg_font_size"])
                    # å‚ç›´ã‚®ãƒ£ãƒƒãƒ—é–¾å€¤ (ãƒªã‚µã‚¤ã‚ºå¾Œã®åº§æ¨™ãªã®ã§å…ƒã®å€¤ã«æˆ»ã™)
                    threshold_y = max(base_size * 2.5, 50)

                    if gap_y > threshold_y: continue
                    if current["avg_font_size"] > target["avg_font_size"] * 2.5: continue
                    if target["avg_font_size"] > current["avg_font_size"] * 2.0: continue
                    
                    # æ°´å¹³ã‚®ãƒ£ãƒƒãƒ— (ãƒªã‚µã‚¤ã‚ºå¾Œã®åº§æ¨™ãªã®ã§ä½™è£•ã‚’æŒãŸã›ã‚‹)
                    gap_x = max(0, target["rect"][0] - current["rect"][2]) if current["rect"][0] < target["rect"][0] else max(0, current["rect"][0] - target["rect"][2])
                    if gap_x > 15: continue

                    new_rect = [
                        min(current["rect"][0], target["rect"][0]),
                        min(current["rect"][1], target["rect"][1]),
                        max(current["rect"][2], target["rect"][2]),
                        max(current["rect"][3], target["rect"][3])
                    ]
                    
                    new_texts = current["texts"] + target["texts"] if current["rect"][1] < target["rect"][1] else target["texts"] + current["texts"]
                    new_avg = max(current["avg_font_size"], target["avg_font_size"])

                    current = {
                        "rect": new_rect, "texts": new_texts,
                        "width": new_rect[2] - new_rect[0],
                        "center_x": (new_rect[0] + new_rect[2]) / 2,
                        "avg_font_size": new_avg
                    }
                    skip_indices.add(j)
                    has_merged = True
                
                new_clusters.append(current)
            clusters = new_clusters
        return clusters

    def _orphan_absorption(self, clusters):
        if not clusters: return []
        areas = [(c["rect"][2]-c["rect"][0]) * (c["rect"][3]-c["rect"][1]) for c in clusters]
        avg_area = statistics.mean(areas) if areas else 0
        orphan_threshold = avg_area * 0.1 
        
        final_clusters = []
        orphans = []
        
        for c in clusters:
            area = (c["rect"][2]-c["rect"][0]) * (c["rect"][3]-c["rect"][1])
            text_len = sum(len(t) for t in c["texts"])
            if area < orphan_threshold or text_len < 3:
                orphans.append(c)
            else:
                final_clusters.append(c)

        if not final_clusters: return clusters

        for orphan in orphans:
            best_parent = None
            min_dist = float('inf')
            r1 = orphan["rect"]
            for parent in final_clusters:
                r2 = parent["rect"]
                dx = max(0, r2[0] - r1[2]) if r1[0] < r2[0] else max(0, r1[0] - r2[2])
                dy = max(0, r2[1] - r1[3]) if r1[1] < r2[1] else max(0, r1[1] - r2[3])
                dist = dx + dy
                if dist < 200 and dist < min_dist:
                    min_dist = dist
                    best_parent = parent
            
            if best_parent:
                r_p = best_parent["rect"]
                r_o = orphan["rect"]
                best_parent["rect"] = [min(r_p[0], r_o[0]), min(r_p[1], r_o[1]), max(r_p[2], r_o[2]), max(r_p[3], r_o[3])]
                best_parent["texts"].extend(orphan["texts"])
                best_parent["width"] = best_parent["rect"][2] - best_parent["rect"][0]
                best_parent["center_x"] = (best_parent["rect"][0] + best_parent["rect"][2]) / 2
            else:
                final_clusters.append(orphan)
        return final_clusters

    def _ocr_tall_image_chunks(self, image: Image.Image):
        """
        éå¸¸ã«ç¸¦é•·ã®ç”»åƒã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦OCRã‚’å®Ÿè¡Œ
        å„ãƒãƒ£ãƒ³ã‚¯ã®åº§æ¨™ã‚’å…ƒç”»åƒåº§æ¨™ã«å¤‰æ›ã—ã¦çµåˆ
        """
        from google.cloud import vision
        
        original_width = image.width
        original_height = image.height
        
        # ãƒãƒ£ãƒ³ã‚¯è¨­å®š
        max_chunk_height = 4000  # å„ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§é«˜ã•
        target_width = min(original_width, 1200)  # OCRç”¨ã®å¹…
        
        # ãƒªã‚µã‚¤ã‚ºæ¯”ç‡
        width_scale = target_width / original_width
        scaled_height = int(original_height * width_scale)
        
        # ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’è¨ˆç®—
        num_chunks = math.ceil(scaled_height / max_chunk_height)
        chunk_height_original = math.ceil(original_height / num_chunks)
        
        print(f"ğŸ“ ãƒãƒ£ãƒ³ã‚¯OCR: {num_chunks}ãƒãƒ£ãƒ³ã‚¯ (å„{chunk_height_original}px)")
        
        all_blocks = []
        all_raw_words = []
        
        for i in range(num_chunks):
            # ãƒãƒ£ãƒ³ã‚¯ã‚’åˆ‡ã‚Šå‡ºã—
            y_start = i * chunk_height_original
            y_end = min((i + 1) * chunk_height_original, original_height)
            chunk = image.crop((0, y_start, original_width, y_end))
            
            # ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒªã‚µã‚¤ã‚º
            chunk_width = target_width
            chunk_height = int(chunk.height * width_scale)
            chunk_resized = chunk.resize((chunk_width, chunk_height), Image.Resampling.LANCZOS)
            
            # RGBå¤‰æ›
            if chunk_resized.mode != 'RGB':
                chunk_resized = chunk_resized.convert('RGB')
            
            # OCRå®Ÿè¡Œ
            img_byte_arr = io.BytesIO()
            chunk_resized.save(img_byte_arr, format='PNG')  # PNG for quality
            content = img_byte_arr.getvalue()
            
            vision_image = vision.Image(content=content)
            response = self.client.document_text_detection(image=vision_image)
            
            if response.error.message:
                print(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯{i+1}ã‚¨ãƒ©ãƒ¼: {response.error.message}")
                continue
            
            # åº§æ¨™ã‚’å…ƒç”»åƒåº§æ¨™ã«å¤‰æ›
            inverse_scale = 1.0 / width_scale
            y_offset = y_start  # å…ƒç”»åƒã§ã®ãƒãƒ£ãƒ³ã‚¯é–‹å§‹ä½ç½®
            
            for page in response.full_text_annotation.pages:
                for block in page.blocks:
                    block_text_parts = []
                    symbol_heights = []
                    
                    for paragraph in block.paragraphs:
                        for word in paragraph.words:
                            word_text = "".join([symbol.text for symbol in word.symbols])
                            block_text_parts.append(word_text)
                            
                            # raw_wordsã«è¿½åŠ 
                            wv = word.bounding_box.vertices
                            wx = [v.x for v in wv]
                            wy = [v.y for v in wv]
                            all_raw_words.append({
                                "text": word_text,
                                "rect": [
                                    int(min(wx) * inverse_scale),
                                    int(min(wy) * inverse_scale + y_offset),
                                    int(max(wx) * inverse_scale),
                                    int(max(wy) * inverse_scale + y_offset)
                                ],
                                "center": (
                                    (min(wx) + max(wx)) / 2 * inverse_scale,
                                    (min(wy) + max(wy)) / 2 * inverse_scale + y_offset
                                )
                            })
                            
                            for symbol in word.symbols:
                                v = symbol.bounding_box.vertices
                                h = v[3].y - v[0].y
                                symbol_heights.append(h)
                    
                    v = block.bounding_box.vertices
                    x_coords = [vertex.x for vertex in v]
                    y_coords = [vertex.y for vertex in v]
                    
                    all_blocks.append({
                        "text": "".join(block_text_parts),
                        "rect": [
                            int(min(x_coords) * inverse_scale),
                            int(min(y_coords) * inverse_scale + y_offset),
                            int(max(x_coords) * inverse_scale),
                            int(max(y_coords) * inverse_scale + y_offset)
                        ],
                        "center_x": (min(x_coords) + max(x_coords)) / 2 * inverse_scale,
                        "width": (max(x_coords) - min(x_coords)) * inverse_scale,
                        "font_size": statistics.mean(symbol_heights) * inverse_scale if symbol_heights else 10
                    })
            
            print(f"  ãƒãƒ£ãƒ³ã‚¯{i+1}/{num_chunks}: {len(all_blocks)}ãƒ–ãƒ­ãƒƒã‚¯æ¤œå‡º")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        vertical_clusters = self._vertical_stack_clustering(all_blocks)
        final_clusters = self._orphan_absorption(vertical_clusters)
        
        # ã‚½ãƒ¼ãƒˆ
        def sort_key(cluster):
            x0, y0, _, _ = cluster["rect"]
            row = round(y0 / 60) * 60
            return (row, x0)
        final_clusters.sort(key=sort_key)
        
        # æ•´å½¢
        formatted_clusters = []
        for i, c in enumerate(final_clusters):
            formatted_clusters.append({
                "id": i + 1,
                "rect": list(map(int, c["rect"])),
                "text": "\n".join(c["texts"])
            })
        
        print(f"âœ… ãƒãƒ£ãƒ³ã‚¯OCRå®Œäº†: {len(formatted_clusters)}ã‚¯ãƒ©ã‚¹ã‚¿, {len(all_raw_words)}å˜èª")
        return formatted_clusters, all_raw_words

