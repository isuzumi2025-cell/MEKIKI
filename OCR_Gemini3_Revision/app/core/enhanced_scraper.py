"""
Phase 1: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ã®å¼·åŒ–
é…å»¶èª­ã¿è¾¼ã¿ï¼ˆLazy Loadingï¼‰å¯¾å¿œ
"""
from playwright.sync_api import sync_playwright, Page
from PIL import Image
from typing import List, Dict, Optional, Tuple
import io
import time


class EnhancedWebScraper:
    """å¼·åŒ–ç‰ˆWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ - ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¨Lazy Loadingå¯¾å¿œ"""
    
    def __init__(self, headless: bool = True, viewport_width: int = 1920, viewport_height: int = 1080):
        """
        Args:
            headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹ã‹
            viewport_width: ãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ãƒˆå¹…
            viewport_height: ãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ãƒˆé«˜ã•
        """
        self.headless = headless
        self.viewport_width = viewport_width
        self.viewport_height = viewport_height
    
    def scrape_with_lazy_loading(
        self,
        url: str,
        username: Optional[str] = None,
        password: Optional[str] = None,
        max_scrolls: int = 10,
        scroll_delay: float = 1.0
    ) -> Tuple[str, str, Image.Image, Image.Image]:
        """
        é…å»¶èª­ã¿è¾¼ã¿å¯¾å¿œã§Webãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        
        Args:
            url: å¯¾è±¡URL
            username: Basicèªè¨¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å
            password: Basicèªè¨¼ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
            max_scrolls: æœ€å¤§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°
            scroll_delay: ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«é–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
        
        Returns:
            (title, text, full_page_image, viewport_image)
        """
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=self.headless)
            
            context_options = {
                'viewport': {'width': self.viewport_width, 'height': self.viewport_height},
                'device_scale_factor': 2.0
            }
            
            # Basicèªè¨¼è¨­å®š
            if username and password:
                context_options['http_credentials'] = {
                    'username': username,
                    'password': password
                }
            
            context = browser.new_context(**context_options)
            page = context.new_page()
            
            try:
                print(f"ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
                page.goto(url, timeout=60000)
                page.wait_for_load_state("networkidle")
                
                # ãƒšãƒ¼ã‚¸ä¸‹éƒ¨ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆLazy Loadingå¯¾å¿œï¼‰
                self._scroll_to_bottom(page, max_scrolls, scroll_delay)
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
                title = page.title()
                text_content = page.inner_text("body")
                
                # 1ç”»é¢åˆ†ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
                page.evaluate("window.scrollTo(0, 0)")  # ãƒˆãƒƒãƒ—ã«æˆ»ã‚‹
                time.sleep(0.5)
                viewport_bytes = page.screenshot(full_page=False)
                viewport_image = Image.open(io.BytesIO(viewport_bytes))
                
                # ãƒ•ãƒ«ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ (ã‚¹ãƒ†ã‚£ãƒƒãƒãƒ³ã‚°æ–¹å¼ã«å¤‰æ›´)
                full_image = self._capture_full_page_stitched(page)
                # full_bytes = page.screenshot(full_page=True)
                # full_image = Image.open(io.BytesIO(full_bytes))
                
                print(f"âœ… å–å¾—å®Œäº†: {title}")
                return title, text_content, full_image, viewport_image
                
            except Exception as e:
                raise Exception(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {str(e)}")
            finally:
                context.close()
                browser.close()
    
    def _scroll_to_bottom(self, page: Page, max_scrolls: int, delay: float):
        """
        ãƒšãƒ¼ã‚¸ä¸‹éƒ¨ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆé…å»¶èª­ã¿è¾¼ã¿å¯¾å¿œï¼‰
        
        Args:
            page: Playwrightã®ãƒšãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            max_scrolls: æœ€å¤§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°
            delay: ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«é–“ã®å¾…æ©Ÿæ™‚é–“
        """
        print(f"ğŸ“œ ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­...")
        
        for i in range(max_scrolls):
            # ç¾åœ¨ã®é«˜ã•ã‚’å–å¾—
            prev_height = page.evaluate("document.body.scrollHeight")
            
            # ä¸‹ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(delay)
            
            # æ–°ã—ã„é«˜ã•ã‚’å–å¾—
            new_height = page.evaluate("document.body.scrollHeight")
            
            # é«˜ã•ãŒå¤‰ã‚ã‚‰ãªã‘ã‚Œã°çµ‚äº†ï¼ˆã“ã‚Œä»¥ä¸Šèª­ã¿è¾¼ã‚€ã‚‚ã®ãŒãªã„ï¼‰
            if new_height == prev_height:
                print(f"âœ… ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†ï¼ˆ{i + 1}å›ï¼‰")
                break
        else:
            print(f"âœ… æœ€å¤§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°ã«åˆ°é”ï¼ˆ{max_scrolls}å›ï¼‰")

    def _capture_full_page_stitched(self, page: Page) -> Image.Image:
        """
        ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ãªãŒã‚‰ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã¦çµåˆã™ã‚‹ï¼ˆUltrathink Stitchingï¼‰
        - ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒãƒ¼ã‚’éè¡¨ç¤ºã«ã™ã‚‹
        - é…å»¶èª­ã¿è¾¼ã¿ã‚’ç¢ºå®Ÿã«å¾…æ©Ÿ
        - æ­£ç¢ºãªã‚¹ãƒ†ã‚£ãƒƒãƒãƒ³ã‚°
        """
        try:
            print("ğŸ“¸ Ultrathinkã‚­ãƒ£ãƒ—ãƒãƒ£é–‹å§‹...")

            # 1. ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒãƒ¼ã‚’éè¡¨ç¤ºã«ã™ã‚‹ (CSSæ³¨å…¥)
            page.add_style_tag(content="""
                body { overflow-y: hidden !important; }
                ::-webkit-scrollbar { display: none; }
            """)
            
            # --- ç”»åƒå¼·åˆ¶èª­ã¿è¾¼ã¿ (Pre-roll Scroll) ---
            print("   ğŸ”„ ç”»åƒèª­ã¿è¾¼ã¿ã®ãŸã‚ã®ãƒ—ãƒ¬ãƒ­ãƒ¼ãƒ«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«...")
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2.0) # ä¸‹éƒ¨ã§å¾…æ©Ÿ
            page.evaluate("window.scrollTo(0, 0)")
            time.sleep(2.0) # ä¸Šéƒ¨ã«æˆ»ã£ã¦å¾…æ©Ÿ
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
            total_height = page.evaluate("document.body.scrollHeight")
            viewport_size = page.viewport_size
            viewport_height = viewport_size['height']
            
            # ã‚­ãƒ£ãƒ—ãƒãƒ£è¨ˆç”»
            images = []
            current_scroll = 0
            
            # ãƒˆãƒƒãƒ—ã«æˆ»ã‚‹
            page.evaluate("window.scrollTo(0, 0)")
            time.sleep(2.0) # åˆæœŸãƒ­ãƒ¼ãƒ‰å¾…æ©Ÿï¼ˆé•·ã‚ã«ï¼‰
            
            # A4æ¨ªæ¯”ç‡ï¼ˆ1.414ï¼‰ã‚’æ„è­˜ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã«ã™ã‚‹ã‹ï¼Ÿ
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã¯ã€ŒA4æ¨ªã‚µã‚¤ã‚ºã”ã¨ã«ã€ã ãŒã€Webãƒšãƒ¼ã‚¸ã¯ç¸¦é•·ãªã®ã§
            # ãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ãƒˆå˜ä½ã§ç¶ºéº—ã«æ’®ã£ã¦ã€å¾Œã§ã©ã†ã«ã§ã‚‚å°åˆ·ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã®ãŒæœ€å–„ã€‚
            # ã“ã“ã§ã¯ãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ãƒˆé«˜ã•ã‚’åŸºæº–ã«ã™ã‚‹ã€‚
            
            while current_scroll < total_height:
                # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ
                page.evaluate(f"window.scrollTo(0, {current_scroll})")
                
                # é‡è¦: Lazy Loadingå¾…æ©Ÿ
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€ŒHTMLã®ã¿ã«ãªã‚‹ã€å•é¡Œã¯ã“ã“ãŒåŸå› ã®å¯èƒ½æ€§å¤§
                time.sleep(1.0) 
                
                # å¿µã®ãŸã‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ã‚¤ãƒ‰ãƒ«ã‚‚å¾…ã¤ï¼ˆçŸ­ã‚ã«åˆ¶é™ï¼‰
                try:
                    page.wait_for_load_state("networkidle", timeout=2000)
                except:
                    pass # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¦ã‚‚ç¶šè¡Œ

                # ã‚­ãƒ£ãƒ—ãƒãƒ£
                screenshot_bytes = page.screenshot()
                img = Image.open(io.BytesIO(screenshot_bytes))
                images.append(img)
                
                print(f"   - ã‚­ãƒ£ãƒ—ãƒãƒ£: {current_scroll}px / {total_height}px")
                
                current_scroll += viewport_height
                
                # å®‰å…¨è£…ç½®
                if len(images) > 100:
                    print("âš ï¸ ãƒšãƒ¼ã‚¸ãŒé•·ã™ãã‚‹ãŸã‚ä¸­æ–­")
                    break
            
            if not images:
                return Image.new('RGB', (100, 100), 'gray')

            # 2. çµåˆå‡¦ç†
            print(f"ğŸ§© {len(images)}æšã®ç”»åƒã‚’çµåˆä¸­...")
            
            # æœ€å¾Œã®ç”»åƒã¯é‡è¤‡éƒ¨åˆ†ãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹
            # ã—ã‹ã—ã€å˜ç´”ãªã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«çµåˆã§å¤šãã®å ´åˆã¯æ©Ÿèƒ½ã™ã‚‹ã€‚
            # å³å¯†ã«ã¯ `total_height` ã«åˆã‚ã›ã¦ãƒˆãƒªãƒŸãƒ³ã‚°ãŒå¿…è¦ã ãŒã€
            # Playwrightã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã¯æ­£ç¢ºãªã®ã§ã€ãã®ã¾ã¾ã‚¹ã‚¿ãƒƒã‚¯ã™ã‚‹ã€‚
            
            # æœ€çµ‚ç”»åƒã®é«˜ã•èª¿æ•´ï¼ˆã¯ã¿å‡ºã—éƒ¨åˆ†ã®ã‚«ãƒƒãƒˆï¼‰
            # æœ€å¾Œã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ä½ç½®ã§ã®è¡¨ç¤ºå†…å®¹ = ãƒšãƒ¼ã‚¸æœ€ä¸‹éƒ¨
            # æœ€å¾Œã®ç”»åƒã®æœ‰åŠ¹é«˜ã• = total_height % viewport_height (0ãªã‚‰ãã®ã¾ã¾)
            # ã„ã‚„ã€viewportã§æ’®ã‚‹ã®ã§ã€æœ€å¾Œã®ç”»åƒã¯ã€Œä¸‹ã‹ã‚‰viewportåˆ†ã€ã§ã¯ãªãã€Œä¸Šã‹ã‚‰ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ãŸä½ç½®ã€
            # æœ€å¾Œã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãŒ `total_height - viewport_height` æœªæº€ã®å ´åˆã€é‡è¤‡ã¯ã—ãªã„ã€‚
            
            # å˜ç´”çµåˆ
            full_w = images[0].width
            full_h = sum(img.height for img in images)
            
            # å®Ÿéš›ã«ãƒšãƒ¼ã‚¸é«˜ã•ã‚ˆã‚Šå¤§ãããªã‚‹å ´åˆï¼ˆæœ€å¾Œã®ç™½ç´™éƒ¨åˆ†ãªã©ï¼‰ã¯ãƒˆãƒªãƒŸãƒ³ã‚°
            # ä½™ç™½å‰Šé™¤ã¯å¾Œå‡¦ç†ã§è¡Œã†ã¨ã—ã¦ã‚‚ã€ä¸€æ—¦çµåˆã€‚
            
            stitched_image = Image.new('RGB', (full_w, full_h))
            curr_y = 0
            for img in images:
                stitched_image.paste(img, (0, curr_y))
                curr_y += img.height
            
            # ãƒšãƒ¼ã‚¸æœ«å°¾ã®é‡è¤‡é™¤å»ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            # ä»Šå›ã¯ã€Œç¢ºå®Ÿæ€§ã€é‡è¦–ã§ã€é‡è¤‡ã—ã¦ã‚‚æƒ…å ±ãŒæ¶ˆãˆã‚‹ã‚ˆã‚Šãƒã‚·ã¨ã™ã‚‹ã€‚
            
            print("âœ… Ultrathinkã‚­ãƒ£ãƒ—ãƒãƒ£å®Œäº†")
            return stitched_image

        except Exception as e:
            print(f"âš ï¸ ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é€šå¸¸ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’è©¦ã¿ã‚‹
            full_bytes = page.screenshot(full_page=True)
            return Image.open(io.BytesIO(full_bytes))
    
    def crawl_site(
        self,
        base_url: str,
        max_pages: int = 50,
        same_domain_only: bool = True,
        username: Optional[str] = None,
        password: Optional[str] = None
    ) -> List[Dict]:
        """
        ã‚µã‚¤ãƒˆå†…ã‚’ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°
        
        Args:
            base_url: é–‹å§‹URL
            max_pages: æœ€å¤§ãƒšãƒ¼ã‚¸æ•°
            same_domain_only: åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿
            username: Basicèªè¨¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å
            password: Basicèªè¨¼ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
        
        Returns:
            [{"url": str, "title": str, "text": str, "full_image": Image, "viewport_image": Image}, ...]
        """
        from urllib.parse import urlparse, urljoin
        import re
        
        visited = set()
        to_visit = [base_url]
        results = []
        base_domain = urlparse(base_url).netloc
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=self.headless)
            
            context_options = {
                'viewport': {'width': self.viewport_width, 'height': self.viewport_height},
                'device_scale_factor': 2.0
            }
            
            if username and password:
                context_options['http_credentials'] = {
                    'username': username,
                    'password': password
                }
            
            context = browser.new_context(**context_options)
            page = context.new_page()
            
            try:
                while to_visit and len(results) < max_pages:
                    current_url = to_visit.pop(0)
                    
                    if current_url in visited:
                        continue
                    
                    # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
                    if same_domain_only:
                        current_domain = urlparse(current_url).netloc
                        if current_domain != base_domain:
                            continue
                    
                    try:
                        print(f"ğŸŒ [{len(results) + 1}/{max_pages}] {current_url}")
                        
                        page.goto(current_url, timeout=60000)
                        page.wait_for_load_state("networkidle")
                        
                        # Lazy Loadingå¯¾å¿œã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                        self._scroll_to_bottom(page, max_scrolls=5, delay=0.5)
                        
                        # ãƒ‡ãƒ¼ã‚¿å–å¾—
                        title = page.title()
                        text_content = page.inner_text("body")
                        
                        # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
                        page.evaluate("window.scrollTo(0, 0)")
                        time.sleep(0.3)
                        viewport_bytes = page.screenshot(full_page=False)
                        viewport_image = Image.open(io.BytesIO(viewport_bytes))
                        
                        full_image = self._capture_full_page_stitched(page)
                        # full_bytes = page.screenshot(full_page=True)
                        # full_image = Image.open(io.BytesIO(full_bytes))
                        
                        results.append({
                            "url": current_url,
                            "title": title,
                            "text": text_content,
                            "full_image": full_image,
                            "viewport_image": viewport_image
                        })
                        
                        visited.add(current_url)
                        
                        # ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                        links = page.evaluate("""
                            () => {
                                return Array.from(document.querySelectorAll('a[href]'))
                                    .map(a => a.href)
                                    .filter(href => href.startsWith('http'));
                            }
                        """)
                        
                        for link in links:
                            # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆé™¤å»
                            link = link.split('#')[0]
                            if link and link not in visited and link not in to_visit:
                                to_visit.append(link)
                        
                        time.sleep(1.0)  # è² è·è»½æ¸›
                        
                    except Exception as e:
                        print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {current_url} - {str(e)}")
                        visited.add(current_url)
                        continue
                
            finally:
                context.close()
                browser.close()
        
        print(f"âœ… ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Œäº†: {len(results)}ãƒšãƒ¼ã‚¸å–å¾—")
        return results


# ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
EnhancedScraper = EnhancedWebScraper

